{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import pickle as pkl\n",
    "from utils.utils import merge_eval_sold_on_df, sort_df_on_d, WRMSSE, RMSSE, _down_cast, data_preprocessing, diff_lists, log_status\n",
    "from utils.utils import customIter, cross_validation_on_validation_set, ensemble_submissions\n",
    "\n",
    "from utils.configure_logger import configure_logger\n",
    "configure_logger()\n",
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = \"../data/m5-forecasting-accuracy/\"\n",
    "SALES_EVALUATION = \"sales_train_evaluation.csv\"\n",
    "SALES_VALIDATION = \"sales_train_validation.csv\"\n",
    "CALENDAR = \"calendar.csv\"\n",
    "SAMPLE_SUBMISSION = \"sample_submission.csv\"\n",
    "SELL_PRICES = \"sell_prices.csv\"\n",
    "\n",
    "PRECOMPUTED_BASE_PATH = \"../data/accuracy/features/\"\n",
    "precomputed_name = lambda store, eval_val: f\"processed_{store}_{eval_val}.pkl\"\n",
    "\n",
    "PREDICTION_BASE_PATH = '../data/accuracy/temp_submissions/'\n",
    "SUBMISSION_BASE_PATH = '../data/accuracy/submissions/'\n",
    "DAYS: int = 28\n",
    "\n",
    "SUB_D_START_VAL: int = 1914\n",
    "SUB_D_START_EVAL: int = 1914 + 28\n",
    "\n",
    "# the columns are always included after feature processing\n",
    "# because they are required in the training and submission format\n",
    "DROP_FEATURE_COLUMNS = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'sold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GridSearch functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_status\n",
    "def grid_search(params: dict, param_grid: dict, train_data: lgb.Dataset, validation_data: lgb.Dataset):\n",
    "    \"\"\" \n",
    "    Given a grid with parameters, train lgb model for all possible combinations.\n",
    "    Returns the parameter set with the best score and the dictionary with all results.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    param_combinations = list(itertools.product(*param_grid.values()))\n",
    "    results = {}\n",
    "    for i, param_combination in enumerate(param_combinations,1):\n",
    "        \n",
    "        # create dictionary with all parameters\n",
    "        param_combination = {k:v for k,v in zip(param_grid.keys(), param_combination)}\n",
    "        param_combination.update(params)\n",
    "        \n",
    "        # train lgb model        \n",
    "        temp_dict = {}\n",
    "        mod: lgb.Booster = lgb.train(param_combination, \n",
    "            train_set = train_data,\n",
    "            valid_sets = validation_data,\n",
    "            evals_result = temp_dict\n",
    "        )\n",
    "        \n",
    "        # store results\n",
    "        results[f\"combination_{i}\"] = {}\n",
    "        results[f\"combination_{i}\"] = {\n",
    "            \"params\": param_combination,\n",
    "            \"validation_score\": temp_dict[\"valid_0\"][\"rmse\"][-1],\n",
    "            \"model\": mod\n",
    "        }\n",
    "        \n",
    "    # sort the results based on evaluation score\n",
    "    sorted_results = dict(sorted(results.items(), key=lambda item: item[1][\"validation_score\"]))\n",
    "    return list(sorted_results.values())[-1], results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Run for Testing (Cross Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of = 'val'\n",
    "features = pd.read_parquet(PRECOMPUTED_BASE_PATH + f'features_{type_of}.parquet')\n",
    "features = _down_cast(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total ~280 seconds\n",
    "params = {\n",
    "    'objective': 'tweedie',\n",
    "    'metric': 'rmse', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': 1,\n",
    "    'n_jobs': 4,\n",
    "    \"tweedie_variance_power\": 1.1, # Set the Tweedie variance power (1 <= p <= 2)\n",
    "    'eval_at': 10,\n",
    "    # 'verbose_eval': 0\n",
    "    'subsample': 0.5,\n",
    "    'subsample_freq': 1,\n",
    "    'feature_fraction': 0.5,\n",
    "    'boost_from_average': False,\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    \"num_leaves\": [255], #[int(2**i) for i in [5, 6, 8]],\n",
    "    'min_data_in_leaf': [255], #[int(2**i -1) for i in [5, 6, 8]]\n",
    "    \"learning_rate\": [0.05],#[0.04, 0.02, 0.01, 0.005],\n",
    "    \"n_estimators\": [45], # [5000, 500, 100]\n",
    "    \"tweedie_variance_power\": [1.1], # Set the Tweedie variance power (1 <= p <= 2)\n",
    "    # 'max_bin': [100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# perform grid search for best parameters\n",
    "group_columns: list = ['store_id']\n",
    "for id, f in features.groupby(group_columns):\n",
    "    # split data to training and testing\n",
    "    x_train, x_test, y_train, y_test = train_test_split(f.drop(DROP_FEATURE_COLUMNS, axis=1), f['sold'], train_size=.8, shuffle=False, random_state=42)\n",
    "    del f\n",
    "    train_data = lgb.Dataset(x_train, y_train)\n",
    "    del x_train, y_train\n",
    "    validation_data = lgb.Dataset(x_test, y_test)\n",
    "    del x_test, y_test\n",
    "\n",
    "    best_combination, results = grid_search(params, param_grid, train_data = train_data, validation_data = validation_data)\n",
    "    print(best_combination)\n",
    "\n",
    "# # train_best_model\n",
    "# mod = lgb.train(best_combination[\"params\"],\n",
    "#     train_set = train_data\n",
    "# )\n",
    "# predictions = mod.predict(features_predict)\n",
    "# df_pred = pd.DataFrame({\n",
    "#     \"prod_id\": id_predict,\n",
    "#     \"d\": d_list_predict,\n",
    "#     \"sold\": predictions\n",
    "# })\n",
    "\n",
    "# plot_best_model: bool = True\n",
    "# if plot_best_model:\n",
    "#     fig, axs = plt.subplots(3,1, figsize = (25,10))\n",
    "    \n",
    "#     # first figure (feature importance)\n",
    "#     lgb.plot_importance(best_combination[\"model\"], ax = axs[0])\n",
    "    \n",
    "#     # second figure (resid)\n",
    "#     pred = best_combination[\"model\"].predict(x_test)\n",
    "#     resid = y_test - pred\n",
    "#     axs[1].plot(resid)\n",
    "    \n",
    "#     # third figure (resid)\n",
    "#     axs[2].hist(resid, bins = 400)\n",
    "#     axs[2].set_xlim(-25, 25)\n",
    "    \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train + Predict submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_parquet(PRECOMPUTED_BASE_PATH + f'features_{type_of}.parquet')\n",
    "features = _down_cast(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 23:43:50 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 / 10\r"
     ]
    }
   ],
   "source": [
    "type_of = 'val'\n",
    "\n",
    "try:\n",
    "    features = pd.DataFrame(features)\n",
    "except Exception:\n",
    "    logger.info('(re)loading features')\n",
    "    features = pd.read_parquet(PRECOMPUTED_BASE_PATH + f'features_{type_of}.parquet')\n",
    "    features = _down_cast(features)\n",
    "\n",
    "# group_columns = ['store_id', 'cat_id'] # 'store_id', 'cat_id', 'dept_id'\n",
    "# group_columns = ['store_id', 'dept_id']\n",
    "group_columns = ['store_id']\n",
    "do_grid_search = False\n",
    "res: list = []\n",
    "\n",
    "exclude_prefix_list = ['auto'] # unconditional, auto, momentum, seasonal\n",
    "\n",
    "for id, features_gr in customIter(features.groupby(group_columns)):\n",
    "    \n",
    "    # logger.info('selecting train and pred indices')\n",
    "    # prepare df\n",
    "    features_gr: pd.DataFrame = features_gr.reset_index(drop=True)\n",
    "    features_gr = features_gr[[c for c in features_gr if c.split('_')[0] not in exclude_prefix_list]]\n",
    "    \n",
    "    sub_d_start = SUB_D_START_VAL if type_of == 'val' else SUB_D_START_EVAL\n",
    "    train_idx = features_gr['sold'].notna() & features_gr['d'].isin([f'd_{sub_d_start - 1 - i}' for i in range(1460)])\n",
    "    pred_idx = features_gr['d'].isin([f'd_{sub_d_start + i}' for i in range(28)])\n",
    "    df_train = features_gr[train_idx]\n",
    "    df_pred = features_gr[pred_idx]\n",
    "    features_train: pd.DataFrame = df_train.drop(DROP_FEATURE_COLUMNS, axis = 1)\n",
    "    targets_train: pd.Series = df_train['sold']\n",
    "    features_predict: pd.DataFrame = df_pred.drop(DROP_FEATURE_COLUMNS, axis = 1)\n",
    "    \n",
    "    # perform grid search for best parameters\n",
    "    if do_grid_search == True:\n",
    "        # split data to training and testing\n",
    "        logger.info('divide for cross validation')\n",
    "        x_train, x_test, y_train, y_test = train_test_split(features_train, targets_train, train_size=.8, shuffle=False, random_state=42)\n",
    "        train_data = lgb.Dataset(x_train, y_train)\n",
    "        validation_data = lgb.Dataset(x_test, y_test)\n",
    "        logger.info('perform gridsearch')\n",
    "        best_combination, results = grid_search(params, param_grid, train_data = train_data, validation_data = validation_data)\n",
    "        del train_data; del validation_data\n",
    "        params_grid_train = best_combination[\"params\"]\n",
    "    else:\n",
    "        params_grid_train = {\n",
    "            'objective': 'tweedie',\n",
    "            'metric': 'rmse', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': 43,\n",
    "            'verbose': -1,\n",
    "            'n_jobs': 4,\n",
    "            'subsample': 0.5,\n",
    "            'subsample_freq': 1,\n",
    "            \"num_leaves\": 2**8-1,\n",
    "            'min_data_in_leaf': 2**8-1,\n",
    "            'feature_fraction': 0.5,\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"n_estimators\": 45,\n",
    "            # \"max_bin\": 100,\n",
    "            'boost_from_average': False,\n",
    "            \"tweedie_variance_power\": 1.1, # Set the Tweedie variance power (1 <= p <= 2)\n",
    "        }\n",
    "\n",
    "    # train_best_model\n",
    "    # logger.info('train final model')\n",
    "    mod = lgb.train(params_grid_train,\n",
    "        train_set = lgb.Dataset(features_train, targets_train)\n",
    "    )\n",
    "    predictions = mod.predict(features_predict)\n",
    "    df_pred = pd.DataFrame({\n",
    "        \"id\": df_pred['id'],\n",
    "        \"d\": df_pred['d'],\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "    res.append(_down_cast(df_pred))\n",
    "    \n",
    "del features\n",
    "    \n",
    "# storing predictions\n",
    "df_sub_val = pd.concat(res)\n",
    "group_names = '_'.join(group_columns)\n",
    "exclude_names = 'None' if len(exclude_prefix_list) == 0 else '_'.join(exclude_prefix_list)\n",
    "df_sub_val.to_csv(PREDICTION_BASE_PATH + f'lgb_multivariate_{type_of}_non_transposed_{group_names}_exclude_{exclude_names}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load val + eval prediction files and merge to one submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub_val = pd.read_csv(PREDICTION_BASE_PATH + 'lgb_multivariate_val_non_transposed_temp.csv')\n",
    "exclude_columns = '_exclude_auto'\n",
    "# exclude_columns = ''\n",
    "df_sub_val = ensemble_submissions(\n",
    "    [\n",
    "        PREDICTION_BASE_PATH + f'lgb_multivariate_val_non_transposed_store_id_cat_id{exclude_columns}.csv', \n",
    "        PREDICTION_BASE_PATH + f'lgb_multivariate_val_non_transposed_store_id_dept_id{exclude_columns}.csv', \n",
    "        PREDICTION_BASE_PATH + f'lgb_multivariate_val_non_transposed_store_id{exclude_columns}.csv', \n",
    "\n",
    "    ]\n",
    ")\n",
    "transpose = True\n",
    "if transpose == True:\n",
    "    sub_validation = df_sub_val.pivot(index='id', columns='d', values='pred').reset_index(drop=False)\n",
    "    sub_validation.columns = [\"id\"] + [f\"F{i}\" for i in range(1,DAYS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub_eval = pd.read_csv(PREDICTION_BASE_PATH + 'lgb_multivariate_eval_non_transposed_temp.csv')\n",
    "exclude_columns = '_exclude_auto'\n",
    "df_sub_eval = ensemble_submissions(\n",
    "    [\n",
    "        PREDICTION_BASE_PATH + f'lgb_multivariate_eval_non_transposed_store_id_cat_id{exclude_columns}.csv', \n",
    "        PREDICTION_BASE_PATH + f'lgb_multivariate_eval_non_transposed_store_id_dept_id{exclude_columns}.csv', \n",
    "        PREDICTION_BASE_PATH + f'lgb_multivariate_eval_non_transposed_store_id{exclude_columns}.csv', \n",
    "\n",
    "    ]\n",
    ")\n",
    "transpose = True\n",
    "if transpose == True:\n",
    "    sub_evaluation = df_sub_eval.pivot(index='id', columns='d', values='pred').reset_index(drop=False)\n",
    "    sub_evaluation.columns = [\"id\"] + [f\"F{i}\" for i in range(1,DAYS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_evaluation = pd.read_csv('../submissions/submission_baseline_evaluation.csv').drop(['Unnamed: 0'], axis=1)\n",
    "pd.concat([sub_validation, sub_evaluation]).to_csv(SUBMISSION_BASE_PATH + f'submission_lgb_ensemble{exclude_columns}.csv', index=False)\n",
    "del sub_validation; del sub_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation Prediction, we can compute WRMSSE locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 23:50:30 - utils - INFO - reading cross validation template\n",
      "2023-08-10 23:50:43 - utils - INFO - reading prediction file\n",
      "2023-08-10 23:50:43 - utils - INFO - merging both files\n",
      "2023-08-10 23:51:12 - utils - INFO - reading weights file\n",
      "2023-08-10 23:51:14 - utils - INFO - Level1 - 1.0359742905071732\n",
      "2023-08-10 23:51:17 - utils - INFO - Level2 - 1.000830124399351\n",
      "2023-08-10 23:51:20 - utils - INFO - Level3 - 0.9942979228305507\n",
      "2023-08-10 23:51:23 - utils - INFO - Level4 - 0.999022628513321\n",
      "2023-08-10 23:51:27 - utils - INFO - Level5 - 1.0373542381165712\n",
      "2023-08-10 23:51:31 - utils - INFO - Level6 - 0.9802534582278319\n",
      "2023-08-10 23:51:35 - utils - INFO - Level7 - 1.0044214089143895\n",
      "2023-08-10 23:51:40 - utils - INFO - Level8 - 0.9717563379210999\n",
      "2023-08-10 23:51:44 - utils - INFO - Level9 - 0.9787406701482261\n",
      "2023-08-10 23:51:52 - utils - INFO - Level10 - 1.0464210558544118\n",
      "2023-08-10 23:52:13 - utils - INFO - Level11 - 0.9649983926241542\n",
      "2023-08-10 23:53:15 - utils - INFO - Level12 - 0.8920313783869543\n",
      "2023-08-10 23:53:16 - utils - INFO - wrmsse: 0.9921751588703365\n"
     ]
    }
   ],
   "source": [
    "cross_validation_on_validation_set(apply_max_zero=False, df_sub_val=df_sub_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Beneath can be used to create submission template\n",
    "The submission template can be used to quickly insert your predictions.\n",
    "It also contains all other (historical) sales to be able to compute the WRMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_validation = pd.read_csv(DATA_BASE_PATH + SALES_VALIDATION)\n",
    "# sales_evaluation = pd.read_csv(DATA_BASE_PATH + SALES_EVALUATION)\n",
    "# calendar = pd.read_csv(DATA_BASE_PATH + CALENDAR)\n",
    "# sell_prices = pd.read_csv(DATA_BASE_PATH + SELL_PRICES)\n",
    "\n",
    "# df_val, submission_idx_val = data_preprocessing(sales_validation, calendar, sell_prices)\n",
    "# del sales_validation\n",
    "# df_eval, submission_idx_eval = data_preprocessing(sales_evaluation, calendar, sell_prices)\n",
    "# del sales_evaluation\n",
    "\n",
    "# df_val_after_release = df_val[(df_val.wm_yr_wk > df_val.release)]# & (df_val[\"sold\"].notna())]\n",
    "# del df_val\n",
    "# df_eval_after_release = df_eval[(df_eval.wm_yr_wk > df_eval.release)]# & (df_eval[\"sold\"].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for validation, combine 'training' df with 'prediction' df\n",
    "# df_sub_merged = pd.merge(\n",
    "#     df_val_after_release,\n",
    "#     df_sub_val,\n",
    "#     how = 'outer',\n",
    "#     on = ['id', 'd']\n",
    "# ).reset_index(drop=True)\n",
    "# df_sub_merged['item_id'] = df_sub_merged['item_id'].fillna(df_sub_merged['id'].apply(lambda x: '_'.join(x.split('_')[0:3])))\n",
    "# df_sub_merged['dept_id'] = df_sub_merged['dept_id'].fillna(df_sub_merged['id'].apply(lambda x: '_'.join(x.split('_')[0:2])))\n",
    "# df_sub_merged['cat_id'] = df_sub_merged['cat_id'].fillna(df_sub_merged['id'].apply(lambda x: x.split('_')[0]))\n",
    "# df_sub_merged['store_id'] = df_sub_merged['store_id'].fillna(df_sub_merged['id'].apply(lambda x: '_'.join(x.split('_')[3:5])))\n",
    "# df_sub_merged['state_id'] = df_sub_merged['state_id'].fillna(df_sub_merged['id'].apply(lambda x: x.split('_')[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge 'true' sold values on dataframe, even for the 'out of sample' ones and sort for safety again\n",
    "# df_sub_merged = merge_eval_sold_on_df(df_sub_merged, df_eval = df_eval)\n",
    "# df_sub_merged = sort_df_on_d(df_sub_merged)\n",
    "# df_sub_merged.to_csv('../submissions/base_cross_validation_template.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
