{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from utils.utils import _down_cast, data_preprocessing, diff_lists, log_status\n",
    "from utils.utils import parse_columns_to_string\n",
    "from utils import constants\n",
    "\n",
    "from utils.configure_logger import configure_logger\n",
    "configure_logger()\n",
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file names and paths\n",
    "DATA_BASE_PATH = constants.DATA_BASE_PATH\n",
    "DATA_BASE_PATH_UNCERTAINTY = constants.DATA_BASE_PATH_UNCERTAINTY\n",
    "SALES_EVALUATION = constants.SALES_EVALUATION\n",
    "SALES_VALIDATION = constants.SALES_VALIDATION\n",
    "CALENDAR = constants.CALENDAR\n",
    "SAMPLE_SUBMISSION = constants.SAMPLE_SUBMISSION\n",
    "SELL_PRICES = constants.SELL_PRICES\n",
    "PRECOMPUTED_BASE_PATH = constants.PRECOMPUTED_BASE_PATH\n",
    "\n",
    "DAYS: int = constants.DAYS #28\n",
    "QUANTILES: int = constants.QUANTILES\n",
    "AGG_LEVEL_COLUMNS = constants.AGG_LEVEL_COLUMNS\n",
    "D_CROSS_VAL_START_LIST = constants.D_CROSS_VAL_START_LIST #[1802, 1830, 1858, 1886, 1914]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all data\n",
    "sales_validation: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SALES_VALIDATION))\n",
    "# sales_evaluation: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SALES_EVALUATION))\n",
    "calendar: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + CALENDAR))\n",
    "sell_prices: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SELL_PRICES))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~25 seconds\n",
    "# 1802, 1830, 1858, 1886, 1914\n",
    "def drop_days_after(df, day_threshold):\n",
    "    \"\"\" \n",
    "    Drop all days after a certain threshold.\n",
    "    By doing this, we ensure that every fold is truly out-of-sample,\n",
    "    because we throw away all future data.\n",
    "    \"\"\"\n",
    "    columns_keep = [c for c in df.columns if c.split('_')[0] != 'd']\n",
    "    columns_keep += [\n",
    "        c for c in \n",
    "            [d for d in df.columns if d.split('_')[0] == 'd'] \n",
    "            if int(c.split('_')[1]) < day_threshold\n",
    "    ]\n",
    "    return df[columns_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_START_VAL = 1914\n",
    "df, submission_idx = data_preprocessing(\n",
    "    drop_days_after(\n",
    "        sales_validation,#.iloc[:int(1000000)],\n",
    "        day_threshold = D_START_VAL\n",
    "    ), \n",
    "    calendar,\n",
    "    sell_prices\n",
    ")\n",
    "df = df[(df.wm_yr_wk > df.release)]\n",
    "df['id'] = df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_status\n",
    "def compute_features(df: pd.DataFrame, group_columns: list, q: int = None, sparse_features: bool = False, agg_level: str = None):\n",
    "    \"\"\"\n",
    "    Type of features computed:\n",
    "     - auto_lagged_values\n",
    "     - auto_sold_ma/ewm/std/qtile\n",
    "     - price_momentum\n",
    "     - price_uncond\n",
    "     - price_auto_std (rolling price std.)\n",
    "    \"\"\"\n",
    "    # drop all NaT dates\n",
    "    idx = df['date'].notna()\n",
    "    df = df[idx]\n",
    "    \n",
    "    # these columns should always be included, because they are required \n",
    "    # in future groupings during training and computing the WSPL\n",
    "    feature_columns = []\n",
    "    for c in ['id', 'state_id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'sold']:\n",
    "        if c in df:\n",
    "            feature_columns += [c]\n",
    "            \n",
    "    # to be sure\n",
    "    df['month'] = df['month'].astype(int)\n",
    "    \n",
    "    # to ensure the data is in the correct order, we need d_{day} as an integer\n",
    "    df[\"index\"] = df.index\n",
    "    df[\"d_int\"] = df[\"d\"].apply(lambda x: int(x.split(\"_\")[-1]))\n",
    "    df = df.sort_values('d_int')\n",
    "    df[\"day\"] = df.date.dt.day # the day of the month\n",
    "    \n",
    "    # to use groupby only once, lower runtime\n",
    "    df_grouped = df.groupby(group_columns)\n",
    "    \n",
    "    ################################################\n",
    "    ############### AUTOCORRELATION ################\n",
    "    ################################################\n",
    "    PREFIX = 'auto_'\n",
    "    logger.info('Computing autocorrelation features')\n",
    "    # DIRECT LAGGED VALUES\n",
    "    old_columns: set = set(df.columns)\n",
    "    LAG_SHIFT: int = 1\n",
    "    for lag in [1, 2, 7, 14, 28, 56]:\n",
    "        if sparse_features: continue\n",
    "        df[PREFIX + f\"sold_{lag}\"] = df_grouped[\"sold\"].shift(lag) # 1-day lag\n",
    "    feature_columns += diff_lists(df.columns, old_columns)\n",
    "    \n",
    "    # ROLLING EWM/MA/STD/QUANTILE\n",
    "    old_columns: set = set(df.columns)\n",
    "    for i in [3,7,14,21,28,56,112,168]:\n",
    "        if sparse_features and i != 28: continue\n",
    "        min_periods = int(np.ceil(i ** 0.8))\n",
    "        \n",
    "        df[PREFIX + f'sold_ma_{i}'] = df_grouped['sold'].transform(lambda x: x.shift(LAG_SHIFT).rolling(i, min_periods).mean()).astype(np.float16)\n",
    "        df[PREFIX + f'sold_std_{i}'] = df_grouped['sold'].transform(lambda x: x.shift(LAG_SHIFT).rolling(i, min_periods).std()).astype(np.float16)\n",
    "        df[PREFIX + f'sold_ewm_{i}'] = df_grouped['sold']\\\n",
    "            .transform(lambda x: x.shift(LAG_SHIFT)\n",
    "            .ewm(span=i, min_periods = min_periods)\n",
    "            .mean())\\\n",
    "            .astype(np.float16)\n",
    "\n",
    "        for quantile in [0.01, 0.1, 0.25, .5, 0.75, 0.9, 0.99]:#QUANTILES:\n",
    "            if i * min(quantile, 1-quantile) >= 1:\n",
    "                if sparse_features and i!=28:continue\n",
    "                df[PREFIX + f'sold_qtile_{i}_{quantile}'] = df_grouped['sold'].transform(lambda x: x.shift(LAG_SHIFT).rolling(i, min_periods).quantile(quantile)).astype(np.float16)\n",
    "    feature_columns += diff_lists(df.columns, old_columns)\n",
    "    \n",
    "    ###############################################\n",
    "    ############ PRICE AUTOCORRELATION ############\n",
    "    ###############################################\n",
    "    logger.info('Computing price autocorrelation features')\n",
    "    \n",
    "    # SIMPLY THE CURRENT PRICE\n",
    "    # 'temp_id' is used for grouping at Level1/Total\n",
    "    # Unconditional features for univariate level would not make sense when using LightGBM\n",
    "    if 'temp_id' not in group_columns:\n",
    "        old_columns = set(df.columns)\n",
    "        df_grouped_d = df.groupby(group_columns + ['d'])\n",
    "        df['price_uncond_avg'] = df_grouped_d['sell_price'].transform(lambda x: x.mean()).astype(np.float32)\n",
    "        df['price_uncond_std'] = df_grouped_d['sell_price'].transform(lambda x: x.std()).astype(np.float32)\n",
    "        df['price_uncond_median'] = df_grouped_d['sell_price'].transform(lambda x: x.median()).astype(np.float32)\n",
    "        feature_columns += diff_lists(df.columns, old_columns)\n",
    "    \n",
    "    # to prevent type issues when storing dataframe as .parquet file\n",
    "    for c in ['wm_yr_wk', 'year', 'month']:\n",
    "        df[c] = df[c].astype('int32')\n",
    "        \n",
    "    # PRICE DIFFERENCES BY WEEK/MONTH/YEAR (USSUALLY ON)\n",
    "    PREFIX = 'price_momentum_'\n",
    "    old_columns = set(df.columns)\n",
    "    df[PREFIX + 'w'] = df['sell_price'] / df.groupby(group_columns + ['wm_yr_wk'])['sell_price'].transform(lambda x: x.shift(LAG_SHIFT)).astype(np.float32)\n",
    "    df[PREFIX + 'm'] = df['sell_price'] / df.groupby(group_columns + ['year', 'month'])['sell_price'].transform(lambda x: x.shift(LAG_SHIFT)).astype(np.float32)\n",
    "    df[PREFIX + 'y'] = df['sell_price'] / df.groupby(group_columns + ['year'])['sell_price'].transform(lambda x: x.shift(LAG_SHIFT)).astype(np.float32)\n",
    "    feature_columns += diff_lists(df.columns, old_columns)\n",
    "    \n",
    "    # VARIATION OF PRICES\n",
    "    PREFIX = 'price_auto_'\n",
    "    old_columns = set(df.columns)\n",
    "    for i in [28, 56, 112]:\n",
    "        min_periods = int(np.ceil(i ** 0.8))\n",
    "        df[PREFIX + f'std_{int(i)}'] = df_grouped['sell_price'].transform(lambda x: x.rolling(i).std()).astype(np.float16)\n",
    "    feature_columns += diff_lists(df.columns, old_columns)\n",
    "\n",
    "    ################################################\n",
    "    ############## SEASONAL FEATURES ###############\n",
    "    ################################################\n",
    "    logger.info('Seasonal features to category')\n",
    "    # set seasonal features as type 'category'\n",
    "    # LightGBM will automatically 'pick them up' and encode\n",
    "    PREFIX = 'seasonal_'\n",
    "    df[PREFIX + 'weekday'] = df['weekday'].astype('category')\n",
    "    feature_columns += [PREFIX + 'weekday']\n",
    "    df[PREFIX + 'monthday'] = df['day'].astype('category')\n",
    "    feature_columns += [PREFIX + 'monthday']\n",
    "    df[PREFIX + 'month'] = df['month'].astype('category')\n",
    "    feature_columns += [PREFIX + 'month']\n",
    "    \n",
    "    ################################################\n",
    "    ################ OTHER FEATURES ################\n",
    "    ################################################\n",
    "    # THIS FEATURE IS ONLY RELEVANT AT LOWER LEVELS WHERE 0 SALES OCCUR\n",
    "    if agg_level in ['Level10', 'Level11', 'Level12']:\n",
    "        logger.info('Computing PCT of non-zero days')\n",
    "        old_columns = set(df.columns)\n",
    "        for i in [7, 14, 28, 28*2, 28*4]:\n",
    "            if sparse_features and i != 28: continue\n",
    "            df[f'sold_pct_nonzero_{i}'] = df_grouped['sold'].transform(lambda x: (x!=0).rolling(i).mean().shift(LAG_SHIFT)).astype(np.float16)\n",
    "        feature_columns += diff_lists(df.columns, old_columns)\n",
    "    \n",
    "    ################################################\n",
    "    ############### STATE/STORE/CAT ################\n",
    "    ################################################\n",
    "    # temporary memory savings\n",
    "    df = _down_cast(df)\n",
    "    \n",
    "    logger.info('Create state_id if store_id in group columns')\n",
    "    if 'store_id' in group_columns:\n",
    "        df['state_id'] = df['store_id'].str.split('_').apply(lambda x: x[0])   \n",
    "        feature_columns += ['state_id']\n",
    "    if 'state_id' in group_columns:\n",
    "        df['state_id'] = df['state_id'].astype('category')\n",
    "    if 'store_id' in group_columns:\n",
    "        df['store_id'] = df['store_id'].astype('category')\n",
    "    \n",
    "    ################################################\n",
    "    ############## TARGET ENGINEERING ##############\n",
    "    ################################################\n",
    "    # The forecasting period is 28 days. We do not want to do recursive forecasting,\n",
    "    # i.e. reusing forecasts and recomputing features to predict another day ahead and so on. \n",
    "    # Instead, we pass the days of forecasting as a feature in the model.\n",
    "    # For the training data, this forecasting period is randomized so the model\n",
    "    # is trained to predict various days ahead.\n",
    "    # For the out-of-sample period, the features are exactly the same,\n",
    "    # except of course for the number of days ahead (i.e. increasing from 1 to 28)\n",
    "    # The code for this is quite ugly, but it works.\n",
    "    \n",
    "    mapping_dict = {int(D_START_VAL+i):i for i in range(DAYS)}\n",
    "    def map_d_to_days_fwd(row):\n",
    "        \"\"\" \n",
    "        This functions is applied on the dataframe.\n",
    "        For the prediction period, the random integers between 1,28\n",
    "        will be replaced by 1 to 28 linearly. For the other rows,\n",
    "        the random number is returned.\n",
    "        \"\"\"\n",
    "        return mapping_dict.get(row['d_int'], row['days_fwd'])\n",
    "\n",
    "    # We assign every target a random number of days ahead.\n",
    "    # Then we 'match' the row of features that belongs to it.\n",
    "    # Example: say we have a sold target recorded at day 1850, \n",
    "    # and the randomly assigned days ahead is 17,\n",
    "    # then we will match this target with the row of features at day 1833.\n",
    "    days_forecast = np.random.randint(low=0, high=DAYS, size=len(df))\n",
    "    \n",
    "    # Create df with solely the target and date\n",
    "    # These columns belong to the target and should not be shifted randomly\n",
    "    columns_temp = ['sold', 'd']\\\n",
    "        + ['seasonal_weekday', 'seasonal_month', 'seasonal_monthday']\\\n",
    "        + ['price_momentum_'+i for i in ['w', 'm', 'y']]\\\n",
    "        + ['price_auto_std_'+str(i) for i in [28, 56, 112]]\n",
    "    df_temp: pd.DataFrame = df[columns_temp + group_columns + ['d_int']]\n",
    "    df_temp['d_int'] = df_temp['d_int'].astype(int)\n",
    "    df_temp['days_fwd'] = days_forecast\n",
    "    df_temp['days_fwd'] = df_temp.apply(map_d_to_days_fwd, axis=1)\n",
    "    # days_fwd = pd.to_timedelta(days_forecast, unit='D')\n",
    "    df_temp['d_int'] -= df_temp['days_fwd']\n",
    "    df_temp['d_int'] = df_temp['d_int'].astype(int)\n",
    "    df_temp['d'] = 'd_' + df_temp['d_int'].astype(str)\n",
    "    feature_columns += ['days_fwd']\n",
    "    \n",
    "    # Match features with shifted targets\n",
    "    df = df.drop(columns_temp, axis=1)\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        df_temp,\n",
    "        on = ['d_int'] + group_columns,\n",
    "        how = 'right'\n",
    "    )\n",
    "    # add DAYS back to d_int and d\n",
    "    df['d_int'] += df['days_fwd']\n",
    "    df['d'] = 'd_' + df['d_int'].astype(str)\n",
    "    \n",
    "    # drop invalid cases of forecasting windows (i.e. negative d)\n",
    "    idx = df['d_int'] > DAYS\n",
    "    df = df[idx]\n",
    "    ############# RETURN FINAL RESULTS ############\n",
    "    \n",
    "    # return final results\n",
    "    df = _down_cast(df)\n",
    "    return df[feature_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial run if feature_engineering works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 11:33:55 - compute_features - INFO - calling\n",
      "2023-12-04 11:33:55 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:33:55 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:33:59 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:33:59 - __main__ - INFO - Create state_id if store_id in group columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sold</th>\n",
       "      <th>auto_sold_56</th>\n",
       "      <th>auto_sold_1</th>\n",
       "      <th>auto_sold_2</th>\n",
       "      <th>auto_sold_28</th>\n",
       "      <th>auto_sold_7</th>\n",
       "      <th>auto_sold_14</th>\n",
       "      <th>auto_sold_qtile_168_0.75</th>\n",
       "      <th>...</th>\n",
       "      <th>price_momentum_m</th>\n",
       "      <th>price_momentum_y</th>\n",
       "      <th>price_auto_std_28</th>\n",
       "      <th>price_auto_std_56</th>\n",
       "      <th>price_auto_std_112</th>\n",
       "      <th>seasonal_weekday</th>\n",
       "      <th>seasonal_monthday</th>\n",
       "      <th>seasonal_month</th>\n",
       "      <th>state_id</th>\n",
       "      <th>days_fwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>CA_2</td>\n",
       "      <td>d_29</td>\n",
       "      <td>3512.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>2064.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998047</td>\n",
       "      <td>0.998047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>CA</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>WI_2</td>\n",
       "      <td>d_29</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.004883</td>\n",
       "      <td>1.004883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>WI</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>TX_3</td>\n",
       "      <td>d_29</td>\n",
       "      <td>2392.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.004883</td>\n",
       "      <td>1.004883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>TX</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>CA_3</td>\n",
       "      <td>d_29</td>\n",
       "      <td>4760.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3264.0</td>\n",
       "      <td>3044.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5168.0</td>\n",
       "      <td>5416.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002930</td>\n",
       "      <td>1.002930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>CA</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>CA_4</td>\n",
       "      <td>d_29</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1915.0</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.007812</td>\n",
       "      <td>1.007812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>CA</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19335</th>\n",
       "      <td>WI_2</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4360.0</td>\n",
       "      <td>5128.0</td>\n",
       "      <td>5404.0</td>\n",
       "      <td>4256.0</td>\n",
       "      <td>4348.0</td>\n",
       "      <td>6228.0</td>\n",
       "      <td>5476.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>WI</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19336</th>\n",
       "      <td>WI_3</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3252.0</td>\n",
       "      <td>4324.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>2964.0</td>\n",
       "      <td>3556.0</td>\n",
       "      <td>4044.0</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>WI</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19337</th>\n",
       "      <td>CA_3</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6292.0</td>\n",
       "      <td>7720.0</td>\n",
       "      <td>7420.0</td>\n",
       "      <td>5488.0</td>\n",
       "      <td>6068.0</td>\n",
       "      <td>6580.0</td>\n",
       "      <td>6592.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>CA</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19338</th>\n",
       "      <td>WI_1</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3452.0</td>\n",
       "      <td>4872.0</td>\n",
       "      <td>4772.0</td>\n",
       "      <td>3040.0</td>\n",
       "      <td>3236.0</td>\n",
       "      <td>3492.0</td>\n",
       "      <td>4340.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>WI</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19339</th>\n",
       "      <td>CA_4</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2636.0</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>2954.0</td>\n",
       "      <td>2444.0</td>\n",
       "      <td>2808.0</td>\n",
       "      <td>2536.0</td>\n",
       "      <td>2642.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>CA</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19130 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      store_id       d    sold  auto_sold_56  auto_sold_1  auto_sold_2  \\\n",
       "210       CA_2    d_29  3512.0           NaN       1844.0       2064.0   \n",
       "211       WI_2    d_29  1954.0           NaN          NaN          NaN   \n",
       "212       TX_3    d_29  2392.0           NaN       1542.0       1763.0   \n",
       "213       CA_3    d_29  4760.0           NaN       3264.0       3044.0   \n",
       "214       CA_4    d_29  1636.0           NaN       1915.0       1634.0   \n",
       "...        ...     ...     ...           ...          ...          ...   \n",
       "19335     WI_2  d_1941     0.0        4360.0       5128.0       5404.0   \n",
       "19336     WI_3  d_1941     0.0        3252.0       4324.0       4688.0   \n",
       "19337     CA_3  d_1941     0.0        6292.0       7720.0       7420.0   \n",
       "19338     WI_1  d_1941     0.0        3452.0       4872.0       4772.0   \n",
       "19339     CA_4  d_1941     0.0        2636.0       3272.0       2954.0   \n",
       "\n",
       "       auto_sold_28  auto_sold_7  auto_sold_14  auto_sold_qtile_168_0.75  ...  \\\n",
       "210             NaN          NaN           NaN                       NaN  ...   \n",
       "211             NaN          NaN           NaN                       NaN  ...   \n",
       "212             NaN          NaN           NaN                       NaN  ...   \n",
       "213             NaN       5168.0        5416.0                       NaN  ...   \n",
       "214             NaN       1501.0           NaN                       NaN  ...   \n",
       "...             ...          ...           ...                       ...  ...   \n",
       "19335        4256.0       4348.0        6228.0                    5476.0  ...   \n",
       "19336        2964.0       3556.0        4044.0                    4300.0  ...   \n",
       "19337        5488.0       6068.0        6580.0                    6592.0  ...   \n",
       "19338        3040.0       3236.0        3492.0                    4340.0  ...   \n",
       "19339        2444.0       2808.0        2536.0                    2642.0  ...   \n",
       "\n",
       "       price_momentum_m  price_momentum_y  price_auto_std_28  \\\n",
       "210            0.998047          0.998047                NaN   \n",
       "211            1.004883          1.004883                NaN   \n",
       "212            1.004883          1.004883                NaN   \n",
       "213            1.002930          1.002930                NaN   \n",
       "214            1.007812          1.007812                NaN   \n",
       "...                 ...               ...                ...   \n",
       "19335          1.000000          1.000000           0.000990   \n",
       "19336          1.000000          1.000000           0.002609   \n",
       "19337          1.000000          1.000000           0.001584   \n",
       "19338          1.000000          1.000000           0.001319   \n",
       "19339          1.000000          1.000000           0.000369   \n",
       "\n",
       "       price_auto_std_56  price_auto_std_112  seasonal_weekday  \\\n",
       "210                  NaN                 NaN          Saturday   \n",
       "211                  NaN                 NaN          Saturday   \n",
       "212                  NaN                 NaN          Saturday   \n",
       "213                  NaN                 NaN          Saturday   \n",
       "214                  NaN                 NaN          Saturday   \n",
       "...                  ...                 ...               ...   \n",
       "19335           0.001777            0.001838            Sunday   \n",
       "19336           0.002550            0.002495            Sunday   \n",
       "19337           0.003113            0.004791            Sunday   \n",
       "19338           0.001660            0.002312            Sunday   \n",
       "19339           0.000406            0.001730            Sunday   \n",
       "\n",
       "       seasonal_monthday  seasonal_month  state_id  days_fwd  \n",
       "210                   26               2        CA        16  \n",
       "211                   26               2        WI        21  \n",
       "212                   26               2        TX        15  \n",
       "213                   26               2        CA         7  \n",
       "214                   26               2        CA        12  \n",
       "...                  ...             ...       ...       ...  \n",
       "19335                 22               5        WI        27  \n",
       "19336                 22               5        WI        27  \n",
       "19337                 22               5        CA        27  \n",
       "19338                 22               5        WI        27  \n",
       "19339                 22               5        CA        27  \n",
       "\n",
       "[19130 rows x 85 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total ~77 seconds\n",
    "D_START_VAL = 1914\n",
    "level = 'Level3'\n",
    "\n",
    "if level == 'Level1':\n",
    "    df['temp_id'] = 'temp_id'\n",
    "agg_columns = AGG_LEVEL_COLUMNS[level] if level != 'Level1' else ['temp_id']\n",
    "agg_dict = {\n",
    "    'sold': np.nansum,\n",
    "    'sell_price': np.nanmean,\n",
    "    'date': 'last',\n",
    "    'weekday': 'last',\n",
    "    'month': 'last',\n",
    "    'year': 'last',\n",
    "    'wm_yr_wk': 'last'\n",
    "}\n",
    "features = compute_features(\n",
    "    df.groupby(agg_columns + ['d']).agg(agg_dict).reset_index(drop=False),\n",
    "    agg_columns,\n",
    "    sparse_features=False,\n",
    "    agg_level=level\n",
    ")\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that for the prediction period,\n",
    "# days forward is not random but 0 to DAYS-1, increasing linearly\n",
    "for i in range(DAYS):\n",
    "    assert len(features[features['d'] == f'd_{int(D_START_VAL+i)}']['days_fwd'].unique())==1\n",
    "    assert features[features['d'] == f'd_{int(D_START_VAL+i)}']['days_fwd'].unique()[0]==i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Runs Feature Engineering for Validation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_status\n",
    "def groupby_agglevel(df: pd.DataFrame, agg_columns: list, agg_dict: dict):\n",
    "    return df.groupby(agg_columns).agg(agg_dict).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 11:36:58 - __main__ - INFO - D_START_VAL: 1802\n",
      "2023-12-04 11:37:02 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-12-04 11:37:02 - compute_features - INFO - calling\n",
      "2023-12-04 11:37:02 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:37:02 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:37:02 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:37:02 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:37:02 - compute_features - INFO - calling\n",
      "2023-12-04 11:37:02 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:37:02 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:37:04 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:37:04 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:37:04 - compute_features - INFO - calling\n",
      "2023-12-04 11:37:04 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:37:04 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:37:08 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:37:08 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:37:08 - compute_features - INFO - calling\n",
      "2023-12-04 11:37:08 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:37:08 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:37:10 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:37:10 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:37:10 - compute_features - INFO - calling\n",
      "2023-12-04 11:37:10 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:37:10 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:37:13 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:37:13 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:37:13 - compute_features - INFO - calling\n",
      "2023-12-04 11:37:13 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:37:13 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:37:17 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:37:17 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:37:17 - compute_features - INFO - calling\n",
      "2023-12-04 11:37:17 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:37:18 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:37:27 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:37:27 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:37:27 - compute_features - INFO - calling\n",
      "2023-12-04 11:37:27 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:37:28 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:37:40 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:37:40 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:37:41 - compute_features - INFO - calling\n",
      "2023-12-04 11:37:41 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:37:43 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:38:10 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:38:10 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:38:11 - __main__ - INFO - D_START_VAL: 1830\n",
      "2023-12-04 11:38:14 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-12-04 11:38:15 - compute_features - INFO - calling\n",
      "2023-12-04 11:38:15 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:38:15 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:38:15 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:38:15 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:38:15 - compute_features - INFO - calling\n",
      "2023-12-04 11:38:15 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:38:15 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:38:16 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:38:16 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:38:16 - compute_features - INFO - calling\n",
      "2023-12-04 11:38:16 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:38:16 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:38:21 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:38:21 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:38:21 - compute_features - INFO - calling\n",
      "2023-12-04 11:38:21 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:38:21 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:38:22 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:38:22 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:38:22 - compute_features - INFO - calling\n",
      "2023-12-04 11:38:22 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:38:23 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:38:26 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:38:26 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:38:26 - compute_features - INFO - calling\n",
      "2023-12-04 11:38:26 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:38:26 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:38:30 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:38:30 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:38:30 - compute_features - INFO - calling\n",
      "2023-12-04 11:38:30 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:38:31 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:38:40 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:38:40 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:38:41 - compute_features - INFO - calling\n",
      "2023-12-04 11:38:41 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:38:42 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:38:56 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:38:56 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:38:56 - compute_features - INFO - calling\n",
      "2023-12-04 11:38:56 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:38:59 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:39:28 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:39:29 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:39:30 - __main__ - INFO - D_START_VAL: 1858\n",
      "2023-12-04 11:39:34 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-12-04 11:39:34 - compute_features - INFO - calling\n",
      "2023-12-04 11:39:34 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:39:34 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:39:34 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:39:34 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:39:34 - compute_features - INFO - calling\n",
      "2023-12-04 11:39:34 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:39:34 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:39:36 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:39:36 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:39:36 - compute_features - INFO - calling\n",
      "2023-12-04 11:39:36 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:39:36 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:39:40 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:39:41 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:39:41 - compute_features - INFO - calling\n",
      "2023-12-04 11:39:41 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:39:41 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:39:42 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:39:42 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:39:42 - compute_features - INFO - calling\n",
      "2023-12-04 11:39:42 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:39:43 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:39:46 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:39:46 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:39:46 - compute_features - INFO - calling\n",
      "2023-12-04 11:39:46 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:39:47 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:39:51 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:39:51 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:39:51 - compute_features - INFO - calling\n",
      "2023-12-04 11:39:51 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:39:52 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:40:01 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:40:01 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:40:01 - compute_features - INFO - calling\n",
      "2023-12-04 11:40:01 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:40:02 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:40:16 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:40:16 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:40:16 - compute_features - INFO - calling\n",
      "2023-12-04 11:40:16 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:40:20 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:40:49 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:40:50 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:40:51 - __main__ - INFO - D_START_VAL: 1886\n",
      "2023-12-04 11:40:55 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-12-04 11:40:55 - compute_features - INFO - calling\n",
      "2023-12-04 11:40:55 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:40:56 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:40:56 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:40:56 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:40:56 - compute_features - INFO - calling\n",
      "2023-12-04 11:40:56 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:40:56 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:40:57 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:40:57 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:40:57 - compute_features - INFO - calling\n",
      "2023-12-04 11:40:57 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:40:58 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:41:02 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:41:02 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:41:02 - compute_features - INFO - calling\n",
      "2023-12-04 11:41:02 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:41:03 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:41:04 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:41:04 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:41:04 - compute_features - INFO - calling\n",
      "2023-12-04 11:41:04 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:41:04 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:41:07 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:41:07 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:41:07 - compute_features - INFO - calling\n",
      "2023-12-04 11:41:07 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:41:08 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:41:12 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:41:12 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:41:12 - compute_features - INFO - calling\n",
      "2023-12-04 11:41:12 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:41:13 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:41:22 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:41:22 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:41:22 - compute_features - INFO - calling\n",
      "2023-12-04 11:41:22 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:41:23 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:41:36 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:41:36 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:41:36 - compute_features - INFO - calling\n",
      "2023-12-04 11:41:36 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:41:39 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:42:07 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:42:07 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:42:08 - __main__ - INFO - D_START_VAL: 1914\n",
      "2023-12-04 11:42:12 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-12-04 11:42:12 - compute_features - INFO - calling\n",
      "2023-12-04 11:42:12 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:42:12 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:42:12 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:42:12 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:42:12 - compute_features - INFO - calling\n",
      "2023-12-04 11:42:12 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:42:12 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:42:13 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:42:13 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:42:13 - compute_features - INFO - calling\n",
      "2023-12-04 11:42:13 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:42:14 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:42:18 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:42:18 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:42:18 - compute_features - INFO - calling\n",
      "2023-12-04 11:42:18 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:42:18 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:42:19 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:42:19 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:42:20 - compute_features - INFO - calling\n",
      "2023-12-04 11:42:20 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:42:20 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:42:23 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:42:23 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:42:23 - compute_features - INFO - calling\n",
      "2023-12-04 11:42:23 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:42:23 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:42:28 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:42:28 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:42:28 - compute_features - INFO - calling\n",
      "2023-12-04 11:42:28 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:42:29 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:42:40 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:42:40 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:42:41 - compute_features - INFO - calling\n",
      "2023-12-04 11:42:41 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:42:42 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:42:56 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:42:56 - __main__ - INFO - Create state_id if store_id in group columns\n",
      "2023-12-04 11:42:56 - compute_features - INFO - calling\n",
      "2023-12-04 11:42:56 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-12-04 11:42:59 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-12-04 11:43:30 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-12-04 11:43:30 - __main__ - INFO - Create state_id if store_id in group columns\n"
     ]
    }
   ],
   "source": [
    "# params for un\n",
    "SPARSE_FEATURES = False\n",
    "TEST_RUN = False\n",
    "MAX_AGG_LEVEL = 3 \n",
    "# this makes level indicates to only \n",
    "# compute for aggregation levels for\n",
    "# Level1, Level2 ... Level{12-MAX_AGG_LEVEl}\n",
    "# i.e. MAX_AGG_LEVEL=3 results in;\n",
    "# Level1, Level2, ..., Level9\n",
    "\n",
    "# compute features+targets in each fold (targets are stored in the same df)\n",
    "for D_START_VAL in D_CROSS_VAL_START_LIST:\n",
    "    logger.info(f'D_START_VAL: {D_START_VAL}')\n",
    "\n",
    "    # the grouped df's can be precomputed, which saves time\n",
    "    # here we check if this is the case\n",
    "    all_found = True\n",
    "    for agg_level in AGG_LEVEL_COLUMNS:\n",
    "        try:\n",
    "            aaaaa = pd.read_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet')\n",
    "        except:\n",
    "            all_found = False\n",
    "            logger.info(f'grouped df not computed for level: {agg_level}')\n",
    "        \n",
    "    if not all_found:\n",
    "        # pivot initial dataframe and compute features/targets\n",
    "        df_val, submission_idx_validation = data_preprocessing(\n",
    "            drop_days_after(sales_validation,\n",
    "            day_threshold = D_START_VAL), \n",
    "            calendar,\n",
    "            sell_prices\n",
    "        )\n",
    "        # drop all leading rows with leading zeros for each product\n",
    "        df_val_after_release = df_val[(df_val.wm_yr_wk > df_val.release)]\n",
    "        del df_val\n",
    "    else:\n",
    "        logger.info('grouped df computed for all levels, original dataframe is not loaded')\n",
    "\n",
    "    # for each aggregation level\n",
    "    for agg_level, agg_columns in AGG_LEVEL_COLUMNS.items(): \n",
    "        # remove index to compute all\n",
    "        if agg_level in [f'Level{int(12 - i)}' for i in range(0,MAX_AGG_LEVEL)]:\n",
    "            continue\n",
    "            \n",
    "        # group data for specific grouping columns per level\n",
    "        # and compute features\n",
    "        agg_dict = {\n",
    "            'sold': np.nansum,\n",
    "            'sell_price': np.nanmean,\n",
    "            'date': 'last',\n",
    "            'weekday': 'last',\n",
    "            'month': 'last',\n",
    "            'year': 'last',\n",
    "            'wm_yr_wk': 'last'\n",
    "        }\n",
    "\n",
    "        if len(agg_columns) == 0:\n",
    "            agg_columns = ['temp_id']\n",
    "        \n",
    "        # load grouped df if already exists\n",
    "        try:\n",
    "            aaaaa = pd.read_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet')\n",
    "            aaaaa = _down_cast(aaaaa)\n",
    "        except:\n",
    "            logger.info(f'not existing yet: ../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet')\n",
    "            logger.info(f'computing and storing grouped dataframe for level: {agg_level}')\n",
    "            # get data on aggregated level\n",
    "            if len(agg_columns) == 0 or 'temp_id' in agg_columns:\n",
    "                df_val_after_release['temp_id'] = 'temp_id'\n",
    "\n",
    "            aaaaa = groupby_agglevel(df_val_after_release, agg_columns + ['d'], agg_dict)\n",
    "            idx_keep = aaaaa['date'].notna()\n",
    "            aaaaa = aaaaa[idx_keep]\n",
    "            # to suitable type for .parquet\n",
    "            for c in aaaaa.columns:\n",
    "                if c not in agg_columns + ['d','date','weekday']:#['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'state_id']:\n",
    "                    aaaaa[c] = aaaaa[c].astype(np.float32)\n",
    "            aaaaa.to_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet', index=False)\n",
    "            aaaaa = _down_cast(aaaaa)\n",
    "            \n",
    "        # compute all features\n",
    "        features = compute_features(\n",
    "            aaaaa,\n",
    "            agg_columns,\n",
    "            sparse_features=SPARSE_FEATURES,\n",
    "            agg_level=agg_level\n",
    "        )\n",
    "        \n",
    "        # to suitable type for .parquet file storage\n",
    "        for c in features.columns:\n",
    "            if c not in ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'state_id', 'seasonal_weekday', 'seasonal_monthday', 'seasonal_month']:\n",
    "                features[c] = features[c].astype(np.float32)\n",
    "        \n",
    "        # format string and save file\n",
    "        agg_string = parse_columns_to_string(agg_columns)\n",
    "        if not TEST_RUN:\n",
    "            features.to_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/features/' + f'features_val_{agg_string}.parquet', index=False)\n",
    "            del features\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # in the case of a test run,\n",
    "    # we might want to verify if the computed dataframe is correct\n",
    "    # else, remove the df to reduce memory usage\n",
    "    if not TEST_RUN:\n",
    "        try:\n",
    "            del df_val_after_release\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pivot initial dataframe and compute features/targets\n",
    "# df_eval, submission_idx_validation = data_preprocessing(sales_evaluation, calendar, sell_prices)\n",
    "# df_eval_after_release = df_eval[(df_eval.wm_yr_wk > df_eval.release)]\n",
    "# del df_eval\n",
    "\n",
    "# # set prediction values to nan values\n",
    "# pred_index = df_eval_after_release['d'].isin(D_CV_OOS)\n",
    "# df_eval_after_release.loc[pred_index, 'sold'] = np.nan\n",
    "# del pred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_RUN = False\n",
    "# for agg_level in AGG_LEVEL_COLUMNS:\n",
    "#     agg_columns = AGG_LEVEL_COLUMNS[agg_level]\n",
    "#     # get data on aggregated level\n",
    "#     if len(agg_columns) == 0:\n",
    "#         df_eval_after_release['temp_id'] = 'temp_id'\n",
    "#         agg_columns = ['temp_id']\n",
    "#     agg_dict = {\n",
    "#         'sold': np.nansum,\n",
    "#         'date': 'last',\n",
    "#         'weekday': 'last',\n",
    "#         'month': 'last'\n",
    "#     }\n",
    "#     features = compute_features(\n",
    "#         groupby_agglevel(df_eval_after_release, agg_columns + ['d'], agg_dict),\n",
    "#         agg_columns\n",
    "#     )\n",
    "    \n",
    "#     # to suitable format for .parquet\n",
    "#     for c in features.columns:\n",
    "#         if c not in ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'state_id']:\n",
    "#             features[c] = features[c].astype(np.float32)\n",
    "        \n",
    "#     # format string and save file\n",
    "#     agg_string = parse_columns_to_string(agg_columns)\n",
    "#     if not TEST_RUN:\n",
    "#         features.to_parquet(PRECOMPUTED_BASE_PATH + f'features_eval_{agg_string}.parquet', index=False)\n",
    "#         del features\n",
    "#     else:\n",
    "#         features.to_parquet(PRECOMPUTED_BASE_PATH + f'/test/features_eval_{agg_string}.parquet', index=False)\n",
    "        \n",
    "# if not TEST_RUN: \n",
    "#     del df_eval_after_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
