{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from utils.utils import merge_eval_sold_on_df, sort_df_on_d, WRMSSE, RMSSE, _down_cast, data_preprocessing, diff_lists, log_status #create_submission_df\n",
    "from utils.utils import customIter, parse_columns_to_string\n",
    "from utils import constants\n",
    "\n",
    "from utils.configure_logger import configure_logger\n",
    "configure_logger()\n",
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = constants.DATA_BASE_PATH #'../data/m5-forecasting-accuracy/'\n",
    "DATA_BASE_PATH_UNCERTAINTY = constants.DATA_BASE_PATH_UNCERTAINTY #'../data/m5-forecasting-uncertainty/'\n",
    "SALES_EVALUATION = constants.SALES_EVALUATION #'sales_train_evaluation.csv'\n",
    "SALES_VALIDATION = constants.SALES_VALIDATION #'sales_train_validation.csv'\n",
    "CALENDAR = constants.CALENDAR #'calendar.csv'\n",
    "SAMPLE_SUBMISSION = constants.SAMPLE_SUBMISSION #'sample_submission.csv'\n",
    "SELL_PRICES = constants.SELL_PRICES #'sell_prices.csv'\n",
    "\n",
    "PRECOMPUTED_BASE_PATH = constants.PRECOMPUTED_BASE_PATH #'../data/uncertainty/features/'\n",
    "\n",
    "DAYS: int = constants.DAYS #28\n",
    "QUANTILES: int = constants.QUANTILES #[0.005, 0.025, 0.165, 0.25, 0.50, 0.75, 0.835, 0.975, 0.995]\n",
    "\n",
    "AGG_LEVEL_COLUMNS = constants.AGG_LEVEL_COLUMNS\n",
    "D_CROSS_VAL_START_LIST = constants.D_CROSS_VAL_START_LIST #[1802, 1830, 1858, 1886, 1914]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m calendar: pd\u001b[39m.\u001b[39mDataFrame \u001b[39m=\u001b[39m _down_cast(pd\u001b[39m.\u001b[39mread_csv(DATA_BASE_PATH \u001b[39m+\u001b[39m CALENDAR))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# sample_submission: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SAMPLE_SUBMISSION))\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m sell_prices: pd\u001b[39m.\u001b[39mDataFrame \u001b[39m=\u001b[39m _down_cast(pd\u001b[39m.\u001b[39;49mread_csv(DATA_BASE_PATH \u001b[39m+\u001b[39;49m SELL_PRICES))\n",
      "File \u001b[0;32m~/projects/m5-forecasting-uncertainty/code/utils/utils.py:177\u001b[0m, in \u001b[0;36m_down_cast\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    175\u001b[0m             df[cols[i]] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[cols[i]], \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    176\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m             df[cols[i]] \u001b[39m=\u001b[39m df[cols[i]]\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mcategory\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/generic.py:6324\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6317\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m   6318\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m   6319\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m   6320\u001b[0m     ]\n\u001b[1;32m   6322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6323\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6324\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   6325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6327\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/internals/managers.py:451\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    449\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    452\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    454\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    455\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    456\u001b[0m     using_cow\u001b[39m=\u001b[39;49musing_copy_on_write(),\n\u001b[1;32m    457\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    353\u001b[0m     result_blocks \u001b[39m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    355\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfrom_blocks(result_blocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/internals/blocks.py:511\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m--> 511\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m    513\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    515\u001b[0m refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/dtypes/astype.py:242\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    239\u001b[0m     dtype \u001b[39m=\u001b[39m dtype\u001b[39m.\u001b[39mnumpy_dtype\n\u001b[1;32m    241\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    243\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    244\u001b[0m     \u001b[39m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/dtypes/astype.py:187\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    184\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     values \u001b[39m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    189\u001b[0m \u001b[39m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/dtypes/astype.py:84\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# dispatch on extension dtype if needed\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mreturn\u001b[39;00m dtype\u001b[39m.\u001b[39;49mconstruct_array_type()\u001b[39m.\u001b[39;49m_from_sequence(arr, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m     86\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdtype must be np.dtype or ExtensionDtype\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:477\u001b[0m, in \u001b[0;36mCategorical._from_sequence\u001b[0;34m(cls, scalars, dtype, copy)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    474\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_from_sequence\u001b[39m(\n\u001b[1;32m    475\u001b[0m     \u001b[39mcls\u001b[39m, scalars, \u001b[39m*\u001b[39m, dtype: Dtype \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    476\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Categorical:\n\u001b[0;32m--> 477\u001b[0m     \u001b[39mreturn\u001b[39;00m Categorical(scalars, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:425\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath, copy)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mif\u001b[39;00m dtype\u001b[39m.\u001b[39mcategories \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    424\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m         codes, categories \u001b[39m=\u001b[39m factorize(values, sort\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    426\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    427\u001b[0m         codes, categories \u001b[39m=\u001b[39m factorize(values, sort\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/algorithms.py:780\u001b[0m, in \u001b[0;36mfactorize\u001b[0;34m(values, sort, use_na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[39m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[1;32m    778\u001b[0m             values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[0;32m--> 780\u001b[0m     codes, uniques \u001b[39m=\u001b[39m factorize_array(\n\u001b[1;32m    781\u001b[0m         values,\n\u001b[1;32m    782\u001b[0m         use_na_sentinel\u001b[39m=\u001b[39;49muse_na_sentinel,\n\u001b[1;32m    783\u001b[0m         size_hint\u001b[39m=\u001b[39;49msize_hint,\n\u001b[1;32m    784\u001b[0m     )\n\u001b[1;32m    786\u001b[0m \u001b[39mif\u001b[39;00m sort \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    787\u001b[0m     uniques, codes \u001b[39m=\u001b[39m safe_sort(\n\u001b[1;32m    788\u001b[0m         uniques,\n\u001b[1;32m    789\u001b[0m         codes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    792\u001b[0m         verify\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    793\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/algorithms.py:581\u001b[0m, in \u001b[0;36mfactorize_array\u001b[0;34m(values, use_na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[1;32m    578\u001b[0m hash_klass, values \u001b[39m=\u001b[39m _get_hashtable_algo(values)\n\u001b[1;32m    580\u001b[0m table \u001b[39m=\u001b[39m hash_klass(size_hint \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(values))\n\u001b[0;32m--> 581\u001b[0m uniques, codes \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39;49mfactorize(\n\u001b[1;32m    582\u001b[0m     values,\n\u001b[1;32m    583\u001b[0m     na_sentinel\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    584\u001b[0m     na_value\u001b[39m=\u001b[39;49mna_value,\n\u001b[1;32m    585\u001b[0m     mask\u001b[39m=\u001b[39;49mmask,\n\u001b[1;32m    586\u001b[0m     ignore_na\u001b[39m=\u001b[39;49muse_na_sentinel,\n\u001b[1;32m    587\u001b[0m )\n\u001b[1;32m    589\u001b[0m \u001b[39m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[1;32m    590\u001b[0m uniques \u001b[39m=\u001b[39m _reconstruct_data(uniques, original\u001b[39m.\u001b[39mdtype, original)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# read all data\n",
    "sales_validation: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SALES_VALIDATION))\n",
    "# sales_evaluation: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SALES_EVALUATION))\n",
    "calendar: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + CALENDAR))\n",
    "# sample_submission: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SAMPLE_SUBMISSION))\n",
    "sell_prices: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SELL_PRICES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aaaaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m aaa \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     aaaaa,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     calendar\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mweekday\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmonth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwm_yr_wk\u001b[39m\u001b[39m'\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     on \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     how \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m weekday_average \u001b[39m=\u001b[39m aaa\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mweekday\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39msold\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m event_name \u001b[39min\u001b[39;00m calendar[\u001b[39m'\u001b[39m\u001b[39mevent_name_1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aaaaa' is not defined"
     ]
    }
   ],
   "source": [
    "aaa = pd.merge(\n",
    "    aaaaa,\n",
    "    calendar.drop(['weekday', 'date', 'month', 'year', 'wm_yr_wk'],axis=1),\n",
    "    on = 'd',\n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "weekday_average = aaa.groupby('weekday')['sold'].mean()\n",
    "\n",
    "for event_name in calendar['event_name_1'].unique():\n",
    "    r = aaa[aaa['event_name_1'] == event_name]\n",
    "    if len(r)>0:\n",
    "        for i, row in r.iterrows():\n",
    "            if abs(row['sold'] - weekday_average[row['weekday']]) > 8000 and row['sold'] != 0:\n",
    "                print(row['d'], row['event_name_1'], row['weekday'], row['sold'], weekday_average[row['weekday']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~25 seconds\n",
    "# 1830, 1858, 1886, 1914\n",
    "def drop_days_after(df, day_threshold):\n",
    "    columns_keep = [c for c in df.columns if c.split('_')[0] != 'd']\n",
    "    columns_keep += [\n",
    "        c for c in \n",
    "            [d for d in df.columns if d.split('_')[0] == 'd'] \n",
    "            if int(c.split('_')[1]) < day_threshold\n",
    "    ]\n",
    "    return df[columns_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_START_VAL = 1800\n",
    "df, submission_idx = data_preprocessing(\n",
    "    drop_days_after(sales_validation.iloc[:int(1000000)],\n",
    "        day_threshold = D_START_VAL), \n",
    "    calendar,\n",
    "    sell_prices\n",
    ")\n",
    "df = df[(df.wm_yr_wk > df.release)]\n",
    "df['id'] = df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_status\n",
    "def compute_features(df: pd.DataFrame, group_columns, q: int = None, sparse_features: bool = False):\n",
    "    \"\"\"\n",
    "    Type of features computed:\n",
    "     - autocorrelation_ - moving averages mean/std | ewm mean\n",
    "     - autoquantiles_   - rolling sales quantiles\n",
    "     - momentum_        - changes in price\n",
    "    \"\"\"\n",
    "    # drop all NaT dates\n",
    "    idx = df['date'].notna()\n",
    "    df = df[idx]\n",
    "    \n",
    "    feature_columns = []\n",
    "    # feature_columns += ['sell_price']\n",
    "    for c in ['id', 'state_id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'sold']:\n",
    "        if c in df:\n",
    "            feature_columns += [c]\n",
    "            \n",
    "    # to be sure\n",
    "    df['month'] = df['month'].astype(int)\n",
    "    \n",
    "    # precomputing\n",
    "    df[\"index\"] = df.index\n",
    "    df[\"d_int\"] = df[\"d\"].apply(lambda x: int(x.split(\"_\")[-1]))\n",
    "    df[\"day\"] = str(df.date.dt.day)\n",
    "    \n",
    "    # to use groupby only once, saves time\n",
    "    df_grouped = df.groupby(group_columns)\n",
    "    \n",
    "    ################################################\n",
    "    ############### AUTOCORRELATION ################\n",
    "    ################################################\n",
    "    PREFIX = 'auto_'\n",
    "    logger.info('Computing autocorrelation features')\n",
    "    # DIRECT LAGGED VALUES\n",
    "    old_columns: set = set(df.columns)\n",
    "    LAG_SHIFT: int = 1\n",
    "    for lag in [1, 2, 7, 14, 21]:\n",
    "        if sparse_features: continue\n",
    "        df[PREFIX + f\"sold_{lag}\"] = df_grouped[\"sold\"].shift(lag) # 1-day lag\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # MOVING AVERAGES MEAN\n",
    "    old_columns: set = set(df.columns)\n",
    "    for i in [7, 28, 56, 168]:\n",
    "        if sparse_features and i != 28: continue\n",
    "        df[PREFIX + f'sold_ma_{i}'] = df_grouped['sold'].transform(lambda x: x.shift(LAG_SHIFT).rolling(i).mean()).astype(np.float16)\n",
    "        df[PREFIX + f'sold_std_{i}'] = df_grouped['sold'].transform(lambda x: x.shift(LAG_SHIFT).rolling(i).std()).astype(np.float16)\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # EXPONENTIAL MOVING AVERAGES\n",
    "    old_columns: set = set(df.columns)\n",
    "    for i in [3, 7, 15, 30, 100]:\n",
    "        if sparse_features and i<15: continue\n",
    "        df[PREFIX + f'sold_ewm_{i}'] = df_grouped['sold']\\\n",
    "            .transform(lambda x: x.shift(LAG_SHIFT)\n",
    "            .ewm(span=i, min_periods = int(np.ceil(i ** 0.8)))\n",
    "            .mean())\\\n",
    "            .astype(np.float16)\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # ROLLING QUANTILES\n",
    "    PREFIX = 'autoquantiles_'\n",
    "    old_columns: set = set(df.columns)\n",
    "    for quantile in [0.01, 0.1, 0.25, .5, 0.75, 0.9, 0.99]:#QUANTILES:\n",
    "        for i in [14, 28, 56, 112]:\n",
    "            if i * quantile >= 1:\n",
    "                if sparse_features and i!=28:continue\n",
    "                df[PREFIX + f'sold_qtile_{i}_{quantile}'] = df_grouped['sold'].transform(lambda x: x.shift(LAG_SHIFT).rolling(i).quantile(quantile)).astype(np.float16)\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # # DIFFERENCES\n",
    "    # old_columns = set(df.columns)\n",
    "    # feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    ###############################################\n",
    "    ############ PRICE AUTOCORRELATION ############ ## ONLY FOR LOWEST AGGREGATION LEVEL USABLE?\n",
    "    ###############################################\n",
    "    logger.info('Computing price autocorrelation features')\n",
    "    # # DIRECT LAGGED VALUES\n",
    "    # old_columns = set(df.columns)\n",
    "    # feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # # MOVING AVERAGES\n",
    "    # old_columns = set(df.columns)\n",
    "    # feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # # EXPONENTIAL SMOOTHING AVERAGES\n",
    "    # old_columns = set(df.columns)\n",
    "    # feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # PRICE DIFFERENCES (USSUALLY ON)\n",
    "    # PREFIX = 'momentum_'\n",
    "    # old_columns = set(df.columns)\n",
    "    # df[PREFIX + 'sell_price_w'] = df['sell_price'] / df.groupby(group_columns + ['wm_yr_wk'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    # df[PREFIX + 'sell_price_m'] = df['sell_price'] / df.groupby(group_columns + ['year', 'month'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    # df[PREFIX + 'sell_price_y'] = df['sell_price'] / df.groupby(group_columns + ['year'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    # feature_columns += list(set(df.columns) - old_columns)\n",
    "\n",
    "    ################################################\n",
    "    ############ PRODUCT CHARACTERISTICS ###########\n",
    "    ################################################\n",
    "    logger.info('Computing unconditional sold values')\n",
    "    # UNCONDITIONAL SALES\n",
    "    PREFIX = 'unconditional_'\n",
    "    # old_columns = set(df.columns)\n",
    "    # df['sold_mean'] = df.groupby(['id'])['sold'].transform(lambda x: x.mean())\n",
    "    # df['sold_std'] = df.groupby(['id'])['sold'].transform(lambda x: x.std())\n",
    "    # df['sold_max'] = df.groupby(['id'])['sold'].transform(lambda x: x.max())\n",
    "    # feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # old_columns = set(df.columns)    \n",
    "    # icols =  [\n",
    "    #     ['state_id'],\n",
    "    #     ['store_id'],\n",
    "    #     ['cat_id'],\n",
    "    #     ['dept_id'],\n",
    "    #     ['state_id', 'cat_id'],\n",
    "    #     ['state_id', 'dept_id'],\n",
    "    #     ['store_id', 'cat_id'],\n",
    "    #     ['store_id', 'dept_id'],\n",
    "    #     ['item_id'],\n",
    "    #     ['item_id', 'state_id'],\n",
    "    #     ['item_id', 'store_id']\n",
    "    # ]\n",
    "    # for col in icols:\n",
    "    #     col_name = '_'+'_'.join(col)+'_'\n",
    "    #     df[PREFIX + 'sold'+col_name+'median'] = df.groupby(col)['sold'].transform('median').astype(np.float16)\n",
    "    #     df[PREFIX + 'sold'+col_name+'mean'] = df.groupby(col)['sold'].transform('mean').astype(np.float16)\n",
    "    #     df[PREFIX + 'sold'+col_name+'std'] = df.groupby(col)['sold'].transform('std').astype(np.float16)\n",
    "    # feature_columns += list(set(df.columns) - old_columns)\n",
    "\n",
    "    ################################################\n",
    "    ############## SEASONAL FEATURES ###############\n",
    "    ################################################\n",
    "    logger.info('Encoding date features to dummies')\n",
    "    # WEEK / MONTH DUMMIES\n",
    "    PREFIX = 'seasonal_weekday_'\n",
    "    encode_columns = ['weekday']\n",
    "    old_columns = set(df.columns)\n",
    "    df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='')\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    PREFIX = 'seasonal_month_'\n",
    "    encode_columns =['month']\n",
    "    old_columns = set(df.columns)\n",
    "    df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='')\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "\n",
    "    # encode day in month as well\n",
    "    PREFIX = 'seasonal_monthday_'\n",
    "    df['monthday'] = df['date'].dt.day.astype(int)\n",
    "    encode_columns = ['monthday',]\n",
    "    old_columns = set(df.columns)\n",
    "    df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='')\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    ################################################\n",
    "    ################ OTHER FEATURES ################\n",
    "    ################################################\n",
    "    # logger.info('Computing PCT of non-zero days')\n",
    "    # old_columns = set(df.columns)\n",
    "    # for i in [7, 14, 28, 28*2, 28*4]:\n",
    "    #     if sparse_features and i != 28: continue\n",
    "    #     df[f'sales_pct_nonzero_{i}'] = df_grouped['sold'].rolling(i).mean().shift(LAG_SHIFT).astype(np.float16)\n",
    "    # feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    ################################################\n",
    "    ############### STATE/STORE/CAT ################\n",
    "    ################################################\n",
    "    df = _down_cast(df)\n",
    "    \n",
    "    logger.info('Computing state-id dummy')\n",
    "    old_columns = set(df.columns)\n",
    "    \n",
    "    if 'state_id' in group_columns:\n",
    "        # add state dummy\n",
    "        PREFIX = 'state_'\n",
    "        encode_columns = ['state_id']\n",
    "        state_ids = df['state_id']\n",
    "        df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='', )\n",
    "        df['state_id'] = state_ids\n",
    "    if ('store_id' in group_columns and 'item_id' in group_columns):\n",
    "        PREFIX = 'state_'\n",
    "        encode_columns = ['state_id']\n",
    "        df['state_id'] = df['store_id'].str.split('_').apply(lambda x: x[0])\n",
    "        df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='', )\n",
    "    elif 'store_id' in group_columns:\n",
    "        PREFIX = 'store_'\n",
    "        encode_columns = ['store_id']\n",
    "        store_ids = df['store_id']\n",
    "        df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='', )\n",
    "        df['store_id'] = store_ids\n",
    "        \n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    ################################################\n",
    "    ############## TARGET ENGINEERING ##############\n",
    "    ################################################\n",
    "    # the forecasting period is 28 days. We do not want to do recursive forecasting or something similar. \n",
    "    # Instead, we pass the days of forecasting as a feature in the model\n",
    "    # For the training data, this forecasting period is randomized\n",
    "    # For the validation and evaluation period, these are of course fixed\n",
    "    \n",
    "    mapping_dict = {int(D_START_VAL+i):i for i in range(DAYS)}\n",
    "    def map_d_to_days_fwd(row):\n",
    "        return mapping_dict.get(row['d_int'], row['days_fwd'])\n",
    "\n",
    "    # add random forecasting periods\n",
    "    days_forecast = np.random.randint(low = 0, high=DAYS, size = len(df))\n",
    "    \n",
    "    # create df with solely the target and date\n",
    "    columns_temp = ['sold', 'd']\\\n",
    "        + ['seasonal_weekday_' + day for day in ['Monday', 'Tuesday', 'Thursday', 'Wednesday', 'Friday', 'Saturday', 'Sunday']]\\\n",
    "        + ['seasonal_month_' + str(m) for m in range(1,12+1)]\\\n",
    "        + ['seasonal_monthday_' + str(i) for i in range(1,31+1)]\n",
    "\n",
    "    df_temp: pd.DataFrame = df[columns_temp + group_columns + ['d_int']]\n",
    "    df_temp['d_int'] = df_temp['d_int'].astype(int)\n",
    "    df_temp['days_fwd'] = days_forecast\n",
    "    # for the validation set, we know the forecasting period. Specifically map these increasingly from 0 to DAYS-1\n",
    "    df_temp['days_fwd'] = df_temp.apply(map_d_to_days_fwd, axis=1)\n",
    "    # days_fwd = pd.to_timedelta(days_forecast, unit='D')\n",
    "    df_temp['d_int'] -= df_temp['days_fwd']\n",
    "    df_temp['d_int'] = df_temp['d_int'].astype(int)\n",
    "    df_temp['d'] = 'd_' + df_temp['d_int'].astype(str)\n",
    "    feature_columns += ['days_fwd']\n",
    "    \n",
    "    # merge shifted sales back\n",
    "    df = df.drop(columns_temp, axis=1)\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        df_temp,\n",
    "        on = ['d_int'] + group_columns,\n",
    "        how = 'right'\n",
    "    )\n",
    "    # add DAYS back to d_int and d\n",
    "    df['d_int'] += df['days_fwd']\n",
    "    df['d'] = 'd_' + df['d_int'].astype(str)\n",
    "    \n",
    "    # drop invalid cases of forecasting windows (i.e. negative d)\n",
    "    idx = df['d_int'] > DAYS\n",
    "    df = df[idx]    \n",
    "    ############# RETURN FINAL RESULTS ############\n",
    "    \n",
    "    # return final results\n",
    "    df = _down_cast(df)\n",
    "    return df[feature_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test function if feature_engineering works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 23:23:09 - compute_features - INFO - calling\n",
      "2023-11-15 23:23:09 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:23:09 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:23:09 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:23:09 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:23:09 - __main__ - INFO - Computing state-id dummy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['d', 'sold', 'auto_sold_1', 'auto_sold_2', 'auto_sold_14',\n",
       "       'auto_sold_21', 'auto_sold_7', 'auto_sold_std_28', 'auto_sold_ma_28',\n",
       "       'auto_sold_ma_168', 'auto_sold_std_56', 'auto_sold_ma_56',\n",
       "       'auto_sold_ma_7', 'auto_sold_std_168', 'auto_sold_std_7',\n",
       "       'auto_sold_ewm_7', 'auto_sold_ewm_3', 'auto_sold_ewm_30',\n",
       "       'auto_sold_ewm_100', 'auto_sold_ewm_15',\n",
       "       'autoquantiles_sold_qtile_56_0.5', 'autoquantiles_sold_qtile_56_0.25',\n",
       "       'autoquantiles_sold_qtile_28_0.75', 'autoquantiles_sold_qtile_112_0.1',\n",
       "       'autoquantiles_sold_qtile_28_0.25', 'autoquantiles_sold_qtile_56_0.99',\n",
       "       'autoquantiles_sold_qtile_14_0.99', 'autoquantiles_sold_qtile_14_0.1',\n",
       "       'autoquantiles_sold_qtile_28_0.99', 'autoquantiles_sold_qtile_14_0.5',\n",
       "       'autoquantiles_sold_qtile_14_0.25', 'autoquantiles_sold_qtile_112_0.25',\n",
       "       'autoquantiles_sold_qtile_28_0.1', 'autoquantiles_sold_qtile_14_0.75',\n",
       "       'autoquantiles_sold_qtile_112_0.99', 'autoquantiles_sold_qtile_112_0.5',\n",
       "       'autoquantiles_sold_qtile_112_0.9', 'autoquantiles_sold_qtile_56_0.75',\n",
       "       'autoquantiles_sold_qtile_112_0.75',\n",
       "       'autoquantiles_sold_qtile_112_0.01', 'autoquantiles_sold_qtile_14_0.9',\n",
       "       'autoquantiles_sold_qtile_28_0.5', 'autoquantiles_sold_qtile_28_0.9',\n",
       "       'autoquantiles_sold_qtile_56_0.1', 'autoquantiles_sold_qtile_56_0.9',\n",
       "       'seasonal_weekday_Monday', 'seasonal_weekday_Sunday',\n",
       "       'seasonal_weekday_Friday', 'seasonal_weekday_Thursday',\n",
       "       'seasonal_weekday_Saturday', 'seasonal_weekday_Wednesday',\n",
       "       'seasonal_weekday_Tuesday', 'seasonal_month_3', 'seasonal_month_6',\n",
       "       'seasonal_month_5', 'seasonal_month_2', 'seasonal_month_8',\n",
       "       'seasonal_month_12', 'seasonal_month_11', 'seasonal_month_1',\n",
       "       'seasonal_month_4', 'seasonal_month_10', 'seasonal_month_9',\n",
       "       'seasonal_month_7', 'seasonal_monthday_22', 'seasonal_monthday_15',\n",
       "       'seasonal_monthday_18', 'seasonal_monthday_9', 'seasonal_monthday_25',\n",
       "       'seasonal_monthday_12', 'seasonal_monthday_27', 'seasonal_monthday_17',\n",
       "       'seasonal_monthday_6', 'seasonal_monthday_16', 'seasonal_monthday_29',\n",
       "       'seasonal_monthday_30', 'seasonal_monthday_2', 'seasonal_monthday_14',\n",
       "       'seasonal_monthday_26', 'seasonal_monthday_1', 'seasonal_monthday_21',\n",
       "       'seasonal_monthday_24', 'seasonal_monthday_20', 'seasonal_monthday_8',\n",
       "       'seasonal_monthday_10', 'seasonal_monthday_4', 'seasonal_monthday_7',\n",
       "       'seasonal_monthday_28', 'seasonal_monthday_13', 'seasonal_monthday_5',\n",
       "       'seasonal_monthday_19', 'seasonal_monthday_11', 'seasonal_monthday_31',\n",
       "       'seasonal_monthday_3', 'seasonal_monthday_23', 'days_fwd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total ~77 seconds\n",
    "D_START_VAL = 1800\n",
    "level = 'Level1'\n",
    "df['temp_id'] = 'temp_id'\n",
    "# agg_columns = ['cat_id', 'state_id']\n",
    "agg_columns = AGG_LEVEL_COLUMNS[level] if level != 'Level1' else ['temp_id']\n",
    "agg_dict = {\n",
    "    'sold': np.nansum,\n",
    "    'sell_price': np.nanmean,\n",
    "    'date': 'last',\n",
    "    'weekday': 'last',\n",
    "    'month': 'last',\n",
    "    'year': 'last',\n",
    "    'wm_yr_wk': 'last'\n",
    "}\n",
    "features = compute_features(\n",
    "    df.groupby(agg_columns + ['d']).agg(agg_dict).reset_index(drop=False),\n",
    "    agg_columns,\n",
    "    sparse_features=False\n",
    ")\n",
    "# features[features['d'].isin([f'd_{int(i)}' for i in range(D_START_VAL, D_START_VAL+DAYS)])]\n",
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that for the prediction period,\n",
    "# days forward is not random but 0 to DAYS-1, increasing linearly\n",
    "for i in range(DAYS):\n",
    "    assert len(features[features['d'] == f'd_{int(D_START_VAL+i)}']['days_fwd'].unique())==1\n",
    "    assert features[features['d'] == f'd_{int(D_START_VAL+i)}']['days_fwd'].unique()[0]==i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Runs Feature Engineering for Validation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_status\n",
    "def groupby_agglevel(df: pd.DataFrame, agg_columns: list, agg_dict: dict):\n",
    "    return df.groupby(agg_columns).agg(agg_dict).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 23:24:35 - __main__ - INFO - D_START_VAL: 1802\n",
      "2023-11-15 23:24:38 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-11-15 23:24:38 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:38 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:38 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:38 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:38 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:38 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:38 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:38 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:39 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:39 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:39 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:39 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:40 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:40 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:40 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:40 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:40 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:40 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:40 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:40 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:40 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:40 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:40 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:40 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:41 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:41 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:41 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:41 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:41 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:41 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:42 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:42 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:42 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:42 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:43 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:43 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:44 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:44 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:44 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:44 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:45 - __main__ - INFO - D_START_VAL: 1830\n",
      "2023-11-15 23:24:48 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-11-15 23:24:48 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:48 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:48 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:48 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:49 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:49 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:49 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:49 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:50 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:50 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:50 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:50 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:50 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:50 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:50 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:51 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:51 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:51 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:51 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:51 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:51 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:52 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:52 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:54 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:54 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:54 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:54 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:55 - __main__ - INFO - D_START_VAL: 1858\n",
      "2023-11-15 23:24:58 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-11-15 23:24:58 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:58 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:58 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:58 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:58 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:58 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:58 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:58 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:58 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:58 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:58 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:58 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:58 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:58 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:59 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:59 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:24:59 - compute_features - INFO - calling\n",
      "2023-11-15 23:24:59 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:00 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:00 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:00 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:00 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:00 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:00 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:00 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:00 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:00 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:00 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:01 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:01 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:02 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:02 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:02 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:02 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:02 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:02 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:04 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:04 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:04 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:04 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:06 - __main__ - INFO - D_START_VAL: 1886\n",
      "2023-11-15 23:25:08 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-11-15 23:25:09 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:09 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:09 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:09 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:09 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:10 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:10 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:10 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:11 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:11 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:11 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:11 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:11 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:11 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:12 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:12 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:12 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:12 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:12 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:13 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:13 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:13 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:15 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:15 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:15 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:15 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:16 - __main__ - INFO - D_START_VAL: 1914\n",
      "2023-11-15 23:25:19 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-11-15 23:25:19 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:19 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:19 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:19 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:20 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:20 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:20 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:20 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:20 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:20 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:20 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:20 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:20 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:20 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:20 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:20 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:20 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:20 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:21 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:21 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:21 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:21 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:21 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:21 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:21 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:21 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:21 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:21 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:22 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:22 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:23 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:23 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:23 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:23 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-15 23:25:23 - compute_features - INFO - calling\n",
      "2023-11-15 23:25:23 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-15 23:25:25 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-15 23:25:25 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-15 23:25:25 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-15 23:25:25 - __main__ - INFO - Computing state-id dummy\n"
     ]
    }
   ],
   "source": [
    "# params for un\n",
    "SPARSE_FEATURES = False\n",
    "TEST_RUN = False\n",
    "MAX_QUANTILE = 9\n",
    "MAX_QUANTILE = 12 - MAX_QUANTILE\n",
    "\n",
    "# for each fold\n",
    "# 1830, 1858, 1886, 1914\n",
    "for D_START_VAL in D_CROSS_VAL_START_LIST:\n",
    "    logger.info(f'D_START_VAL: {D_START_VAL}')\n",
    "\n",
    "    # if all grouped dfs for all levels is already computed\n",
    "    # do not load original df at all\n",
    "    all_found = True\n",
    "    for agg_level in AGG_LEVEL_COLUMNS:\n",
    "        try:\n",
    "            aaaaa = pd.read_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet')\n",
    "        except:\n",
    "            all_found = False\n",
    "            logger.info(f'grouped df not computed for level: {agg_level}')\n",
    "        \n",
    "    if not all_found:\n",
    "        # pivot initial dataframe and compute features/targets\n",
    "        df_val, submission_idx_validation = data_preprocessing(\n",
    "            drop_days_after(sales_validation,\n",
    "            day_threshold = D_START_VAL), \n",
    "            calendar,\n",
    "            sell_prices\n",
    "        )\n",
    "        # drop all leading rows with leading zeros for each product\n",
    "        df_val_after_release = df_val[(df_val.wm_yr_wk > df_val.release)]\n",
    "        del df_val\n",
    "    else:\n",
    "        logger.info('grouped df computed for all levels, original dataframe is not loaded')\n",
    "\n",
    "    # set prediction values to nan values\n",
    "    # pred_index = df_val_after_release['d'].isin(D_CV_OOS)\n",
    "    # df_val_after_release.loc[pred_index, 'sold'] = np.nan\n",
    "    # del pred_index\n",
    "\n",
    "    # for each level\n",
    "    for agg_level, agg_columns in AGG_LEVEL_COLUMNS.items(): \n",
    "        # remove index to compute all\n",
    "        if agg_level in [f'Level{int(12 - i)}' for i in range(0,MAX_QUANTILE)]:\n",
    "            continue\n",
    "            \n",
    "        # group data for specific grouping columns per level\n",
    "        # and compute features\n",
    "        agg_dict = {\n",
    "            'sold': np.nansum,\n",
    "            'sell_price': np.nanmean,\n",
    "            'date': 'last',\n",
    "            'weekday': 'last',\n",
    "            'month': 'last',\n",
    "            'year': 'last',\n",
    "            'wm_yr_wk': 'last'\n",
    "        }\n",
    "\n",
    "        if len(agg_columns) == 0:\n",
    "            agg_columns = ['temp_id']\n",
    "        \n",
    "        # load grouped df if already exists\n",
    "        try:\n",
    "            aaaaa = pd.read_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet')\n",
    "            aaaaa = _down_cast(aaaaa)\n",
    "        except:\n",
    "            logger.info(f'not existing yet: ../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet')\n",
    "            logger.info(f'computing and storing grouped dataframe for level: {agg_level}')\n",
    "            # get data on aggregated level\n",
    "            if len(agg_columns) == 0 or 'temp_id' in agg_columns:\n",
    "                df_val_after_release['temp_id'] = 'temp_id'\n",
    "\n",
    "            aaaaa = groupby_agglevel(df_val_after_release, agg_columns + ['d'], agg_dict)\n",
    "            idx_keep = aaaaa['date'].notna()\n",
    "            aaaaa = aaaaa[idx_keep]\n",
    "            # to suitable type for .parquet\n",
    "            for c in aaaaa.columns:\n",
    "                if c not in agg_columns + ['d','date','weekday']:#['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'state_id']:\n",
    "                    aaaaa[c] = aaaaa[c].astype(np.float32)\n",
    "            aaaaa.to_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet', index=False)\n",
    "            aaaaa = _down_cast(aaaaa)\n",
    "            \n",
    "        features = compute_features(\n",
    "            aaaaa,\n",
    "            agg_columns,\n",
    "            sparse_features=SPARSE_FEATURES\n",
    "        )\n",
    "        \n",
    "        # to suitable type for .parquet\n",
    "        for c in features.columns:\n",
    "            if c not in ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'state_id']:\n",
    "                features[c] = features[c].astype(np.float32)\n",
    "        \n",
    "        # format string and save file\n",
    "        agg_string = parse_columns_to_string(agg_columns)\n",
    "        if not TEST_RUN:\n",
    "            features.to_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/features/' + f'features_val_{agg_string}.parquet', index=False)\n",
    "            del features\n",
    "        else:\n",
    "            features.to_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/features/' + f'/test/features_val_{agg_string}.parquet', index=False)\n",
    "\n",
    "    if not TEST_RUN:\n",
    "        try:\n",
    "            del df_val_after_release\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pivot initial dataframe and compute features/targets\n",
    "# df_eval, submission_idx_validation = data_preprocessing(sales_evaluation, calendar, sell_prices)\n",
    "# df_eval_after_release = df_eval[(df_eval.wm_yr_wk > df_eval.release)]\n",
    "# del df_eval\n",
    "\n",
    "# # set prediction values to nan values\n",
    "# pred_index = df_eval_after_release['d'].isin(D_CV_OOS)\n",
    "# df_eval_after_release.loc[pred_index, 'sold'] = np.nan\n",
    "# del pred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_RUN = False\n",
    "# for agg_level in AGG_LEVEL_COLUMNS:\n",
    "#     agg_columns = AGG_LEVEL_COLUMNS[agg_level]\n",
    "#     # get data on aggregated level\n",
    "#     if len(agg_columns) == 0:\n",
    "#         df_eval_after_release['temp_id'] = 'temp_id'\n",
    "#         agg_columns = ['temp_id']\n",
    "#     agg_dict = {\n",
    "#         'sold': np.nansum,\n",
    "#         'date': 'last',\n",
    "#         'weekday': 'last',\n",
    "#         'month': 'last'\n",
    "#     }\n",
    "#     features = compute_features(\n",
    "#         groupby_agglevel(df_eval_after_release, agg_columns + ['d'], agg_dict),\n",
    "#         agg_columns\n",
    "#     )\n",
    "    \n",
    "#     # to suitable format for .parquet\n",
    "#     for c in features.columns:\n",
    "#         if c not in ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'state_id']:\n",
    "#             features[c] = features[c].astype(np.float32)\n",
    "        \n",
    "#     # format string and save file\n",
    "#     agg_string = parse_columns_to_string(agg_columns)\n",
    "#     if not TEST_RUN:\n",
    "#         features.to_parquet(PRECOMPUTED_BASE_PATH + f'features_eval_{agg_string}.parquet', index=False)\n",
    "#         del features\n",
    "#     else:\n",
    "#         features.to_parquet(PRECOMPUTED_BASE_PATH + f'/test/features_eval_{agg_string}.parquet', index=False)\n",
    "        \n",
    "# if not TEST_RUN: \n",
    "#     del df_eval_after_release"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
