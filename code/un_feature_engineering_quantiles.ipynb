{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from utils.utils import merge_eval_sold_on_df, sort_df_on_d, WRMSSE, RMSSE, _down_cast, data_preprocessing, diff_lists, log_status #create_submission_df\n",
    "from utils.utils import customIter, parse_columns_to_string\n",
    "from utils import constants\n",
    "\n",
    "from utils.configure_logger import configure_logger\n",
    "configure_logger()\n",
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = constants.DATA_BASE_PATH #'../data/m5-forecasting-accuracy/'\n",
    "DATA_BASE_PATH_UNCERTAINTY = constants.DATA_BASE_PATH_UNCERTAINTY #'../data/m5-forecasting-uncertainty/'\n",
    "SALES_EVALUATION = constants.SALES_EVALUATION #'sales_train_evaluation.csv'\n",
    "SALES_VALIDATION = constants.SALES_VALIDATION #'sales_train_validation.csv'\n",
    "CALENDAR = constants.CALENDAR #'calendar.csv'\n",
    "SAMPLE_SUBMISSION = constants.SAMPLE_SUBMISSION #'sample_submission.csv'\n",
    "SELL_PRICES = constants.SELL_PRICES #'sell_prices.csv'\n",
    "\n",
    "PRECOMPUTED_BASE_PATH = constants.PRECOMPUTED_BASE_PATH #'../data/uncertainty/features/'\n",
    "\n",
    "DAYS: int = constants.DAYS #28\n",
    "QUANTILES: int = constants.QUANTILES #[0.005, 0.025, 0.165, 0.25, 0.50, 0.75, 0.835, 0.975, 0.995]\n",
    "\n",
    "AGG_LEVEL_COLUMNS = constants.AGG_LEVEL_COLUMNS\n",
    "D_CROSS_VAL_START_LIST = constants.D_CROSS_VAL_START_LIST #[1802, 1830, 1858, 1886, 1914]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all data\n",
    "sales_validation: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SALES_VALIDATION))\n",
    "# sales_evaluation: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SALES_EVALUATION))\n",
    "calendar: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + CALENDAR))\n",
    "# sample_submission: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SAMPLE_SUBMISSION))\n",
    "sell_prices: pd.DataFrame = _down_cast(pd.read_csv(DATA_BASE_PATH + SELL_PRICES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa = pd.merge(\n",
    "#     aaaaa,\n",
    "#     calendar.drop(['weekday', 'date', 'month', 'year', 'wm_yr_wk'],axis=1),\n",
    "#     on = 'd',\n",
    "#     how = 'left'\n",
    "# )\n",
    "\n",
    "# weekday_average = aaa.groupby('weekday')['sold'].mean()\n",
    "\n",
    "# for event_name in calendar['event_name_1'].unique():\n",
    "#     r = aaa[aaa['event_name_1'] == event_name]\n",
    "#     if len(r)>0:\n",
    "#         for i, row in r.iterrows():\n",
    "#             if abs(row['sold'] - weekday_average[row['weekday']]) > 8000 and row['sold'] != 0:\n",
    "#                 print(row['d'], row['event_name_1'], row['weekday'], row['sold'], weekday_average[row['weekday']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~25 seconds\n",
    "# 1830, 1858, 1886, 1914\n",
    "def drop_days_after(df, day_threshold):\n",
    "    columns_keep = [c for c in df.columns if c.split('_')[0] != 'd']\n",
    "    columns_keep += [\n",
    "        c for c in \n",
    "            [d for d in df.columns if d.split('_')[0] == 'd'] \n",
    "            if int(c.split('_')[1]) < day_threshold\n",
    "    ]\n",
    "    return df[columns_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_START_VAL = 1914\n",
    "df, submission_idx = data_preprocessing(\n",
    "    drop_days_after(sales_validation.iloc[:int(1000000)],\n",
    "        day_threshold = D_START_VAL), \n",
    "    calendar,\n",
    "    sell_prices\n",
    ")\n",
    "df = df[(df.wm_yr_wk > df.release)]\n",
    "df['id'] = df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_status\n",
    "def compute_features(df: pd.DataFrame, group_columns, q: int = None, sparse_features: bool = False, agg_level: str = None):\n",
    "    \"\"\"\n",
    "    Type of features computed:\n",
    "     - autocorrelation_ - moving averages mean/std | ewm mean\n",
    "     - autoquantiles_   - rolling sales quantiles\n",
    "     - momentum_        - changes in price\n",
    "    \"\"\"\n",
    "    # drop all NaT dates\n",
    "    idx = df['date'].notna()\n",
    "    df = df[idx]\n",
    "    \n",
    "    feature_columns = []\n",
    "    # feature_columns += ['sell_price']\n",
    "    for c in ['id', 'state_id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'sold']:\n",
    "        if c in df:\n",
    "            feature_columns += [c]\n",
    "            \n",
    "    # to be sure\n",
    "    df['month'] = df['month'].astype(int)\n",
    "    \n",
    "    # precomputing\n",
    "    df[\"index\"] = df.index\n",
    "    df[\"d_int\"] = df[\"d\"].apply(lambda x: int(x.split(\"_\")[-1]))\n",
    "    df[\"day\"] = str(df.date.dt.day)\n",
    "    \n",
    "    # to use groupby only once, saves time\n",
    "    df_grouped = df.groupby(group_columns)\n",
    "    \n",
    "    ################################################\n",
    "    ############### AUTOCORRELATION ################\n",
    "    ################################################\n",
    "    PREFIX = 'auto_'\n",
    "    logger.info('Computing autocorrelation features')\n",
    "    # DIRECT LAGGED VALUES\n",
    "    old_columns: set = set(df.columns)\n",
    "    LAG_SHIFT: int = 1\n",
    "    for lag in [1, 2, 7, 14, 28, 56]:\n",
    "        if sparse_features: continue\n",
    "        df[PREFIX + f\"sold_{lag}\"] = df_grouped[\"sold\"].shift(lag) # 1-day lag\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # EWM/MA/STD/QUANTILE\n",
    "    old_columns: set = set(df.columns)\n",
    "    for i in [3,7,14,21,28,56,112,168]:\n",
    "        if sparse_features and i != 28: continue\n",
    "        min_periods = int(np.ceil(i ** 0.8))\n",
    "        \n",
    "        df[PREFIX + f'sold_ma_{i}'] = df_grouped['sold'].transform(lambda x: x.shift(LAG_SHIFT).rolling(i, min_periods).mean()).astype(np.float16)\n",
    "        df[PREFIX + f'sold_std_{i}'] = df_grouped['sold'].transform(lambda x: x.shift(LAG_SHIFT).rolling(i, min_periods).std()).astype(np.float16)\n",
    "        df[PREFIX + f'sold_ewm_{i}'] = df_grouped['sold']\\\n",
    "            .transform(lambda x: x.shift(LAG_SHIFT)\n",
    "            .ewm(span=i, min_periods = min_periods)\n",
    "            .mean())\\\n",
    "            .astype(np.float16)\n",
    "            \n",
    "        for quantile in [0.01, 0.1, 0.25, .5, 0.75, 0.9, 0.99]:#QUANTILES:\n",
    "            if i * min(quantile, 1-quantile) >= 1:\n",
    "                if sparse_features and i!=28:continue\n",
    "                df[PREFIX + f'sold_qtile_{i}_{quantile}'] = df_grouped['sold'].transform(lambda x: x.shift(LAG_SHIFT).rolling(i, min_periods).quantile(quantile)).astype(np.float16)\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # # EXPONENTIAL MOVING AVERAGES\n",
    "    # old_columns: set = set(df.columns)\n",
    "    # for i in [3,7,15,30,100]:\n",
    "    #     if sparse_features and i<15: continue\n",
    "    #     df[PREFIX + f'sold_ewm_{i}'] = df_grouped['sold']\\\n",
    "    #         .transform(lambda x: x.shift(LAG_SHIFT)\n",
    "    #         .ewm(span=i, min_periods = int(np.ceil(i ** 0.8)))\n",
    "    #         .mean())\\\n",
    "    #         .astype(np.float16)\n",
    "    # feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # # ROLLING QUANTILES\n",
    "    # PREFIX = 'autoquantiles_'\n",
    "    # old_columns: set = set(df.columns)\n",
    "    # for quantile in [0.01, 0.1, 0.25, .5, 0.75, 0.9, 0.99]:#QUANTILES:\n",
    "    #     for i in [14, 28, 56, 112]:\n",
    "    #         if i * min(quantile, 1-quantile) >= 1:\n",
    "    #             if sparse_features and i!=28:continue\n",
    "    #             df[PREFIX + f'sold_qtile_{i}_{quantile}'] = df_grouped['sold'].transform(lambda x: x.shift(LAG_SHIFT).rolling(i).quantile(quantile)).astype(np.float16)\n",
    "    # feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    ###############################################\n",
    "    ############ PRICE AUTOCORRELATION ############ ## ONLY FOR LOWEST AGGREGATION LEVEL USABLE?\n",
    "    ###############################################\n",
    "    logger.info('Computing price autocorrelation features')\n",
    "    \n",
    "    # SIMPLY THE CURRENT PRICE\n",
    "    # Level1 is univariate, unconditional features would not make sense\n",
    "    if 'temp_id' not in group_columns:\n",
    "        old_columns = set(df.columns)\n",
    "        df_grouped_d = df.groupby(group_columns + ['d'])\n",
    "        df['price_uncond_avg'] = df_grouped_d['sell_price'].transform(lambda x: x.mean()).astype(np.float32)\n",
    "        df['price_uncond_std'] = df_grouped_d['sell_price'].transform(lambda x: x.std()).astype(np.float32)\n",
    "        df['price_uncond_median'] = df_grouped_d['sell_price'].transform(lambda x: x.median()).astype(np.float32)\n",
    "        feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # type issues\n",
    "    for c in ['wm_yr_wk', 'year', 'month']:\n",
    "        df[c] = df[c].astype('int32')\n",
    "        \n",
    "    # PRICE DIFFERENCES BY WEEK/MONTH/YEAR (USSUALLY ON)\n",
    "    PREFIX = 'price_momentum_'\n",
    "    old_columns = set(df.columns)\n",
    "    df[PREFIX + 'w'] = df['sell_price'] / df.groupby(group_columns + ['wm_yr_wk'])['sell_price'].transform(lambda x: x.shift(LAG_SHIFT)).astype(np.float32)\n",
    "    df[PREFIX + 'm'] = df['sell_price'] / df.groupby(group_columns + ['year', 'month'])['sell_price'].transform(lambda x: x.shift(LAG_SHIFT)).astype(np.float32)\n",
    "    df[PREFIX + 'y'] = df['sell_price'] / df.groupby(group_columns + ['year'])['sell_price'].transform(lambda x: x.shift(LAG_SHIFT)).astype(np.float32)\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    # VARIATION OF PRICES\n",
    "    PREFIX = 'price_auto_'\n",
    "    old_columns = set(df.columns)\n",
    "    for i in [28, 56, 112]:\n",
    "        min_periods = int(np.ceil(i ** 0.8))\n",
    "        # df[PREFIX + f'std_{int(i)}'] = df['sell_price'] / df.groupby(group_columns + ['wm_yr_wk'])['sell_price'].transform(lambda x: x.rolling(i, min_periods).std()).astype(np.float32)\n",
    "        df[PREFIX + f'std_{int(i)}'] = df_grouped['sell_price'].transform(lambda x: x.rolling(i).std()).astype(np.float16)\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "\n",
    "    ################################################\n",
    "    ############## SEASONAL FEATURES ###############\n",
    "    ################################################\n",
    "    logger.info('Encoding date features to dummies')\n",
    "    # WEEK / MONTH DUMMIES\n",
    "    PREFIX = 'seasonal_weekday_'\n",
    "    encode_columns = ['weekday']\n",
    "    old_columns = set(df.columns)\n",
    "    df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='')\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    PREFIX = 'seasonal_month_'\n",
    "    encode_columns =['month']\n",
    "    old_columns = set(df.columns)\n",
    "    df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='')\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "\n",
    "    # encode day in month as well\n",
    "    PREFIX = 'seasonal_monthday_'\n",
    "    df['monthday'] = df['date'].dt.day.astype(int)\n",
    "    encode_columns = ['monthday',]\n",
    "    old_columns = set(df.columns)\n",
    "    df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='')\n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    ################################################\n",
    "    ################ OTHER FEATURES ################\n",
    "    ################################################\n",
    "    if agg_level in ['Level10', 'Level11', 'Level12']:\n",
    "        logger.info('Computing PCT of non-zero days')\n",
    "        old_columns = set(df.columns)\n",
    "        for i in [7, 14, 28, 28*2, 28*4]:\n",
    "            if sparse_features and i != 28: continue\n",
    "            df[f'sold_pct_nonzero_{i}'] = df_grouped['sold'].transform(lambda x: (x!=0).rolling(i).mean().shift(LAG_SHIFT)).astype(np.float16)\n",
    "        feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    ################################################\n",
    "    ############### STATE/STORE/CAT ################\n",
    "    ################################################\n",
    "    df = _down_cast(df)\n",
    "    \n",
    "    logger.info('Computing state-id dummy')\n",
    "    old_columns = set(df.columns)\n",
    "    \n",
    "    if 'state_id' in group_columns:\n",
    "        # add state dummy\n",
    "        PREFIX = 'state_'\n",
    "        encode_columns = ['state_id']\n",
    "        state_ids = df['state_id']\n",
    "        df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='', )\n",
    "        df['state_id'] = state_ids\n",
    "    if ('store_id' in group_columns and 'item_id' in group_columns):\n",
    "        PREFIX = 'state_'\n",
    "        encode_columns = ['state_id']\n",
    "        df['state_id'] = df['store_id'].str.split('_').apply(lambda x: x[0])\n",
    "        df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='', )\n",
    "    elif 'store_id' in group_columns:\n",
    "        PREFIX = 'store_'\n",
    "        encode_columns = ['store_id']\n",
    "        store_ids = df['store_id']\n",
    "        df = pd.get_dummies(df, columns = encode_columns, prefix=PREFIX, prefix_sep='', )\n",
    "        df['store_id'] = store_ids\n",
    "        \n",
    "    feature_columns += list(set(df.columns) - old_columns)\n",
    "    \n",
    "    ################################################\n",
    "    ############## TARGET ENGINEERING ##############\n",
    "    ################################################\n",
    "    # the forecasting period is 28 days. We do not want to do recursive forecasting or something similar. \n",
    "    # Instead, we pass the days of forecasting as a feature in the model\n",
    "    # For the training data, this forecasting period is randomized\n",
    "    # For the validation and evaluation period, these are of course fixed\n",
    "    \n",
    "    mapping_dict = {int(D_START_VAL+i):i for i in range(DAYS)}\n",
    "    def map_d_to_days_fwd(row):\n",
    "        return mapping_dict.get(row['d_int'], row['days_fwd'])\n",
    "\n",
    "    # add random forecasting periods\n",
    "    days_forecast = np.random.randint(low = 0, high=DAYS, size = len(df))\n",
    "    \n",
    "    # create df with solely the target and date\n",
    "    columns_temp = ['sold', 'd']\\\n",
    "        + ['seasonal_weekday_' + day for day in ['Monday', 'Tuesday', 'Thursday', 'Wednesday', 'Friday', 'Saturday', 'Sunday']]\\\n",
    "        + ['seasonal_month_' + str(m) for m in range(1,12+1)]\\\n",
    "        + ['seasonal_monthday_' + str(i) for i in range(1,31+1)]\\\n",
    "        + ['price_momentum_'+i for i in ['w', 'm', 'y']]\\\n",
    "        + ['price_auto_std_'+str(i) for i in [28, 56, 112]]\n",
    "    # price_auto_\n",
    "    # price_momentum_\n",
    "\n",
    "    df_temp: pd.DataFrame = df[columns_temp + group_columns + ['d_int']]\n",
    "    df_temp['d_int'] = df_temp['d_int'].astype(int)\n",
    "    df_temp['days_fwd'] = days_forecast\n",
    "    # for the validation set, we know the forecasting period. Specifically map these increasingly from 0 to DAYS-1\n",
    "    df_temp['days_fwd'] = df_temp.apply(map_d_to_days_fwd, axis=1)\n",
    "    # days_fwd = pd.to_timedelta(days_forecast, unit='D')\n",
    "    df_temp['d_int'] -= df_temp['days_fwd']\n",
    "    df_temp['d_int'] = df_temp['d_int'].astype(int)\n",
    "    df_temp['d'] = 'd_' + df_temp['d_int'].astype(str)\n",
    "    feature_columns += ['days_fwd']\n",
    "    \n",
    "    # merge shifted sales back\n",
    "    df = df.drop(columns_temp, axis=1)\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        df_temp,\n",
    "        on = ['d_int'] + group_columns,\n",
    "        how = 'right'\n",
    "    )\n",
    "    # add DAYS back to d_int and d\n",
    "    df['d_int'] += df['days_fwd']\n",
    "    df['d'] = 'd_' + df['d_int'].astype(str)\n",
    "    \n",
    "    # drop invalid cases of forecasting windows (i.e. negative d)\n",
    "    idx = df['d_int'] > DAYS\n",
    "    df = df[idx]\n",
    "    ############# RETURN FINAL RESULTS ############\n",
    "    \n",
    "    # return final results\n",
    "    df = _down_cast(df)\n",
    "    return df[feature_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test function if feature_engineering works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m D_START_VAL \u001b[39m=\u001b[39m \u001b[39m1914\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m level \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLevel1\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtemp_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtemp_id\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# agg_columns = ['cat_id', 'state_id']\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m agg_columns \u001b[39m=\u001b[39m AGG_LEVEL_COLUMNS[level] \u001b[39mif\u001b[39;00m level \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLevel1\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtemp_id\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# total ~77 seconds\n",
    "D_START_VAL = 1914\n",
    "level = 'Level1'\n",
    "df['temp_id'] = 'temp_id'\n",
    "# agg_columns = ['cat_id', 'state_id']\n",
    "agg_columns = AGG_LEVEL_COLUMNS[level] if level != 'Level1' else ['temp_id']\n",
    "agg_dict = {\n",
    "    'sold': np.nansum,\n",
    "    'sell_price': np.nanmean,\n",
    "    'date': 'last',\n",
    "    'weekday': 'last',\n",
    "    'month': 'last',\n",
    "    'year': 'last',\n",
    "    'wm_yr_wk': 'last'\n",
    "}\n",
    "features = compute_features(\n",
    "    df.groupby(agg_columns + ['d']).agg(agg_dict).reset_index(drop=False),\n",
    "    agg_columns,\n",
    "    sparse_features=False,\n",
    "    agg_level=level\n",
    ")\n",
    "# features[features['d'].isin([f'd_{int(i)}' for i in range(D_START_VAL, D_START_VAL+DAYS)])]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# verify that for the prediction period,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# days forward is not random but 0 to DAYS-1, increasing linearly\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(DAYS):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(features[features[\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39md_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(D_START_VAL\u001b[39m+\u001b[39mi)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdays_fwd\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique())\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_feature_engineering_quantiles.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39massert\u001b[39;00m features[features[\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39md_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(D_START_VAL\u001b[39m+\u001b[39mi)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdays_fwd\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()[\u001b[39m0\u001b[39m]\u001b[39m==\u001b[39mi\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "# verify that for the prediction period,\n",
    "# days forward is not random but 0 to DAYS-1, increasing linearly\n",
    "for i in range(DAYS):\n",
    "    assert len(features[features['d'] == f'd_{int(D_START_VAL+i)}']['days_fwd'].unique())==1\n",
    "    assert features[features['d'] == f'd_{int(D_START_VAL+i)}']['days_fwd'].unique()[0]==i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Runs Feature Engineering for Validation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_status\n",
    "def groupby_agglevel(df: pd.DataFrame, agg_columns: list, agg_dict: dict):\n",
    "    return df.groupby(agg_columns).agg(agg_dict).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 09:26:35 - __main__ - INFO - D_START_VAL: 1802\n",
      "2023-11-29 09:26:38 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-11-29 09:26:38 - compute_features - INFO - calling\n",
      "2023-11-29 09:26:38 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:26:38 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:26:38 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:26:38 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:26:38 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:26:38 - compute_features - INFO - calling\n",
      "2023-11-29 09:26:38 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:26:38 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:26:39 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:26:39 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:26:39 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:26:39 - compute_features - INFO - calling\n",
      "2023-11-29 09:26:39 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:26:39 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:26:41 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:26:41 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:26:41 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:26:41 - compute_features - INFO - calling\n",
      "2023-11-29 09:26:41 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:26:41 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:26:41 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:26:41 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:26:41 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:26:42 - compute_features - INFO - calling\n",
      "2023-11-29 09:26:42 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:26:42 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:26:43 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:26:43 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:26:43 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:26:43 - compute_features - INFO - calling\n",
      "2023-11-29 09:26:43 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:26:43 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:26:44 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:26:44 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:26:44 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:26:45 - compute_features - INFO - calling\n",
      "2023-11-29 09:26:45 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:26:45 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:26:48 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:26:48 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:26:48 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:26:48 - compute_features - INFO - calling\n",
      "2023-11-29 09:26:48 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:26:49 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:26:54 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:26:54 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:26:54 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:26:54 - compute_features - INFO - calling\n",
      "2023-11-29 09:26:54 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:26:56 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:06 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:06 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:06 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:07 - __main__ - INFO - D_START_VAL: 1830\n",
      "2023-11-29 09:27:10 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-11-29 09:27:10 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:10 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:11 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:11 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:11 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:11 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:11 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:11 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:11 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:11 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:11 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:11 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:11 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:11 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:11 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:13 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:13 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:13 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:13 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:13 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:13 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:13 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:13 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:13 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:14 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:14 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:14 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:15 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:15 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:15 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:15 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:15 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:15 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:16 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:16 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:16 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:17 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:17 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:17 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:20 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:20 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:20 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:20 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:20 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:21 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:25 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:25 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:25 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:25 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:26 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:27 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:36 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:36 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:36 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:38 - __main__ - INFO - D_START_VAL: 1858\n",
      "2023-11-29 09:27:41 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-11-29 09:27:41 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:41 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:41 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:41 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:41 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:41 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:41 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:41 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:41 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:42 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:42 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:42 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:42 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:42 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:42 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:43 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:43 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:43 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:44 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:44 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:44 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:44 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:44 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:44 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:44 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:44 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:44 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:45 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:45 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:45 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:45 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:45 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:46 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:47 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:47 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:47 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:47 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:47 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:48 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:51 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:51 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:51 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:51 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:51 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:52 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:27:56 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:27:56 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:27:56 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:27:56 - compute_features - INFO - calling\n",
      "2023-11-29 09:27:56 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:27:58 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:07 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:07 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:07 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:08 - __main__ - INFO - D_START_VAL: 1886\n",
      "2023-11-29 09:28:12 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-11-29 09:28:12 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:12 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:12 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:12 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:12 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:12 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:12 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:12 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:12 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:13 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:13 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:13 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:13 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:13 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:13 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:15 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:15 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:15 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:15 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:15 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:15 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:15 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:15 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:15 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:15 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:15 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:15 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:16 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:16 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:16 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:17 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:17 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:17 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:18 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:18 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:18 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:18 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:18 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:19 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:22 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:22 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:22 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:22 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:22 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:23 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:27 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:27 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:27 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:28 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:28 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:29 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:38 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:38 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:39 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:40 - __main__ - INFO - D_START_VAL: 1914\n",
      "2023-11-29 09:28:43 - __main__ - INFO - grouped df computed for all levels, original dataframe is not loaded\n",
      "2023-11-29 09:28:44 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:44 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:44 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:44 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:44 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:44 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:44 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:44 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:44 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:44 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:44 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:44 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:44 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:44 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:45 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:46 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:46 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:46 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:46 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:46 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:46 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:47 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:47 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:47 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:47 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:47 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:47 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:48 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:48 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:48 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:48 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:48 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:49 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:50 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:50 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:50 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:50 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:50 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:51 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:53 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:53 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:54 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:54 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:54 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:28:55 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:28:59 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:28:59 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:28:59 - __main__ - INFO - Computing state-id dummy\n",
      "2023-11-29 09:28:59 - compute_features - INFO - calling\n",
      "2023-11-29 09:28:59 - __main__ - INFO - Computing autocorrelation features\n",
      "2023-11-29 09:29:01 - __main__ - INFO - Computing price autocorrelation features\n",
      "2023-11-29 09:29:11 - __main__ - INFO - Computing unconditional sold values\n",
      "2023-11-29 09:29:11 - __main__ - INFO - Encoding date features to dummies\n",
      "2023-11-29 09:29:11 - __main__ - INFO - Computing state-id dummy\n"
     ]
    }
   ],
   "source": [
    "# params for un\n",
    "SPARSE_FEATURES = False\n",
    "TEST_RUN = False\n",
    "MAX_QUANTILE = 3\n",
    "# MAX_QUANTILE = 12 - MAX_QUANTILE\n",
    "\n",
    "# for each fold\n",
    "# 1830, 1858, 1886, 1914\n",
    "for D_START_VAL in D_CROSS_VAL_START_LIST:\n",
    "    logger.info(f'D_START_VAL: {D_START_VAL}')\n",
    "\n",
    "    # if all grouped dfs for all levels is already computed\n",
    "    # do not load original df at all\n",
    "    all_found = True\n",
    "    for agg_level in AGG_LEVEL_COLUMNS:\n",
    "        try:\n",
    "            aaaaa = pd.read_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet')\n",
    "        except:\n",
    "            all_found = False\n",
    "            logger.info(f'grouped df not computed for level: {agg_level}')\n",
    "        \n",
    "    if not all_found:\n",
    "        # pivot initial dataframe and compute features/targets\n",
    "        df_val, submission_idx_validation = data_preprocessing(\n",
    "            drop_days_after(sales_validation,\n",
    "            day_threshold = D_START_VAL), \n",
    "            calendar,\n",
    "            sell_prices\n",
    "        )\n",
    "        # drop all leading rows with leading zeros for each product\n",
    "        df_val_after_release = df_val[(df_val.wm_yr_wk > df_val.release)]\n",
    "        del df_val\n",
    "    else:\n",
    "        logger.info('grouped df computed for all levels, original dataframe is not loaded')\n",
    "\n",
    "    # set prediction values to nan values\n",
    "    # pred_index = df_val_after_release['d'].isin(D_CV_OOS)\n",
    "    # df_val_after_release.loc[pred_index, 'sold'] = np.nan\n",
    "    # del pred_index\n",
    "\n",
    "    # for each level\n",
    "    for agg_level, agg_columns in AGG_LEVEL_COLUMNS.items(): \n",
    "        # remove index to compute all\n",
    "        if agg_level in [f'Level{int(12 - i)}' for i in range(0,MAX_QUANTILE)]:\n",
    "            continue\n",
    "            \n",
    "        # group data for specific grouping columns per level\n",
    "        # and compute features\n",
    "        agg_dict = {\n",
    "            'sold': np.nansum,\n",
    "            'sell_price': np.nanmean,\n",
    "            'date': 'last',\n",
    "            'weekday': 'last',\n",
    "            'month': 'last',\n",
    "            'year': 'last',\n",
    "            'wm_yr_wk': 'last'\n",
    "        }\n",
    "\n",
    "        if len(agg_columns) == 0:\n",
    "            agg_columns = ['temp_id']\n",
    "        \n",
    "        # load grouped df if already exists\n",
    "        try:\n",
    "            aaaaa = pd.read_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet')\n",
    "            aaaaa = _down_cast(aaaaa)\n",
    "        except:\n",
    "            logger.info(f'not existing yet: ../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet')\n",
    "            logger.info(f'computing and storing grouped dataframe for level: {agg_level}')\n",
    "            # get data on aggregated level\n",
    "            if len(agg_columns) == 0 or 'temp_id' in agg_columns:\n",
    "                df_val_after_release['temp_id'] = 'temp_id'\n",
    "\n",
    "            aaaaa = groupby_agglevel(df_val_after_release, agg_columns + ['d'], agg_dict)\n",
    "            idx_keep = aaaaa['date'].notna()\n",
    "            aaaaa = aaaaa[idx_keep]\n",
    "            # to suitable type for .parquet\n",
    "            for c in aaaaa.columns:\n",
    "                if c not in agg_columns + ['d','date','weekday']:#['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'state_id']:\n",
    "                    aaaaa[c] = aaaaa[c].astype(np.float32)\n",
    "            aaaaa.to_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/grouped/grouped_{agg_level}.parquet', index=False)\n",
    "            aaaaa = _down_cast(aaaaa)\n",
    "            \n",
    "        # compute all features\n",
    "        features = compute_features(\n",
    "            aaaaa,\n",
    "            agg_columns,\n",
    "            sparse_features=SPARSE_FEATURES,\n",
    "            agg_level=agg_level\n",
    "        )\n",
    "        \n",
    "        # to suitable type for .parquet\n",
    "        for c in features.columns:\n",
    "            if c not in ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'state_id']:\n",
    "                features[c] = features[c].astype(np.float32)\n",
    "        \n",
    "        # format string and save file\n",
    "        agg_string = parse_columns_to_string(agg_columns)\n",
    "        if not TEST_RUN:\n",
    "            features.to_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/features/' + f'features_val_{agg_string}.parquet', index=False)\n",
    "            del features\n",
    "        else:\n",
    "            # features.to_parquet(f'../data/uncertainty/fold_{int(D_START_VAL)}/features/' + f'/test/features_val_{agg_string}.parquet', index=False)\n",
    "            pass\n",
    "\n",
    "    if not TEST_RUN:\n",
    "        try:\n",
    "            del df_val_after_release\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pivot initial dataframe and compute features/targets\n",
    "# df_eval, submission_idx_validation = data_preprocessing(sales_evaluation, calendar, sell_prices)\n",
    "# df_eval_after_release = df_eval[(df_eval.wm_yr_wk > df_eval.release)]\n",
    "# del df_eval\n",
    "\n",
    "# # set prediction values to nan values\n",
    "# pred_index = df_eval_after_release['d'].isin(D_CV_OOS)\n",
    "# df_eval_after_release.loc[pred_index, 'sold'] = np.nan\n",
    "# del pred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_RUN = False\n",
    "# for agg_level in AGG_LEVEL_COLUMNS:\n",
    "#     agg_columns = AGG_LEVEL_COLUMNS[agg_level]\n",
    "#     # get data on aggregated level\n",
    "#     if len(agg_columns) == 0:\n",
    "#         df_eval_after_release['temp_id'] = 'temp_id'\n",
    "#         agg_columns = ['temp_id']\n",
    "#     agg_dict = {\n",
    "#         'sold': np.nansum,\n",
    "#         'date': 'last',\n",
    "#         'weekday': 'last',\n",
    "#         'month': 'last'\n",
    "#     }\n",
    "#     features = compute_features(\n",
    "#         groupby_agglevel(df_eval_after_release, agg_columns + ['d'], agg_dict),\n",
    "#         agg_columns\n",
    "#     )\n",
    "    \n",
    "#     # to suitable format for .parquet\n",
    "#     for c in features.columns:\n",
    "#         if c not in ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'state_id']:\n",
    "#             features[c] = features[c].astype(np.float32)\n",
    "        \n",
    "#     # format string and save file\n",
    "#     agg_string = parse_columns_to_string(agg_columns)\n",
    "#     if not TEST_RUN:\n",
    "#         features.to_parquet(PRECOMPUTED_BASE_PATH + f'features_eval_{agg_string}.parquet', index=False)\n",
    "#         del features\n",
    "#     else:\n",
    "#         features.to_parquet(PRECOMPUTED_BASE_PATH + f'/test/features_eval_{agg_string}.parquet', index=False)\n",
    "        \n",
    "# if not TEST_RUN: \n",
    "#     del df_eval_after_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
