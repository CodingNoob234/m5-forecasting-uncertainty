{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import lightgbm as lgb\n",
    "from utils.utils import _down_cast, data_preprocessing, diff_lists, log_status\n",
    "from utils.utils import ensemble_submissions, ensemble_submissions_uncertainty\n",
    "from utils.metrics import WSPL\n",
    "from utils.configure_logger import configure_logger\n",
    "from utils.utils import prefixes_in_column\n",
    "from utils import constants\n",
    "\n",
    "configure_logger()\n",
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = constants.DATA_BASE_PATH\n",
    "DATA_BASE_PATH_UNCERTAINTY = constants.DATA_BASE_PATH_UNCERTAINTY\n",
    "SALES_EVALUATION = constants.SALES_EVALUATION \n",
    "SALES_VALIDATION = constants.SALES_VALIDATION\n",
    "CALENDAR = constants.CALENDAR \n",
    "SAMPLE_SUBMISSION = constants.SAMPLE_SUBMISSION \n",
    "SELL_PRICES = constants.SELL_PRICES\n",
    "\n",
    "PRECOMPUTED_BASE_PATH = constants.PRECOMPUTED_BASE_PATH #'../data/uncertainty/features/'\n",
    "\n",
    "DAYS: int = constants.DAYS #28\n",
    "QUANTILES: int = constants.QUANTILES \n",
    "\n",
    "AGG_LEVEL_COLUMNS = constants.AGG_LEVEL_COLUMNS\n",
    "D_CROSS_VAL_START_LIST = constants.D_CROSS_VAL_START_LIST\n",
    "\n",
    "# to simple get the precomputed name\n",
    "precomputed_name = lambda store, eval_val: f'processed_{store}_{eval_val}.pkl'\n",
    "\n",
    "TEST_PATH = constants.TEST_PATH#'test/'\n",
    "PREDICTION_BASE_PATH = constants.PREDICTION_BASE_PATH\n",
    "SUBMISSION_BASE_PATH = constants.SUBMISSION_BASE_PATH\n",
    "\n",
    "SUB_D_START_VAL: int = constants.SUB_D_START_VAL\n",
    "SUB_D_START_EVAL: int = constants.SUB_D_START_EVAL\n",
    "\n",
    "# the columns are always included after feature processing\n",
    "# because they are required in the training and submission format\n",
    "DROP_FEATURE_COLUMNS: list = constants.DROP_FEATURE_COLUMNS #['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'sold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GridSearch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_status\n",
    "def grid_search(\n",
    "    params: dict,\n",
    "    param_grid: dict,\n",
    "    features: pd.DataFrame, \n",
    "    targets: pd.DataFrame, \n",
    "    n_folds: int = 1,\n",
    "    fig: plt.Figure = None,\n",
    "    ax: plt.Axes = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Given a grid with hyperparameters, train LightGBM for all possible combinations.\n",
    "    Returns the parameter set with the best score and the dictionary with all results.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    \n",
    "    # to be sure\n",
    "    features = features.reset_index(drop=True)\n",
    "    targets = targets.reset_index(drop=True)\n",
    "\n",
    "    param_combinations = list(itertools.product(*param_grid.values()))\n",
    "    results = {}\n",
    "    \n",
    "    if PLOT_EVAL and not (fig and ax):\n",
    "        fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "\n",
    "    for i, param_combination in enumerate(param_combinations,1):\n",
    "        \n",
    "        # create dictionary with all parameters\n",
    "        param_combination = {k:v for k,v in zip(param_grid.keys(), param_combination)}\n",
    "        param_combination.update(params)\n",
    "                \n",
    "        # init dict\n",
    "        results[f\"combination_{i}\"] = {\n",
    "            'params': param_combination,\n",
    "            'res': []\n",
    "        }\n",
    "        \n",
    "        # perform n_folds\n",
    "        for j in range(n_folds):\n",
    "            \n",
    "            # kfold\n",
    "            features_train, features_validation, targets_train, targets_validation =\\\n",
    "                train_test_split(features, targets, train_size = .8, random_state=43)#shuffle=True)#, random_state=42)\n",
    "\n",
    "            # train lgb model\n",
    "            temp_dict = {} # this dict object will be used to add all (intermediate) evaluation scores during the training process\n",
    "            mod: lgb.Booster = lgb.train(param_combination, \n",
    "                train_set = lgb.Dataset(features_train, targets_train),\n",
    "                valid_sets = lgb.Dataset(features_validation, targets_validation),\n",
    "                evals_result = temp_dict,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "\n",
    "            # plot results\n",
    "            if PLOT_EVAL:\n",
    "                # if not ax:\n",
    "                evals = temp_dict['valid_0']['quantile']\n",
    "                ax.plot(range(1,len(evals)+1), np.log(evals), label = 'lr: ' + str(param_combination['learning_rate']))\n",
    "                ax.set_xlim(-100, len(evals)+100)\n",
    "                \n",
    "            # store results\n",
    "            results[f\"combination_{i}\"]['res']\\\n",
    "                .append(temp_dict[\"valid_0\"][\"quantile\"][-1],\n",
    "                )\n",
    "\n",
    "        # compute average results\n",
    "        results[f\"combination_{i}\"]['validation_score'] = \\\n",
    "            np.mean(results[f\"combination_{i}\"]['res'])\n",
    "            \n",
    "        # REMOVE\n",
    "        p = results[f\"combination_{i}\"]['params']\n",
    "        logger.info(f\"{p['learning_rate']} - {p['num_leaves']} - {p['n_estimators']}\" + ' - score: ' + str(np.mean(results[f\"combination_{i}\"]['res'])) + ' ' + str(np.std(results[f\"combination_{i}\"]['res'])))\n",
    "        # REMOVE\n",
    "        \n",
    "    # sort the results based on evaluation score\n",
    "    sorted_results = dict(sorted(results.items(), key=lambda item: item[1][\"validation_score\"]))\n",
    "    return list(sorted_results.values())[0], results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoadData\n",
    "This class is used to load data much faster. The class implementation prevents reloading and processing the features over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData:\n",
    "    \"\"\" Class to load data quickly (and prevent reloading) \"\"\"\n",
    "    def __init__(self):\n",
    "        self.level = None\n",
    "        \n",
    "    def prep_data(self,level, sub_d_start):\n",
    "        \"\"\" read the precomputed features and targets for specified aggregation level,  \"\"\"\n",
    "        # define params\n",
    "        agg_level = level\n",
    "        # sub_d_start: int = int(1886)\n",
    "        exclude_columns = []\n",
    "        test = False\n",
    "        type_of = 'val'\n",
    "\n",
    "        # read file\n",
    "        agg_columns = AGG_LEVEL_COLUMNS[agg_level]\n",
    "        if len(agg_columns) == 0:\n",
    "            agg_str: str = 'Total_X'\n",
    "        elif len(agg_columns) == 1:\n",
    "            agg_str: str = f'{agg_columns[0]}_X'\n",
    "        else:\n",
    "            agg_str: str = '_'.join(agg_columns)\n",
    "\n",
    "        # if level already loaded\n",
    "        if self.level == level:\n",
    "            pass\n",
    "        else:\n",
    "            self.level = level\n",
    "            logger.info('(re)loading features')\n",
    "            features = pd.read_parquet(f'../data/uncertainty/fold_{sub_d_start}/features/' + (TEST_PATH if test else '') + f'features_{type_of}_{agg_str}.parquet')\n",
    "            features = _down_cast(features)\n",
    "\n",
    "            group_columns = agg_columns\n",
    "            exclude_prefix_list = exclude_columns # unconditional, auto, momentum, seasonal\n",
    "            \n",
    "            features_gr = features.copy()\n",
    "            features_gr = features_gr[[c for c in features_gr if c.split('_')[0] not in exclude_prefix_list]]\n",
    "\n",
    "            # preparations\n",
    "            train_idx = features_gr['sold'].notna() & features_gr['d'].isin([f'd_{sub_d_start - 1 - i}' for i in range(1460)])\n",
    "            df_train = features_gr[train_idx]\n",
    "            features_train: pd.DataFrame = df_train.drop(DROP_FEATURE_COLUMNS, axis = 1, errors = 'ignore')\n",
    "            targets_train: pd.Series = df_train['sold']\n",
    "            self.features_train = features_train\n",
    "            self.targets_train = targets_train\n",
    "        \n",
    "    def get_prep_data(self):\n",
    "        return self.features_train, self.targets_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Training a Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 17:20:21 - __main__ - INFO - (re)loading features\n"
     ]
    }
   ],
   "source": [
    "# load data example, to investigate which features are computed, among other things\n",
    "level = 'Level1'\n",
    "dataLoader = LoadData()\n",
    "dataLoader.prep_data(level, 1914)\n",
    "features, targets = dataLoader.get_prep_data()\n",
    "# list(features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold\n",
    "prefixes = ['seasonal', 'auto_sold_ewm']\n",
    "features_train, features_validation, targets_train, targets_validation =\\\n",
    "    train_test_split(features, targets, test_size = 28, shuffle=False, random_state=42)\n",
    "\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -100,\n",
    "    'n_jobs': 4,\n",
    "    # 'subsample': .9,\n",
    "    # 'subsample_freq': 1,\n",
    "    \"num_leaves\": 30,\n",
    "    \"min_child_weight\": .1,\n",
    "    \"min_child_samples\": 4,\n",
    "    \"hist_pool_size\": 1000,\n",
    "    'feature_fraction': 0.9, #.5\n",
    "    # 'bagging_fraction': .8,\n",
    "    \"learning_rate\": 0.005, #0.07,\n",
    "    \"n_estimators\": 2000,#100\n",
    "    \"max_depth\": 10,\n",
    "    # 'reg_sqrt': True,\n",
    "    # 'req_lambda': .00001,\n",
    "    # 'reg_alpha': .00001,\n",
    "    'alpha': .25,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# # train lgb model       \n",
    "# for q in [0.005, 0.025, 0.135, 0.25, 0.5, 0.75, 0.865, 0.975, 0.995]:\n",
    "#     params['alpha'] = q \n",
    "#     temp_dict = {}\n",
    "#     mod: lgb.Booster = lgb.train(params, \n",
    "#         train_set = lgb.Dataset(features_train, targets_train),\n",
    "#         valid_sets = lgb.Dataset(features_validation, targets_validation),\n",
    "#         evals_result = temp_dict,\n",
    "#         verbose_eval = False\n",
    "#     )\n",
    "#     plt.plot(mod.predict(features_validation), label = f'{q}')\n",
    "\n",
    "# plt.scatter(range(len(targets_validation.index)), targets_validation, label = 'true', s = 10)\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': 4,\n",
    "    'hist_pool_size': 1000,\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10],\n",
    "    'n_estimators': [200, 800],\n",
    "    # 'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n",
    "    'min_child_samples': [4],\n",
    "    'min_child_weight': [0.1 ],\n",
    "    'num_leaves': [30], # 50, 70, 90, ],\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.02],   # 0.02, 0.03,        \n",
    "    'subsample': [  0.9, 1],\n",
    "    'subsample_freq': [1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test grid search for all quantiles\n",
    "# for q in QUANTILES[:1]:\n",
    "    \n",
    "#     # of course, update quantile in params\n",
    "#     params['alpha'] = q\n",
    "#     best_res, res = grid_search(params, param_grid, features_train, targets_train, 1)\n",
    "#     logger.info(best_res['params'])\n",
    "    \n",
    "#     mod = lgb.train(best_res['params'],\n",
    "#         train_set = lgb.Dataset(features_train, targets_train)\n",
    "#     )\n",
    "#     predictions = mod.predict(features_validation)\n",
    "#     plt.plot(predictions, label = str(q))\n",
    "\n",
    "# plt.scatter(range(len(targets_validation)), targets_validation)\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model for all quantiles for specific fold + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_level_all_quantiles(\n",
    "    agg_level: str, \n",
    "    type_of: str, \n",
    "    sub_d_start: int, \n",
    "    exclude_columns: list = [], \n",
    "    include_columns: list = None,\n",
    "    test: bool = False, \n",
    "    do_grid_search: bool = False, \n",
    "    store_submissions_path: str = None, \n",
    "    normalize: bool = False,\n",
    "):\n",
    "    \"\"\" \n",
    "    Train, for a specific aggregation level, models for all quantiles.\n",
    "    For aggregation levels 10, 11 and 12, undersampling is used to drastically reduce training time.\n",
    "    \"\"\"\n",
    "    if PLOT_EVAL and PLOT_PREDICTIONS:\n",
    "        raise ValueError('PLOT_EVAL and PLOT_PREDICTIONS cannot be both True')\n",
    "    \n",
    "    ALWAYS_KEEP_COLUMNS = ['days_fwd', 'sold', 'd']\n",
    "    \n",
    "    # transform 'level{i}' to agg columns concatenated\n",
    "    agg_columns = AGG_LEVEL_COLUMNS[agg_level]\n",
    "    if len(agg_columns) == 0:\n",
    "        agg_str: str = 'Total_X'\n",
    "    elif len(agg_columns) == 1:\n",
    "        agg_str: str = f'{agg_columns[0]}_X'\n",
    "    else:\n",
    "        agg_str: str = '_'.join(agg_columns)\n",
    "\n",
    "    # load feature set\n",
    "    logger.info('loading features')\n",
    "    features = pd.read_parquet(f'../data/uncertainty/fold_{sub_d_start}/features/' + (TEST_PATH if test else '') + f'features_{type_of}_{agg_str}.parquet')\n",
    "    features = _down_cast(features)\n",
    "    features_gr = features.copy()\n",
    "    \n",
    "    # seperate train/pred indices\n",
    "    train_idx = features_gr['sold'].notna() & features_gr['d'].isin([f'd_{sub_d_start - 1 - i}' for i in range(1300)])\n",
    "    pred_idx = features_gr['d'].isin([f'd_{sub_d_start + i}' for i in range(DAYS)])\n",
    "\n",
    "    group_columns = agg_columns\n",
    "    res: list = []\n",
    "    \n",
    "    def check_any_prefix_matches(column, prefixes):\n",
    "        \"\"\" Return true if any prefix is in column \"\"\"\n",
    "        for prefix in prefixes:\n",
    "            if prefix in column:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # select features\n",
    "    if USE_ALL or 'kbest' in include_columns:\n",
    "        columns = features_gr.columns\n",
    "    elif SPARSE_FEATURES:\n",
    "        columns = [c for c in features_gr.columns if c in SPARSE_FEATURES]\n",
    "    elif include_columns == None:\n",
    "        # exclude features from exclusion prefix list\n",
    "        exclude_prefix_list = exclude_columns \n",
    "        columns = [c for c in features_gr.columns if not check_any_prefix_matches(c, exclude_prefix_list)]\n",
    "    elif isinstance(include_columns, list):\n",
    "        include_prefix_list = include_columns\n",
    "        columns = [c for c in features_gr.columns if check_any_prefix_matches(c, include_prefix_list)]\n",
    "\n",
    "    # add always keep columns to selected features\n",
    "    for column in ALWAYS_KEEP_COLUMNS + group_columns:\n",
    "        if column not in columns:\n",
    "            columns.append(column)\n",
    "            \n",
    "    # get final dataframes\n",
    "    features_gr = features_gr[columns]\n",
    "    df_pred = features_gr[pred_idx]\n",
    "    df_train = features_gr[train_idx]\n",
    "    if agg_level not in ['Level9', 'Level10', 'Level11', 'Level12']:\n",
    "        df_train = df_train[df_train['sold'] >= 50]\n",
    "\n",
    "    from copy import deepcopy\n",
    "    temp_drop_feature_columns = deepcopy(DROP_FEATURE_COLUMNS)\n",
    "    if not USE_ALL and 'kbest' not in include_columns:\n",
    "        if 'state_id' in include_prefix_list:\n",
    "            temp_drop_feature_columns.remove('state_id')\n",
    "        if 'store_id' in include_prefix_list:\n",
    "            temp_drop_feature_columns.remove('store_id')\n",
    "    if USE_ALL or 'kbest' in include_columns:\n",
    "        temp_drop_feature_columns.remove('state_id')\n",
    "        temp_drop_feature_columns.remove('store_id')\n",
    "        \n",
    "    features_train: pd.DataFrame = df_train.drop(temp_drop_feature_columns, axis = 1, errors = 'ignore')\n",
    "    # logger.info(f'feature: {str(features_train.columns)}')\n",
    "    targets_train: pd.Series = df_train['sold']\n",
    "    features_predict: pd.DataFrame = df_pred.drop(temp_drop_feature_columns, axis = 1, errors = 'ignore')\n",
    "    targets_test: pd.Series = df_pred['sold']\n",
    "    \n",
    "    #### SELECT FEATURES ####\n",
    "    if 'kbest' in include_columns:\n",
    "        # cannot do selectkbest for category variables\n",
    "        # others of these should always be kept\n",
    "        exclude_from_kbest = ['state_id', 'store_id', 'seasonal_weekday', 'seasonal_monthday', 'seasonal_month', 'days_fwd']\n",
    "        # temp_drop_idx = features_train.notna().all(axis=1)\n",
    "        temp_drop_idx = features_train.drop(exclude_from_kbest, axis=1, errors='ignore').fillna(0).notna().all(axis=1)\n",
    "        from sklearn import metrics\n",
    "        from sklearn import feature_selection\n",
    "        fit = SelectKBest(\n",
    "                k=5,\n",
    "                # score_func=metrics.mean_pinball_loss\n",
    "                score_func=feature_selection.f_regression\n",
    "            ).fit(\n",
    "                features_train.drop(exclude_from_kbest, axis=1, errors='ignore').fillna(0)[temp_drop_idx], \n",
    "                targets_train[temp_drop_idx]\n",
    "            )\n",
    "        # print(fit.get_feature_names_out())\n",
    "        features_keep = list(fit.get_feature_names_out())\n",
    "        for c in exclude_from_kbest:\n",
    "            if c in features_train.columns:\n",
    "                features_keep.append(c)\n",
    "        print(features_keep)\n",
    "        features_train = features_train[features_keep]\n",
    "        features_predict = features_predict[features_keep]\n",
    "    #### SELECT FEATURES ####\n",
    "    \n",
    "    # undersample data\n",
    "    if agg_level in undersampling_dict.keys() and HIGH_UNDERSAMPLING:\n",
    "        undersampling_pct = undersampling_dict[agg_level]\n",
    "        features_train, _, targets_train, _ = train_test_split(features_train, targets_train, train_size = undersampling_pct, shuffle=True, random_state=43)\n",
    "\n",
    "    # normalise targets\n",
    "    if normalize:\n",
    "        logger.info('scaling targets')\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        targets_train = scaler.fit_transform(targets_train.values.reshape(-1,1))\n",
    "        \n",
    "    # REMOVE THIS\n",
    "    import matplotlib.pyplot as plt\n",
    "    if PLOT_PREDICTIONS:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (10,5))\n",
    "        aaa = [i for i in range(targets_test.shape[0])]\n",
    "    # REMOVE THIS\n",
    "        \n",
    "    # train model for all quantiles\n",
    "    for quantile in QUANTILES:\n",
    "        \n",
    "        # perform grid search for best parameters\n",
    "        if do_grid_search == True:\n",
    "            # split data to training and testing\n",
    "            logger.info('perform gridsearch')\n",
    "            params['alpha'] = quantile\n",
    "            if PLOT_EVAL:\n",
    "                fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "                ax.set_title(f'LightGBM Out-of-Sample Pinball-Loss - {agg_level.capitalize()} - q: {quantile}')\n",
    "            best_combination, results = grid_search(params, param_grid, features_train, targets_train, n_folds = 1, fig = fig, ax = ax)\n",
    "            # del train_data; del validation_data\n",
    "            params_grid_train = best_combination[\"params\"]\n",
    "            logger.info(f'q: {quantile} - cv best params: {params_grid_train}')\n",
    "            \n",
    "            if PLOT_EVAL:\n",
    "                exclude_names = 'None' if len(include_prefix_list) == 0 else '_'.join(include_prefix_list)\n",
    "                ax.set_xlabel('Number of Trained Trees')\n",
    "                ax.set_ylabel('Log(Pinball-loss)')\n",
    "                ax.legend()\n",
    "                ax.grid()\n",
    "                fig.tight_layout()\n",
    "                fig.savefig('../figure/results/' + f'training_iteration_f_{sub_d_start}_include_{exclude_names}_q={quantile}.png', dpi=300)\n",
    "                plt.show()\n",
    "        \n",
    "        else:\n",
    "            params_grid_train = PARAM_GRID_TRAIN\n",
    "            params_grid_train['alpha'] = quantile\n",
    "\n",
    "        # train_best_model\n",
    "        # logger.info(f'features: {str(features_train.columns)}')\n",
    "        mod = lgb.train(params_grid_train,\n",
    "            train_set = lgb.Dataset(features_train, targets_train)\n",
    "        )\n",
    "        # create filepath to store model in\n",
    "        group_names = '_'.join(group_columns)\n",
    "        if group_names == '':\n",
    "            group_names = 'Total_X'\n",
    "        if USE_ALL:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_use_all_q={quantile}.joblib'\n",
    "        elif 'kbest' in include_columns:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_include_k_best_q={quantile}.joblib'\n",
    "        elif SPARSE_FEATURES:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_sparse_q={quantile}.joblib' \n",
    "        elif include_columns == None:\n",
    "            exclude_names = 'None' if len(exclude_prefix_list) == 0 else '_'.join(exclude_prefix_list)\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_exclude_{exclude_names}_q={quantile}.joblib'\n",
    "        elif isinstance(include_columns, list):\n",
    "            exclude_names = 'None' if len(include_prefix_list) == 0 else '_'.join(include_prefix_list)\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_include_{exclude_names}_q={quantile}.joblib'\n",
    "\n",
    "        # save model under filepath\n",
    "        import joblib\n",
    "        joblib.dump(mod, file_path)\n",
    "        \n",
    "        # make predictions\n",
    "        predictions = mod.predict(features_predict)\n",
    "        if normalize:\n",
    "            predictions = scaler.inverse_transform(predictions.reshape(-1,1)).reshape(-1,)\n",
    "        \n",
    "        if PLOT_PREDICTIONS:\n",
    "            ax.plot(aaa, predictions, label = f'{quantile}')\n",
    "        \n",
    "        # store predictions\n",
    "        df_p = pd.DataFrame(\n",
    "            {\n",
    "                'pred': predictions,\n",
    "                'd': df_pred['d'],\n",
    "            }\n",
    "        )\n",
    "        df_p['quantile'] = quantile\n",
    "        df_p['Level'] = agg_level\n",
    "        df_p['type_of'] = 'validation' if type_of == 'val' else 'evaluation'\n",
    "        if len(agg_columns) == 0:\n",
    "            df_p['agg_column1'] = 'Total'\n",
    "            df_p['agg_column2'] = 'X'\n",
    "        elif len(agg_columns) == 1:\n",
    "            df_p['agg_column1'] = df_pred[agg_columns[0]].values\n",
    "            df_p['agg_column2'] = 'X'\n",
    "        else:\n",
    "            df_p['agg_column1'] = df_pred[agg_columns[0]].values\n",
    "            df_p['agg_column2'] = df_pred[agg_columns[1]].values\n",
    "            \n",
    "        df_p = df_p[['Level', 'agg_column1', 'agg_column2', 'd', 'quantile', 'pred', 'type_of']]\n",
    "        \n",
    "        res.append(_down_cast(df_p))\n",
    "        \n",
    "    # REMOVE THIS\n",
    "    if PLOT_PREDICTIONS:\n",
    "        plt.show()\n",
    "    # REMOVE THIS\n",
    "        \n",
    "    # remove to reduce memory usage asap\n",
    "    del features\n",
    "        \n",
    "    # storing predictions in specified file + folder\n",
    "    df_sub_val = pd.concat(res)\n",
    "    group_names = '_'.join(group_columns)\n",
    "    if group_names == '':\n",
    "        group_names = 'Total_X'\n",
    "\n",
    "    if store_submissions_path:\n",
    "        if USE_ALL:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_use_all.csv'\n",
    "        elif 'kbest' in include_columns:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_include_k_best.csv'\n",
    "        elif SPARSE_FEATURES:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_sparse.csv'  \n",
    "        elif include_columns == None:\n",
    "            exclude_names = 'None' if len(exclude_prefix_list) == 0 else '_'.join(exclude_prefix_list)\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_exclude_{exclude_names}.csv'\n",
    "        elif isinstance(include_columns, list):\n",
    "            exclude_names = 'None' if len(include_prefix_list) == 0 else '_'.join(include_prefix_list)\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_include_{exclude_names}.csv'\n",
    "\n",
    "        df_sub_val.to_csv(file_path, index = False)\n",
    "        logger.info('saved under: ' + file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seasonal',\n",
       " 'state vs. store',\n",
       " 'ewm vs. ma',\n",
       " 'quantiles vs. std',\n",
       " 'price auto/momentum',\n",
       " 'best models',\n",
       " 'full vs. sparse ma',\n",
       " 'sparse vs. kbest',\n",
       " 'full vs. sparse']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPERIMENTS_DICT = {\n",
    "    \"seasonal\": {\n",
    "        \"BASE\": [],\n",
    "        \"INCLUDE_COLUMNS_LIST\": [\n",
    "            ['auto_sold_ewm'],\n",
    "            ['seasonal_weekday','auto_sold_ewm'],\n",
    "            ['seasonal_monthday','auto_sold_ewm'],\n",
    "            ['seasonal_weekday','seasonal_monthday','auto_sold_ewm'],\n",
    "            ['seasonal','auto_sold_ewm'],\n",
    "        ]\n",
    "    },\n",
    "    \"state vs. store\": {\n",
    "        \"BASE\": ['seasonal', 'auto_sold_ma'],\n",
    "        \"INCLUDE_COLUMNS_LIST\": [\n",
    "            [],\n",
    "            ['state_id',],\n",
    "            ['store_id',],\n",
    "            ['state_id', 'store_id']\n",
    "        ]\n",
    "    },\n",
    "    \"ewm vs. ma\": {\n",
    "        \"BASE\": ['seasonal'],\n",
    "        \"INCLUDE_COLUMNS_LIST\": [\n",
    "            ['auto_sold_ewm'],\n",
    "            ['auto_sold_ma'],\n",
    "            ['auto_sold_ewm', 'auto_sold_ma'],\n",
    "        ]\n",
    "    },\n",
    "    \"quantiles vs. std\": {\n",
    "        \"BASE\": ['seasonal', 'auto_sold_ma'],\n",
    "        \"INCLUDE_COLUMNS_LIST\": [\n",
    "            [],\n",
    "            ['auto_sold_qtile'],\n",
    "            ['auto_sold_std'],\n",
    "            ['auto_sold_qtile','auto_sold_std'],   \n",
    "        ]\n",
    "    },\n",
    "    \"price auto/momentum\": {\n",
    "        \"BASE\": ['seasonal', 'auto_sold_ma'],\n",
    "        \"INCLUDE_COLUMNS_LIST\": [\n",
    "            [],\n",
    "            ['price_auto_std'],\n",
    "            ['price_momentum'],\n",
    "            ['price_uncond'],\n",
    "            ['price_auto_std', 'price_momentum'],\n",
    "            ['price_auto_std', 'price_momentum', 'price_uncond']\n",
    "        ]\n",
    "    },\n",
    "    \"best models\": {\n",
    "        \"BASE\": ['seasonal'],\n",
    "        \"INCLUDE_COLUMNS_LIST\": [\n",
    "            ['auto_sold_ma', 'state_id', 'store_id'],\n",
    "            ['auto_sold_ma', 'auto_sold_std', 'state_id', 'store_id'],\n",
    "        ]\n",
    "    },\n",
    "    \"full vs. sparse ma\" : {\n",
    "        \"BASE\": ['seasonal'],\n",
    "        \"INCLUDE_COLUMNS_LIST\": [\n",
    "            ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm', 'state_id', 'store_id'],\n",
    "            ['auto_sold_ma_28', 'auto_sold_ma_56', 'auto_sold_ma_168', 'state_id', 'store_id']\n",
    "        ]\n",
    "    },\n",
    "    \"sparse vs. kbest\": {\n",
    "        \"BASE\": ['seasonal', 'state_id', 'store_id'],\n",
    "        \"INCLUDE_COLUMNS_LIST\": [\n",
    "            ['auto_sold', 'price', 'kbest'],\n",
    "            ['auto_sold_ewm_112', 'auto_sold_ewm_28',\n",
    "             'auto_sold_qtile_28_0.5', 'auto_sold_ma_28', \n",
    "             'auto_sold_qtile_28_0.9',],\n",
    "        ]\n",
    "    },\n",
    "    'full vs. sparse': {\n",
    "        \"BASE\": ['seasonal'],\n",
    "        \"INCLUDE_COLUMNS_LIST\": [\n",
    "            ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm', 'state_id', 'store_id'],\n",
    "            ['auto_sold_ma_28', 'auto_sold_ma_56', 'auto_sold_ma_168', 'state_id', 'store_id'],\n",
    "            ['auto_sold_std_3', 'auto_sold_std_56', 'auto_sold_std_168', \n",
    "            'auto_sold_ma_7',  'auto_sold_ma_28', 'auto_sold_ma_56', \n",
    "            'auto_sold_qtile_28_0.25', 'auto_sold_qtile_168_0.25', 'auto_sold_qtile_56_0.1', \n",
    "            'state_id', 'store_id'],\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "list(EXPERIMENTS_DICT.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_ALL = False\n",
    "SPARSE_FEATURES = None\n",
    "PLOT_PREDICTIONS = False\n",
    "\n",
    "undersampling_dict = {\n",
    "    'Level10': .1, #.001\n",
    "    'Level11': .1, #.0001\n",
    "    'Level12': .1 #.00001\n",
    "}\n",
    "\n",
    "HIGH_UNDERSAMPLING = True\n",
    "TEST_NUMBER = 9 # 9\n",
    "TEST_NUMB = 0 # 0\n",
    "PARAM_GRID_TRAIN = {\n",
    "    'objective': 'quantile',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': 4,\n",
    "    \"num_leaves\": 10, # 30\n",
    "    \"hist_pool_size\": 300,\n",
    "    \"learning_rate\": .01, # .01\n",
    "    \"n_estimators\": 1000, #1000\n",
    "    \"max_depth\": 10, #10\n",
    "}\n",
    "PARAM_GRID_TRAIN_HIGH_LEVEL = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': 4,\n",
    "    \"num_leaves\": 30,\n",
    "    \"hist_pool_size\": 300,\n",
    "    # 'feature_fraction': 0.9, #.5\n",
    "    # 'bagging_fraction': .8,\n",
    "    \"learning_rate\": .01, # .01 always\n",
    "    \"n_estimators\": 3000, # 3000 always\n",
    "    \"max_depth\": 10,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -100,\n",
    "    'n_jobs': 4,\n",
    "    # 'eval_at': 10,\n",
    "    'hist_pool_size': 300,\n",
    "    'verbose_eval': -100,\n",
    "}\n",
    "param_grid = {\n",
    "    'max_depth': [10,],\n",
    "    'n_estimators': [3000],#[200, 500, 1000, 2000],\n",
    "    # 'min_child_samples': [4],\n",
    "    # 'min_child_weight': [0,0.1],\n",
    "    'num_leaves': [30], #[10, 30, 90]\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.02]# [.001, .005, .01, .02],#[0.04, 0.07, 0.1],   # 0.02, 0.03,        \n",
    "    # 'subsample': [ 0.9, 1 ],\n",
    "    # 'subsample_freq': [1],\n",
    "}\n",
    "\n",
    "# ALL_PREFIXES = ['auto_sold', 'auto_sold_ma', 'auto_sold_std', 'auto_sold_ewm', 'auto_sold_qtile',\n",
    "#     'price_momentum', 'price_uncond', 'price_auto_std','seasonal_', 'state_', 'store_',\n",
    "# ]\n",
    "\n",
    "experiment_name = 'seasonal'\n",
    "experiment_name = 'full vs. sparse'\n",
    "experiment_specs = EXPERIMENTS_DICT[experiment_name]\n",
    "BASE = experiment_specs['BASE']\n",
    "INCLUDE_COLUMNS_LIST = experiment_specs['INCLUDE_COLUMNS_LIST']\n",
    "\n",
    "INCLUDE_COLUMNS_LIST = [BASE + i for i in INCLUDE_COLUMNS_LIST]\n",
    "DO_GRID_SEARCH = True\n",
    "PLOT_EVAL = True\n",
    "\n",
    "EXCLUDE_COLUMNS_LIST = ()\n",
    "logger.info('starting with all EXCLUDE_COLUMNS')\n",
    "for exclude_columns in EXCLUDE_COLUMNS_LIST: # for each specified feature combination\n",
    "    logger.info(f'Exclude columns: {str(exclude_columns)}')\n",
    "    for sub_d_start in D_CROSS_VAL_START_LIST:\n",
    "        for agg_level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]: # for each aggregation level\n",
    "            logger.info(f'starting with agg_level: {agg_level}')\n",
    "            train_level_all_quantiles(\n",
    "                agg_level,\n",
    "                sub_d_start=sub_d_start,\n",
    "                type_of='val', \n",
    "                exclude_columns=exclude_columns,\n",
    "                do_grid_search=DO_GRID_SEARCH,\n",
    "                store_submissions_path=None#'temp_submissions/',\n",
    "            )\n",
    "logger.info('finished all EXCLUDE_COLUMNS')\n",
    "\n",
    "logger.info('---------------------------------')            \n",
    "logger.info('starting with all INCLUDE_COLUMNS')            \n",
    "for include_columns in INCLUDE_COLUMNS_LIST: # for each specified feature combination\n",
    "    logger.info(f'Include columns: {str(include_columns)}')\n",
    "    for sub_d_start in D_CROSS_VAL_START_LIST:\n",
    "        for agg_level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]: # for each aggregation level\n",
    "            logger.info(f'starting with agg_level: {agg_level}')\n",
    "            train_level_all_quantiles(\n",
    "                agg_level,\n",
    "                sub_d_start=sub_d_start,\n",
    "                type_of='val', \n",
    "                exclude_columns=None,\n",
    "                include_columns=include_columns,\n",
    "                do_grid_search=DO_GRID_SEARCH,\n",
    "                store_submissions_path=None#'temp_submissions/',\n",
    "            )\n",
    "logger.info('finished all INCLUDE_COLUMNS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load val + eval prediction files and merge to one submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_columns = '_'.join([])\n",
    "# if exclude_columns == '':\n",
    "#     exclude_columns = 'None'\n",
    "\n",
    "# dfs: list = []\n",
    "# for level in AGG_LEVEL_COLUMNS:\n",
    "#     agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "#     group_names = '_'.join(agg_columns)\n",
    "#     if group_names == '':\n",
    "#         group_names = 'Total_X'\n",
    "#     i = str(1914)\n",
    "#     dfs.append(\n",
    "#         f'../data/uncertainty/fold_{i}/temp_submissions/' + f'lgb_multivariate_val_non_transposed_{group_names}_exclude_{exclude_columns}.csv',\n",
    "#     )\n",
    "\n",
    "# df_sub_val = ensemble_submissions_uncertainty(dfs)\n",
    "# transpose = True\n",
    "# if transpose == True:\n",
    "#     sub_validation = df_sub_val.pivot(index='id', columns='d', values='pred').reset_index(drop=False)\n",
    "#     sub_validation.columns = [\"id\"] + [f\"F{i}\" for i in range(1,DAYS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_concat_predictions(fold_name: int, exclude_columns: list = [], include_columns: list = [], sparse = False, use_all = False, load_submissions_path: str = 'temp_submissions/'):\n",
    "    \"\"\" \n",
    "    For specified fold, read the predictions for all aggregation levels \n",
    "    and stack them together in one dataframe.\n",
    "    \"\"\"\n",
    "    # D_CV_START_LIST\n",
    "    # if fold_name not in D_CV_START_LIST:\n",
    "        # raise ValueError('fold_name must be a value in D_CV_START_LIST')\n",
    "        \n",
    "    exclude_columns = '_'.join(exclude_columns)\n",
    "    if exclude_columns == '':\n",
    "        exclude_columns = 'None'\n",
    "\n",
    "    logger.info('loading files under path:' + f'../data/uncertainty/fold_{fold_name}/' + load_submissions_path)\n",
    "\n",
    "    dfs: list = []\n",
    "    for level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]:\n",
    "        agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "        group_names = '_'.join(agg_columns)\n",
    "        if group_names == '':\n",
    "            group_names = 'Total_X'\n",
    "        \n",
    "        file_path = f'../data/uncertainty/fold_{str(fold_name)}/' + load_submissions_path \n",
    "        file_path += f'lgb_val_nt_{group_names}_'\n",
    "        if use_all:\n",
    "            file_path += f'use_all.csv'  \n",
    "        elif include_columns == None:\n",
    "            file_path += f'exclude_{\"_\".join(exclude_columns)}.csv'            \n",
    "        elif isinstance(include_columns, list):\n",
    "            file_path += f'include_{\"_\".join(include_columns)}.csv'\n",
    "        \n",
    "        dfs.append(file_path)\n",
    "    return ensemble_submissions_uncertainty(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE = ['seasonal',]\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     # ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm', 'state_id', 'store_id'],\n",
    "#     ['auto_sold_std_3', 'auto_sold_std_56', 'auto_sold_std_168', \n",
    "#      'auto_sold_ma_7',  'auto_sold_ma_28', 'auto_sold_ma_56', \n",
    "#      'auto_sold_qtile_28_0.25', 'auto_sold_qtile_168_0.25', 'auto_sold_qtile_56_0.1', \n",
    "#      'state_id', 'store_id'],\n",
    "#     # ['kbest']\n",
    "# ]\n",
    "# \n",
    "# BASE = ['seasonal']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm', 'state_id', 'store_id'],\n",
    "#     ['auto_sold_ma_28', 'auto_sold_ma_56', 'auto_sold_ma_168', 'state_id', 'store_id']\n",
    "# ]\n",
    "\n",
    "# include_columns = BASE + INCLUDE_COLUMNS_LIST[1]\n",
    "# include_columns_str = '_'.join(include_columns)\n",
    "\n",
    "# dfs: list = []\n",
    "# for level in AGG_LEVEL_COLUMNS:\n",
    "#     agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "#     group_names = '_'.join(agg_columns)\n",
    "#     if group_names == '':\n",
    "#         group_names = 'Total_X'\n",
    "        \n",
    "#     dfs.append(\n",
    "#         f'../data/uncertainty/fold_{1914}/temp_submissions/lgb_val_nt_{group_names}_include_{include_columns_str}.csv'\n",
    "#     )\n",
    "\n",
    "# df_sub_eval = ensemble_submissions_uncertainty(dfs)\n",
    "# transpose = True\n",
    "# if transpose == True:\n",
    "#     sub_evaluation = df_sub_eval.pivot(index='id', columns='d', values='pred').reset_index(drop=False)\n",
    "#     sub_evaluation.columns = [\"id\"] + [f\"F{i}\" for i in range(1,DAYS+1)]\n",
    "\n",
    "# sub_evaluation2 = sub_evaluation.copy()\n",
    "# sub_evaluation2['id'] = sub_evaluation2['id'].str.removesuffix('_validation') + '_evaluation'\n",
    "# \n",
    "# pd.concat([sub_evaluation, sub_evaluation2]).to_csv('../data/uncertainty/fold_1914/final_submissions/' + f'submission_lgb_ensemble{exclude_columns}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_evaluation = pd.read_csv('../submissions/submission_baseline_evaluation.csv').drop(['Unnamed: 0'], axis=1)\n",
    "# pd.concat([sub_validation, sub_evaluation]).to_csv(SUBMISSION_BASE_PATH + f'submission_lgb_ensemble{exclude_columns}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation Prediction, we can compute WRMSSE locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these variables are used later on\n",
    "FORCE_RELOAD = False\n",
    "try:\n",
    "    # simple code to check if variable exists\n",
    "    d_int + 1\n",
    "    if FORCE_RELOAD:\n",
    "        raise Exception()\n",
    "except:\n",
    "    # if not, load again\n",
    "    # takes about 2-3 minutes to reload and parse\n",
    "    # not the most beautiful method but it works\n",
    "    d = pd.read_parquet('../data/uncertainty/cv_template/temp.parquet')\n",
    "    try:\n",
    "        d_int = pd.read_parquet('../data/uncertainty/cv_template/temp_d_int.parquet')['d_int']\n",
    "    except:\n",
    "        d_int = d['d'].apply(lambda x: int(x.split('_')[1]))\n",
    "        d_int.to_frame('d_int').to_parquet('../data/uncertainty/cv_template/temp_d_int.parquet', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cv(df: pd.DataFrame, df_sub: pd.DataFrame):\n",
    "    \n",
    "    # to be able to merge\n",
    "    # df_sub['id_merge'] = df_sub['id'].str.split('.')\\\n",
    "    #     .apply(lambda x: x[0])\n",
    "    df_sub['id_merge'] = df_sub['id']\\\n",
    "        .apply(lambda x: x.split('.')[0])\n",
    "    # df_sub['quantile'] = df_sub['id'].str.split('.')\\\n",
    "    #     .apply(lambda x: float('.'.join([x[-2], x[-1].split('_')[0]])))\n",
    "    df_sub['quantile'] = df_sub['id']\\\n",
    "        .apply(\n",
    "            lambda x: float(\n",
    "                '.'.join([\n",
    "                x.split('.')[-2], \n",
    "                x.split('.')[-1].split('_')[0]\n",
    "                ])\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # merge predictions in cv template\n",
    "    p = pd.merge(\n",
    "        df,\n",
    "        df_sub,\n",
    "        how='left',\n",
    "        on=['id_merge', 'd']\n",
    "    )\n",
    "    # del df; del df_sub_val\n",
    "    p['id_merge'] = p['id_merge'].astype(str)\n",
    "\n",
    "    for c in ['sold', 'revenue']:\n",
    "        p[c] = p[c].astype(np.float32)\n",
    "    # d = d[d_int < (D_CV_START + 28)]\n",
    "\n",
    "    return WSPL(p, [f'd_{i}' for i in range(D_CV_START, D_CV_START + 500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 21:02:22 - __main__ - INFO - start evaluating exclude columns\n",
      "2023-12-16 21:02:22 - __main__ - INFO - start evaluating include columns\n",
      "2023-12-16 21:02:22 - __main__ - INFO - --------------- ['k_best'] ---------------\n",
      "2023-12-16 21:02:25 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1802/temp_submissions/\n",
      "2023-12-16 21:02:45 - utils.metrics - INFO - reading weights file\n",
      "2023-12-16 21:02:45 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-16 21:03:13 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-16 21:03:19 - utils.metrics - INFO - Level1 - 0.1712239568255422\n",
      "2023-12-16 21:03:20 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:03:24 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:03:41 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:03:42 - utils.metrics - INFO - Level2 - 0.24072055648767068\n",
      "2023-12-16 21:03:42 - utils.metrics - INFO - Level3 - 0.2595724408822137\n",
      "2023-12-16 21:03:42 - utils.metrics - INFO - Level4 - 0.20524983388934617\n",
      "2023-12-16 21:03:42 - utils.metrics - INFO - Level5 - 1.5203727265560685\n",
      "2023-12-16 21:03:42 - utils.metrics - INFO - Level6 - 0.31842699399117164\n",
      "2023-12-16 21:03:42 - utils.metrics - INFO - Level7 - 1.0114061693889846\n",
      "2023-12-16 21:03:42 - utils.metrics - INFO - Level8 - 0.34330380348524797\n",
      "2023-12-16 21:03:42 - utils.metrics - INFO - Level9 - 0.8303925476838893\n",
      "2023-12-16 21:03:44 - __main__ - INFO - 1802 - wspl: 0.544518781021126\n",
      "2023-12-16 21:03:48 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1830/temp_submissions/\n",
      "2023-12-16 21:04:06 - utils.metrics - INFO - reading weights file\n",
      "2023-12-16 21:04:06 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-16 21:04:32 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-16 21:04:37 - utils.metrics - INFO - Level1 - 0.2406354598854072\n",
      "2023-12-16 21:04:38 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:04:42 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:04:59 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:05:00 - utils.metrics - INFO - Level2 - 0.2594864475874541\n",
      "2023-12-16 21:05:00 - utils.metrics - INFO - Level3 - 0.3097064121992717\n",
      "2023-12-16 21:05:00 - utils.metrics - INFO - Level4 - 0.2901325668000964\n",
      "2023-12-16 21:05:00 - utils.metrics - INFO - Level5 - 1.2776383949842514\n",
      "2023-12-16 21:05:00 - utils.metrics - INFO - Level6 - 0.4110040879123669\n",
      "2023-12-16 21:05:00 - utils.metrics - INFO - Level7 - 0.9758249832167586\n",
      "2023-12-16 21:05:00 - utils.metrics - INFO - Level8 - 0.3743582224026753\n",
      "2023-12-16 21:05:00 - utils.metrics - INFO - Level9 - 0.8714275867136175\n",
      "2023-12-16 21:05:02 - __main__ - INFO - 1830 - wspl: 0.5566904624113221\n",
      "2023-12-16 21:05:06 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1858/temp_submissions/\n",
      "2023-12-16 21:05:24 - utils.metrics - INFO - reading weights file\n",
      "2023-12-16 21:05:24 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-16 21:05:51 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-16 21:05:56 - utils.metrics - INFO - Level1 - 0.1609495328897632\n",
      "2023-12-16 21:05:57 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:06:02 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:06:19 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:06:19 - utils.metrics - INFO - Level2 - 0.2346027788776534\n",
      "2023-12-16 21:06:19 - utils.metrics - INFO - Level3 - 0.2927883606230057\n",
      "2023-12-16 21:06:19 - utils.metrics - INFO - Level4 - 0.24685518211238483\n",
      "2023-12-16 21:06:19 - utils.metrics - INFO - Level5 - 1.0755029553868412\n",
      "2023-12-16 21:06:19 - utils.metrics - INFO - Level6 - 0.3780224894775054\n",
      "2023-12-16 21:06:20 - utils.metrics - INFO - Level7 - 0.89990108353679\n",
      "2023-12-16 21:06:20 - utils.metrics - INFO - Level8 - 0.40379485464078346\n",
      "2023-12-16 21:06:20 - utils.metrics - INFO - Level9 - 0.8666375621617401\n",
      "2023-12-16 21:06:22 - __main__ - INFO - 1858 - wspl: 0.5065616444118297\n",
      "2023-12-16 21:06:25 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1886/temp_submissions/\n",
      "2023-12-16 21:06:44 - utils.metrics - INFO - reading weights file\n",
      "2023-12-16 21:06:44 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-16 21:07:11 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-16 21:07:16 - utils.metrics - INFO - Level1 - 0.2674794393301125\n",
      "2023-12-16 21:07:17 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:07:21 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:07:39 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:07:39 - utils.metrics - INFO - Level2 - 0.23178935237842535\n",
      "2023-12-16 21:07:39 - utils.metrics - INFO - Level3 - 0.29956275638660856\n",
      "2023-12-16 21:07:39 - utils.metrics - INFO - Level4 - 0.19862676079185082\n",
      "2023-12-16 21:07:39 - utils.metrics - INFO - Level5 - 0.8091927903697541\n",
      "2023-12-16 21:07:39 - utils.metrics - INFO - Level6 - 0.307667976757551\n",
      "2023-12-16 21:07:40 - utils.metrics - INFO - Level7 - 0.7561376353898\n",
      "2023-12-16 21:07:40 - utils.metrics - INFO - Level8 - 0.3985095784849927\n",
      "2023-12-16 21:07:40 - utils.metrics - INFO - Level9 - 0.8619480434140016\n",
      "2023-12-16 21:07:42 - __main__ - INFO - 1886 - wspl: 0.4589904814781219\n",
      "2023-12-16 21:07:45 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1914/temp_submissions/\n",
      "2023-12-16 21:08:04 - utils.metrics - INFO - reading weights file\n",
      "2023-12-16 21:08:04 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-16 21:08:31 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-16 21:08:36 - utils.metrics - INFO - Level1 - 0.5336452963914808\n",
      "2023-12-16 21:08:38 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:08:42 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:09:00 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:09:01 - utils.metrics - INFO - Level2 - 0.4403762812444869\n",
      "2023-12-16 21:09:01 - utils.metrics - INFO - Level3 - 0.4061895961334509\n",
      "2023-12-16 21:09:01 - utils.metrics - INFO - Level4 - 0.41178965625364244\n",
      "2023-12-16 21:09:01 - utils.metrics - INFO - Level5 - 1.2248975323844988\n",
      "2023-12-16 21:09:01 - utils.metrics - INFO - Level6 - 0.5025127820340651\n",
      "2023-12-16 21:09:01 - utils.metrics - INFO - Level7 - 0.8285329816237732\n",
      "2023-12-16 21:09:01 - utils.metrics - INFO - Level8 - 0.4735412350599226\n",
      "2023-12-16 21:09:01 - utils.metrics - INFO - Level9 - 0.8916170701573993\n",
      "2023-12-16 21:09:03 - __main__ - INFO - 1914 - wspl: 0.6347891590314133\n",
      "2023-12-16 21:09:03 - __main__ - INFO - 1914 - mean wspl: 0.5403101056707627 +/- 0.05824802224154129\n",
      "2023-12-16 21:09:03 - __main__ - INFO - 1914 - raw results: [0.544518781021126, 0.5566904624113221, 0.5065616444118297, 0.4589904814781219, 0.6347891590314133]\n",
      "2023-12-16 21:09:03 - __main__ - INFO - --------------- ['seasonal', 'state_id', 'store_id', 'auto_sold_ewm_112', 'auto_sold_ewm_28', 'auto_sold_qtile_28_0.5', 'auto_sold_ma_28', 'auto_sold_qtile_28_0.9'] ---------------\n",
      "2023-12-16 21:09:06 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1802/temp_submissions/\n",
      "2023-12-16 21:09:27 - utils.metrics - INFO - reading weights file\n",
      "2023-12-16 21:09:27 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-16 21:09:53 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-16 21:09:58 - utils.metrics - INFO - Level1 - 0.2025243794116563\n",
      "2023-12-16 21:10:00 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:10:04 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:10:21 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:10:21 - utils.metrics - INFO - Level2 - 0.20895036614198118\n",
      "2023-12-16 21:10:21 - utils.metrics - INFO - Level3 - 0.17597719285507493\n",
      "2023-12-16 21:10:21 - utils.metrics - INFO - Level4 - 0.2231478621532336\n",
      "2023-12-16 21:10:21 - utils.metrics - INFO - Level5 - 0.22450474447184304\n",
      "2023-12-16 21:10:21 - utils.metrics - INFO - Level6 - 0.21327949319160858\n",
      "2023-12-16 21:10:21 - utils.metrics - INFO - Level7 - 0.20679517368308192\n",
      "2023-12-16 21:10:22 - utils.metrics - INFO - Level8 - 0.1937225083149421\n",
      "2023-12-16 21:10:22 - utils.metrics - INFO - Level9 - 0.21779645128461414\n",
      "2023-12-16 21:10:24 - __main__ - INFO - 1802 - wspl: 0.20741090794533734\n",
      "2023-12-16 21:10:27 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1830/temp_submissions/\n",
      "2023-12-16 21:10:45 - utils.metrics - INFO - reading weights file\n",
      "2023-12-16 21:10:45 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-16 21:11:11 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-16 21:11:16 - utils.metrics - INFO - Level1 - 0.2430874375004398\n",
      "2023-12-16 21:11:17 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:11:22 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:11:39 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:11:40 - utils.metrics - INFO - Level2 - 0.2034974446141869\n",
      "2023-12-16 21:11:40 - utils.metrics - INFO - Level3 - 0.19497557475759592\n",
      "2023-12-16 21:11:40 - utils.metrics - INFO - Level4 - 0.24349888261758543\n",
      "2023-12-16 21:11:40 - utils.metrics - INFO - Level5 - 0.21846076791060634\n",
      "2023-12-16 21:11:40 - utils.metrics - INFO - Level6 - 0.2069509537705163\n",
      "2023-12-16 21:11:40 - utils.metrics - INFO - Level7 - 0.20956462649850535\n",
      "2023-12-16 21:11:40 - utils.metrics - INFO - Level8 - 0.19147941906604343\n",
      "2023-12-16 21:11:41 - utils.metrics - INFO - Level9 - 0.21704250015958493\n",
      "2023-12-16 21:11:43 - __main__ - INFO - 1830 - wspl: 0.21428417854389606\n",
      "2023-12-16 21:11:46 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1858/temp_submissions/\n",
      "2023-12-16 21:12:05 - utils.metrics - INFO - reading weights file\n",
      "2023-12-16 21:12:05 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-16 21:12:31 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-16 21:12:37 - utils.metrics - INFO - Level1 - 0.11732598389823425\n",
      "2023-12-16 21:12:38 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:12:43 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:13:00 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:13:01 - utils.metrics - INFO - Level2 - 0.13381448436151389\n",
      "2023-12-16 21:13:01 - utils.metrics - INFO - Level3 - 0.16554805611855508\n",
      "2023-12-16 21:13:01 - utils.metrics - INFO - Level4 - 0.18993872651710067\n",
      "2023-12-16 21:13:01 - utils.metrics - INFO - Level5 - 0.1846963133852523\n",
      "2023-12-16 21:13:01 - utils.metrics - INFO - Level6 - 0.16312960745134492\n",
      "2023-12-16 21:13:01 - utils.metrics - INFO - Level7 - 0.18394834352760164\n",
      "2023-12-16 21:13:01 - utils.metrics - INFO - Level8 - 0.1810594299401598\n",
      "2023-12-16 21:13:02 - utils.metrics - INFO - Level9 - 0.1980060760603481\n",
      "2023-12-16 21:13:04 - __main__ - INFO - 1858 - wspl: 0.16860744680667897\n",
      "2023-12-16 21:13:07 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1886/temp_submissions/\n",
      "2023-12-16 21:13:26 - utils.metrics - INFO - reading weights file\n",
      "2023-12-16 21:13:26 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-16 21:13:53 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-16 21:13:59 - utils.metrics - INFO - Level1 - 0.09950652277919664\n",
      "2023-12-16 21:14:00 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:14:05 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:14:23 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:14:24 - utils.metrics - INFO - Level2 - 0.10886077646210317\n",
      "2023-12-16 21:14:24 - utils.metrics - INFO - Level3 - 0.12148916358344793\n",
      "2023-12-16 21:14:24 - utils.metrics - INFO - Level4 - 0.12109629080584583\n",
      "2023-12-16 21:14:24 - utils.metrics - INFO - Level5 - 0.1336203131833986\n",
      "2023-12-16 21:14:24 - utils.metrics - INFO - Level6 - 0.12582693336299977\n",
      "2023-12-16 21:14:24 - utils.metrics - INFO - Level7 - 0.15656452383222522\n",
      "2023-12-16 21:14:24 - utils.metrics - INFO - Level8 - 0.14501657810752416\n",
      "2023-12-16 21:14:25 - utils.metrics - INFO - Level9 - 0.17113599213710243\n",
      "2023-12-16 21:14:27 - __main__ - INFO - 1886 - wspl: 0.13145745491709376\n",
      "2023-12-16 21:14:30 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1914/temp_submissions/\n",
      "2023-12-16 21:14:51 - utils.metrics - INFO - reading weights file\n",
      "2023-12-16 21:14:51 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-16 21:15:20 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-16 21:15:25 - utils.metrics - INFO - Level1 - 0.15897269048648155\n",
      "2023-12-16 21:15:26 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:15:31 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:15:49 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-16 21:15:50 - utils.metrics - INFO - Level2 - 0.15455154096677387\n",
      "2023-12-16 21:15:50 - utils.metrics - INFO - Level3 - 0.16504350711306404\n",
      "2023-12-16 21:15:50 - utils.metrics - INFO - Level4 - 0.18664701917377247\n",
      "2023-12-16 21:15:50 - utils.metrics - INFO - Level5 - 0.17999460367575493\n",
      "2023-12-16 21:15:50 - utils.metrics - INFO - Level6 - 0.16350156687534548\n",
      "2023-12-16 21:15:50 - utils.metrics - INFO - Level7 - 0.18589644869641997\n",
      "2023-12-16 21:15:50 - utils.metrics - INFO - Level8 - 0.1718541966618605\n",
      "2023-12-16 21:15:50 - utils.metrics - INFO - Level9 - 0.19308073254577604\n",
      "2023-12-16 21:15:52 - __main__ - INFO - 1914 - wspl: 0.17328247846613876\n",
      "2023-12-16 21:15:52 - __main__ - INFO - 1914 - mean wspl: 0.17900849333582897 +/- 0.029843395894028367\n",
      "2023-12-16 21:15:52 - __main__ - INFO - 1914 - raw results: [0.20741090794533734, 0.21428417854389606, 0.16860744680667897, 0.13145745491709376, 0.17328247846613876]\n"
     ]
    }
   ],
   "source": [
    "FILE_NAME_ALL_RESULTS = '../data/uncertainty/all_results.json'\n",
    "USE_ALL = False\n",
    "FOLDER = 'temp_submissions/'\n",
    "\n",
    "TEST_NUMB = 0\n",
    "TEST_NUMBER = 9\n",
    "\n",
    "# load dict to store results in\n",
    "from utils.utils import load_results_as_json\n",
    "results = load_results_as_json(FILE_NAME_ALL_RESULTS)\n",
    "\n",
    "EXCLUDE_COLUMNS_LIST = []\n",
    "logger.info('start evaluating exclude columns')\n",
    "for EXCLUDE_COLUMNS in EXCLUDE_COLUMNS_LIST:\n",
    "    if 'exclude_' + ' '.join(EXCLUDE_COLUMNS) not in results.keys():\n",
    "        results['exclude_' + ' '.join(EXCLUDE_COLUMNS)] = {}\n",
    "    logger.info('--------------- ' + str(EXCLUDE_COLUMNS) + ' ---------------')\n",
    "    res = []\n",
    "    for D_CV_START in D_CROSS_VAL_START_LIST:\n",
    "        mean_wspl, res_dict = perform_cv(\n",
    "            _down_cast(d)[d_int < (D_CV_START + DAYS)], \n",
    "            read_concat_predictions(\n",
    "                fold_name = D_CV_START, \n",
    "                exclude_columns = EXCLUDE_COLUMNS,\n",
    "                include_columns = None,\n",
    "                use_all=USE_ALL,\n",
    "                load_submissions_path=FOLDER\n",
    "            )\n",
    "        )\n",
    "        res.append(mean_wspl)\n",
    "        results['exclude_' + ' '.join(EXCLUDE_COLUMNS)]['fold_' + str(D_CV_START)] = res_dict \n",
    "        logger.info(str(D_CV_START) + ' - wspl: ' + str(mean_wspl))\n",
    "\n",
    "    logger.info(' - mean wspl: ' + str(np.mean(res)) + ' +/- ' + str(np.std(res)))\n",
    "    logger.info(str(D_CV_START) + ' - raw results: ' + str(res))\n",
    "\n",
    "logger.info('start evaluating include columns')\n",
    "for INCLUDE_COLUMNS in INCLUDE_COLUMNS_LIST:\n",
    "    if 'kbest' in INCLUDE_COLUMNS:\n",
    "        INCLUDE_COLUMNS = ['k_best']\n",
    "    if 'include_' + ' '.join(INCLUDE_COLUMNS) not in results.keys():\n",
    "        results['include_' + ' '.join(INCLUDE_COLUMNS)] = {}\n",
    "    logger.info('--------------- ' + str(INCLUDE_COLUMNS) + ' ---------------')\n",
    "    res = []\n",
    "    for D_CV_START in D_CROSS_VAL_START_LIST:\n",
    "        mean_wspl, res_dict = perform_cv(\n",
    "            _down_cast(d)[d_int < (D_CV_START + DAYS)], \n",
    "            read_concat_predictions(\n",
    "                fold_name = D_CV_START, \n",
    "                exclude_columns = [], \n",
    "                include_columns = INCLUDE_COLUMNS,\n",
    "                use_all=USE_ALL,\n",
    "                load_submissions_path=FOLDER\n",
    "            )\n",
    "        )\n",
    "        res.append(mean_wspl)\n",
    "        results['include_' + ' '.join(INCLUDE_COLUMNS)]['fold_' + str(D_CV_START)] = res_dict \n",
    "        logger.info(str(D_CV_START) + ' - wspl: ' + str(mean_wspl))\n",
    "\n",
    "    logger.info(str(D_CV_START) + ' - mean wspl: ' + str(np.mean(res)) + ' +/- ' + str(np.std(res)))\n",
    "    logger.info(str(D_CV_START) + ' - raw results: ' + str(res))\n",
    "\n",
    "from utils.utils import store_results_as_json\n",
    "store_results_as_json(results, FILE_NAME_ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Beneath can be used to create submission template\n",
    "The submission template can be used to quickly insert your predictions.\n",
    "It also contains all other (historical) sales to be able to compute the WRMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_validation = pd.read_csv(DATA_BASE_PATH + SALES_VALIDATION)\n",
    "# sales_evaluation = pd.read_csv(DATA_BASE_PATH + SALES_EVALUATION)\n",
    "# calendar = pd.read_csv(DATA_BASE_PATH + CALENDAR)\n",
    "# sell_prices = pd.read_csv(DATA_BASE_PATH + SELL_PRICES)\n",
    "\n",
    "# df_val, submission_idx_val = data_preprocessing(sales_validation, calendar, sell_prices)\n",
    "# del sales_validation\n",
    "# df_eval, submission_idx_eval = data_preprocessing(sales_evaluation, calendar, sell_prices)\n",
    "# del sales_evaluation\n",
    "\n",
    "# df_val_after_release = df_val[(df_val.wm_yr_wk > df_val.release)]# & (df_val[\"sold\"].notna())]\n",
    "# del df_val\n",
    "# df_eval_after_release = df_eval[(df_eval.wm_yr_wk > df_eval.release)]# & (df_eval[\"sold\"].notna())]\n",
    "# del df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# df_eval_after_release['revenue'] = df_eval_after_release['sold'] * df_eval_after_release['sell_price']\n",
    "# for level in list(AGG_LEVEL_COLUMNS.keys()):\n",
    "#     c = AGG_LEVEL_COLUMNS[level]\n",
    "#     logger.info(level)\n",
    "#     agg_dict = {\n",
    "#         'sold': 'sum',\n",
    "#         'revenue': 'sum'\n",
    "#     }\n",
    "#     d1 = df_eval_after_release.groupby(c + ['d']).agg(agg_dict).reset_index(drop=False)\n",
    "#     d = pd.DataFrame({\n",
    "#         'd': d1['d'],\n",
    "#         'sold': d1['sold'],\n",
    "#         'revenue': d1['revenue']\n",
    "#     })\n",
    "#     if len(c) == 0:\n",
    "#         d['agg_column1'] = 'Total'\n",
    "#         d['agg_column2'] = 'X'\n",
    "#     elif len(c) == 1:\n",
    "#         d['agg_column1'] = d1[c[0]]\n",
    "#         d['agg_column2'] = 'X'\n",
    "#     else:\n",
    "#         d['agg_column1'] = d1[c[0]]\n",
    "#         d['agg_column2'] = d1[c[1]]\n",
    "#     d['id_merge'] = d['agg_column1'] + '_' + d['agg_column2']\n",
    "#     d['Level'] = level\n",
    "#     dfs.append(d[['Level', 'agg_column1', 'agg_column2', 'd', 'sold', 'revenue', 'id_merge']])\n",
    "# d = pd.concat(dfs)\n",
    "# d.head(50)\n",
    "# d.to_parquet('temp.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
