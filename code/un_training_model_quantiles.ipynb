{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import pickle as pkl\n",
    "from utils.utils import merge_eval_sold_on_df, sort_df_on_d, WRMSSE, RMSSE, _down_cast, data_preprocessing, diff_lists, log_status\n",
    "from utils.utils import customIter, cross_validation_on_validation_set, ensemble_submissions, ensemble_submissions_uncertainty\n",
    "from utils.metrics import WSPL\n",
    "from utils.configure_logger import configure_logger\n",
    "from utils.utils import prefixes_in_column, prefix_in_column, parse_columns_to_string\n",
    "from utils import constants\n",
    "\n",
    "configure_logger()\n",
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = constants.DATA_BASE_PATH #'../data/m5-forecasting-accuracy/'\n",
    "DATA_BASE_PATH_UNCERTAINTY = constants.DATA_BASE_PATH_UNCERTAINTY #'../data/m5-forecasting-uncertainty/'\n",
    "SALES_EVALUATION = constants.SALES_EVALUATION \n",
    "SALES_VALIDATION = constants.SALES_VALIDATION\n",
    "CALENDAR = constants.CALENDAR \n",
    "SAMPLE_SUBMISSION = constants.SAMPLE_SUBMISSION \n",
    "SELL_PRICES = constants.SELL_PRICES\n",
    "\n",
    "PRECOMPUTED_BASE_PATH = constants.PRECOMPUTED_BASE_PATH #'../data/uncertainty/features/'\n",
    "\n",
    "DAYS: int = constants.DAYS #28\n",
    "QUANTILES: int = constants.QUANTILES \n",
    "\n",
    "AGG_LEVEL_COLUMNS = constants.AGG_LEVEL_COLUMNS\n",
    "D_CROSS_VAL_START_LIST = constants.D_CROSS_VAL_START_LIST\n",
    "\n",
    "# to simple get the precomputed name\n",
    "precomputed_name = lambda store, eval_val: f'processed_{store}_{eval_val}.pkl'\n",
    "\n",
    "TEST_PATH = constants.TEST_PATH#'test/'\n",
    "PREDICTION_BASE_PATH = constants.PREDICTION_BASE_PATH #'../data/uncertainty/temp_submissions/'\n",
    "SUBMISSION_BASE_PATH = constants.SUBMISSION_BASE_PATH #'../data/uncertainty/final_submissions/'\n",
    "\n",
    "SUB_D_START_VAL: int = constants.SUB_D_START_VAL\n",
    "SUB_D_START_EVAL: int = constants.SUB_D_START_EVAL\n",
    "\n",
    "# the columns are always included after feature processing\n",
    "# because they are required in the training and submission format\n",
    "DROP_FEATURE_COLUMNS: list = constants.DROP_FEATURE_COLUMNS #['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'sold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GridSearch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_status\n",
    "def grid_search(params: dict, param_grid: dict, features, targets, n_folds: int = 1):\n",
    "    \"\"\" \n",
    "    Given a grid with parameters, train lgb model for all possible combinations.\n",
    "    Returns the parameter set with the best score and the dictionary with all results.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    \n",
    "    # to be sure\n",
    "    features = features.reset_index(drop=True)\n",
    "    targets = targets.reset_index(drop=True)\n",
    "\n",
    "    param_combinations = list(itertools.product(*param_grid.values()))\n",
    "    results = {}\n",
    "    for i, param_combination in enumerate(param_combinations,1):\n",
    "        \n",
    "        # create dictionary with all parameters\n",
    "        param_combination = {k:v for k,v in zip(param_grid.keys(), param_combination)}\n",
    "        param_combination.update(params)\n",
    "                \n",
    "        # init dict\n",
    "        results[f\"combination_{i}\"] = {\n",
    "            'params': param_combination,\n",
    "            'res': []\n",
    "        }\n",
    "        \n",
    "        # perform n_folds\n",
    "        # from sklearn.model_selection import KFold\n",
    "        # kfold = KFold(n_splits=n_folds)\n",
    "        # for j, (train_idx, validation_idx) in enumerate(kfold.split(features)):\n",
    "        \n",
    "        for j in range(n_folds):\n",
    "            \n",
    "            # kfold\n",
    "            features_train, features_validation, targets_train, targets_validation =\\\n",
    "                train_test_split(features, targets, train_size = .8, shuffle=True)#, random_state=42)\n",
    "        \n",
    "            # # split data for fold\n",
    "            # features_train, features_validation = features.loc[train_idx], features.loc[validation_idx]\n",
    "            # targets_train, targets_validation = targets.loc[train_idx], targets.loc[validation_idx]\n",
    "\n",
    "            # # normalize\n",
    "            # from sklearn.preprocessing import StandardScaler\n",
    "            # scaler = StandardScaler()\n",
    "            \n",
    "            # targets_train = scaler\\\n",
    "            #     .fit_transform(targets_train.values.reshape(-1,1))\\\n",
    "            #     .reshape(-1)\n",
    "            # targets_validation = scaler\\\n",
    "            #     .transform(targets_validation.values.reshape(-1,1))\\\n",
    "            #     .reshape(-1)\n",
    "\n",
    "            # train lgb model        \n",
    "            temp_dict = {} # this dict object will be used to add all (intermediate) evaluation scores during the training process\n",
    "            mod: lgb.Booster = lgb.train(param_combination, \n",
    "                train_set = lgb.Dataset(features_train, targets_train),\n",
    "                valid_sets = lgb.Dataset(features_validation, targets_validation),\n",
    "                evals_result = temp_dict,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            \n",
    "            # store results\n",
    "            results[f\"combination_{i}\"]['res']\\\n",
    "                .append(temp_dict[\"valid_0\"][\"quantile\"][-1],\n",
    "            )\n",
    "\n",
    "        # compute average results\n",
    "        results[f\"combination_{i}\"]['validation_score'] = \\\n",
    "            np.mean(results[f\"combination_{i}\"]['res'])\n",
    "        \n",
    "    # sort the results based on evaluation score\n",
    "    sorted_results = dict(sorted(results.items(), key=lambda item: item[1][\"validation_score\"]))\n",
    "    return list(sorted_results.values())[0], results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LoadData:\n",
    "    \"\"\" Class to load data \"\"\"\n",
    "    def __init__(self):\n",
    "        self.level = None\n",
    "        \n",
    "    def prep_data(self,level, sub_d_start):\n",
    "        \"\"\" read the precomputed features and targets for specified aggregation level,  \"\"\"\n",
    "        # define params\n",
    "        agg_level = level\n",
    "        # sub_d_start: int = int(1886)\n",
    "        exclude_columns = []\n",
    "        test = False\n",
    "        type_of = 'val'\n",
    "\n",
    "        # read file\n",
    "        agg_columns = AGG_LEVEL_COLUMNS[agg_level]\n",
    "        if len(agg_columns) == 0:\n",
    "            agg_str: str = 'Total_X'\n",
    "        elif len(agg_columns) == 1:\n",
    "            agg_str: str = f'{agg_columns[0]}_X'\n",
    "        else:\n",
    "            agg_str: str = '_'.join(agg_columns)\n",
    "\n",
    "        if self.level == level:\n",
    "            pass\n",
    "        else:\n",
    "            logger.info('(re)loading features')\n",
    "            features = pd.read_parquet(f'../data/uncertainty/fold_{sub_d_start}/features/' + (TEST_PATH if test else '') + f'features_{type_of}_{agg_str}.parquet')\n",
    "            features = _down_cast(features)\n",
    "\n",
    "        group_columns = agg_columns\n",
    "        exclude_prefix_list = exclude_columns # unconditional, auto, momentum, seasonal\n",
    "        \n",
    "        features_gr = features.copy()\n",
    "        features_gr = features_gr[[c for c in features_gr if c.split('_')[0] not in exclude_prefix_list]]\n",
    "\n",
    "        # preparations\n",
    "        train_idx = features_gr['sold'].notna() & features_gr['d'].isin([f'd_{sub_d_start - 1 - i}' for i in range(1460)])\n",
    "        df_train = features_gr[train_idx]\n",
    "        features_train: pd.DataFrame = df_train.drop(DROP_FEATURE_COLUMNS, axis = 1, errors = 'ignore')\n",
    "        targets_train: pd.Series = df_train['sold']\n",
    "        return features_train, targets_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Single Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 13:53:28 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['auto_sold_56',\n",
       " 'auto_sold_14',\n",
       " 'auto_sold_7',\n",
       " 'auto_sold_1',\n",
       " 'auto_sold_2',\n",
       " 'auto_sold_28',\n",
       " 'auto_sold_ma_28',\n",
       " 'auto_sold_std_14',\n",
       " 'auto_sold_ma_7',\n",
       " 'auto_sold_ma_56',\n",
       " 'auto_sold_std_56',\n",
       " 'auto_sold_ma_168',\n",
       " 'auto_sold_ma_21',\n",
       " 'auto_sold_std_7',\n",
       " 'auto_sold_ma_14',\n",
       " 'auto_sold_std_28',\n",
       " 'auto_sold_ma_112',\n",
       " 'auto_sold_std_168',\n",
       " 'auto_sold_std_21',\n",
       " 'auto_sold_std_112',\n",
       " 'auto_sold_ewm_7',\n",
       " 'auto_sold_ewm_3',\n",
       " 'auto_sold_ewm_30',\n",
       " 'auto_sold_ewm_15',\n",
       " 'auto_sold_ewm_100',\n",
       " 'autoquantiles_sold_qtile_28_0.1',\n",
       " 'autoquantiles_sold_qtile_28_0.99',\n",
       " 'autoquantiles_sold_qtile_14_0.75',\n",
       " 'autoquantiles_sold_qtile_56_0.99',\n",
       " 'autoquantiles_sold_qtile_112_0.01',\n",
       " 'autoquantiles_sold_qtile_14_0.25',\n",
       " 'autoquantiles_sold_qtile_14_0.9',\n",
       " 'autoquantiles_sold_qtile_56_0.1',\n",
       " 'autoquantiles_sold_qtile_56_0.75',\n",
       " 'autoquantiles_sold_qtile_28_0.75',\n",
       " 'autoquantiles_sold_qtile_14_0.99',\n",
       " 'autoquantiles_sold_qtile_14_0.1',\n",
       " 'autoquantiles_sold_qtile_56_0.25',\n",
       " 'autoquantiles_sold_qtile_112_0.99',\n",
       " 'autoquantiles_sold_qtile_112_0.9',\n",
       " 'autoquantiles_sold_qtile_56_0.9',\n",
       " 'autoquantiles_sold_qtile_28_0.9',\n",
       " 'autoquantiles_sold_qtile_112_0.25',\n",
       " 'autoquantiles_sold_qtile_112_0.75',\n",
       " 'autoquantiles_sold_qtile_14_0.5',\n",
       " 'autoquantiles_sold_qtile_56_0.5',\n",
       " 'autoquantiles_sold_qtile_112_0.1',\n",
       " 'autoquantiles_sold_qtile_28_0.25',\n",
       " 'autoquantiles_sold_qtile_112_0.5',\n",
       " 'autoquantiles_sold_qtile_28_0.5',\n",
       " 'price_momentum_m',\n",
       " 'price_momentum_y',\n",
       " 'price_momentum_w',\n",
       " 'price_auto_std_28',\n",
       " 'price_auto_std_56',\n",
       " 'price_auto_std_112',\n",
       " 'seasonal_weekday_Friday',\n",
       " 'seasonal_weekday_Thursday',\n",
       " 'seasonal_weekday_Monday',\n",
       " 'seasonal_weekday_Tuesday',\n",
       " 'seasonal_weekday_Saturday',\n",
       " 'seasonal_weekday_Wednesday',\n",
       " 'seasonal_weekday_Sunday',\n",
       " 'seasonal_month_1',\n",
       " 'seasonal_month_4',\n",
       " 'seasonal_month_2',\n",
       " 'seasonal_month_3',\n",
       " 'seasonal_month_6',\n",
       " 'seasonal_month_5',\n",
       " 'seasonal_month_9',\n",
       " 'seasonal_month_8',\n",
       " 'seasonal_month_11',\n",
       " 'seasonal_month_7',\n",
       " 'seasonal_month_10',\n",
       " 'seasonal_month_12',\n",
       " 'seasonal_monthday_28',\n",
       " 'seasonal_monthday_1',\n",
       " 'seasonal_monthday_25',\n",
       " 'seasonal_monthday_20',\n",
       " 'seasonal_monthday_10',\n",
       " 'seasonal_monthday_16',\n",
       " 'seasonal_monthday_12',\n",
       " 'seasonal_monthday_2',\n",
       " 'seasonal_monthday_8',\n",
       " 'seasonal_monthday_23',\n",
       " 'seasonal_monthday_29',\n",
       " 'seasonal_monthday_15',\n",
       " 'seasonal_monthday_4',\n",
       " 'seasonal_monthday_17',\n",
       " 'seasonal_monthday_11',\n",
       " 'seasonal_monthday_9',\n",
       " 'seasonal_monthday_13',\n",
       " 'seasonal_monthday_14',\n",
       " 'seasonal_monthday_6',\n",
       " 'seasonal_monthday_31',\n",
       " 'seasonal_monthday_5',\n",
       " 'seasonal_monthday_3',\n",
       " 'seasonal_monthday_22',\n",
       " 'seasonal_monthday_27',\n",
       " 'seasonal_monthday_7',\n",
       " 'seasonal_monthday_30',\n",
       " 'seasonal_monthday_21',\n",
       " 'seasonal_monthday_26',\n",
       " 'seasonal_monthday_18',\n",
       " 'seasonal_monthday_19',\n",
       " 'seasonal_monthday_24',\n",
       " 'days_fwd']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level = 'Level1'\n",
    "dataLoader = LoadData()\n",
    "features, targets = dataLoader.prep_data(level, 1914)\n",
    "list(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold\n",
    "prefixes = ['autoquantiles_']\n",
    "features_train, features_validation, targets_train, targets_validation =\\\n",
    "    train_test_split(features, targets, test_size = 28, shuffle=False, random_state=42)\n",
    "    # train_test_split(features[[c for c in features.columns if prefixes_in_column(c, prefixes)]], targets, train_size = .8, shuffle=False, random_state=42)\n",
    "\n",
    "\n",
    "# undersample training features\n",
    "# under_sample_pct = \n",
    "# features_train, _, targets_train, _ = train_test_split(features_train, targets_train, train_size=under_sample_pct, shuffle=False, random_state=42)\n",
    "\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -100,\n",
    "    'n_jobs': 4,\n",
    "    # 'subsample': .9,\n",
    "    # 'subsample_freq': 1,\n",
    "    \"num_leaves\": 30,\n",
    "    \"min_child_weight\": .1,\n",
    "    \"min_child_samples\": 4,\n",
    "    \"hist_pool_size\": 1000,\n",
    "    'feature_fraction': 0.9, #.5\n",
    "    # 'bagging_fraction': .8,\n",
    "    \"learning_rate\": 0.005, #0.07,\n",
    "    \"n_estimators\": 2000,#100\n",
    "    \"max_depth\": 10,\n",
    "    # 'reg_sqrt': True,\n",
    "    # 'req_lambda': .00001,\n",
    "    # 'reg_alpha': .00001,\n",
    "    'alpha': .25,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# # normalize\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# targets_train = scaler\\\n",
    "#     .fit_transform(targets_train.values.reshape(-1,1))\\\n",
    "#     .reshape(-1)\n",
    "# targets_validation = scaler\\\n",
    "#     .transform(targets_validation.values.reshape(-1,1))\\\n",
    "#     .reshape(-1)\n",
    "\n",
    "# train lgb model       \n",
    "for q in [0.005, 0.025, 0.135, 0.25, 0.5, 0.75, 0.865, 0.975, 0.995]:\n",
    "    params['alpha'] = q \n",
    "    temp_dict = {}\n",
    "    mod: lgb.Booster = lgb.train(params, \n",
    "        train_set = lgb.Dataset(features_train, targets_train),\n",
    "        valid_sets = lgb.Dataset(features_validation, targets_validation),\n",
    "        evals_result = temp_dict,\n",
    "        verbose_eval = False\n",
    "    )\n",
    "    plt.plot(mod.predict(features_validation), label = f'{q}')\n",
    "\n",
    "# # plot prediction vs. true outcome (with quantile regression, this should correspond to confidence bounds)\n",
    "# n = 100000000\n",
    "# idx = targets_validation[:n].index\n",
    "# plt.scatter(idx, mod.predict(features_validation)[:n], label = 'pred', s = 10)\n",
    "# plt.scatter(idx, targets_validation[:n], label = 'true', s = 10)\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "plt.scatter(range(len(targets_validation.index)), targets_validation, label = 'true', s = 10)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# # plot distribution of residuals, should have roughly q*n values on the left, and (1-q)*n values on the right\n",
    "# plt.hist(mod.predict(features_validation) - targets_validation, bins=200)\n",
    "# plt.show()\n",
    "\n",
    "# # plot histogram of targets \n",
    "# plt.hist(targets)\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# # plot log(loss) of training iterations to get insights in convergence\n",
    "# plt.plot(np.log(temp_dict[\"valid_0\"][\"quantile\"]))\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# # plot feature importance\n",
    "# lgb.plot_importance(mod)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Run for Testing Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total ~280 seconds\n",
    "lgb_quantile_params = {     # fairly well tuned, with high runtimes \n",
    "    'max_depth': [10, 20],\n",
    "    'n_estimators': [ 150, 200, 200],  # 300, 350, 400, ],   \n",
    "    'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n",
    "    'min_child_samples': [ 2, 4, 7, 10, 14, 20, 30, 40, 60, 80, 100, 100, 100, \n",
    "                                        130, 170, 200, 300, 500, 700, 1000 ],\n",
    "    'min_child_weight': [0, 0, 0, 0, 1e-4, 1e-3, 1e-3, 1e-3, 5e-3, 2e-2, 0.1 ],\n",
    "    'num_leaves': [ 20, 30, 50, 50 ], # 50, 70, 90, ],\n",
    "    'learning_rate': [  0.04, 0.05, 0.07, 0.07, 0.07, 0.1, 0.1, 0.1 ],   # 0.02, 0.03,        \n",
    "    'colsample_bytree': [0.3, 0.5, 0.7, 0.8, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "    'colsample_bynode':[0.1, 0.15, 0.2, 0.2, 0.2, 0.25, 0.3, 0.5, 0.65, 0.8, 0.9, 1],\n",
    "    'reg_lambda': [0, 0, 0, 0, 1e-5, 1e-5, 1e-5, 1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100   ],\n",
    "    'reg_alpha': [0, 1e-5, 3e-5, 1e-4, 1e-4, 1e-3, 3e-3, 1e-2, 0.1, 1, 1, 10, 10, 100, 1000,],\n",
    "    'subsample': [  0.9, 1],\n",
    "    'subsample_freq': [1],\n",
    "    'cat_smooth': [0.1, 0.2, 0.5, 1, 2, 5, 7, 10],\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': 4,\n",
    "    # 'eval_at': 10,\n",
    "    'hist_pool_size': 1000,\n",
    "    # 'verbose_eval': 0\n",
    "    # 'subsample': 0.5,\n",
    "    # 'subsample_freq': 1,\n",
    "    # 'feature_fraction': 0.5,\n",
    "    # 'boost_from_average': False,\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10],\n",
    "    'n_estimators': [200, 800],\n",
    "    # 'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n",
    "    'min_child_samples': [4],\n",
    "    'min_child_weight': [0.1 ],\n",
    "    'num_leaves': [30], # 50, 70, 90, ],\n",
    "    'learning_rate': [0.001, 0.005, 0.01 ],   # 0.02, 0.03,        \n",
    "    # 'colsample_bytree': [0.3, 0.5, 0.7, 0.8, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "    # 'colsample_bynode':[0.1, 0.15, 0.2, 0.2, 0.2, 0.25, 0.3, 0.5, 0.65, 0.8, 0.9, 1],\n",
    "    # 'reg_lambda': [0, 1e-5, 1e-5, 1e-4, 1e-2, 1, 10, ],\n",
    "    # 'reg_alpha': [0, 1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 1, 10, 100, 1000,],\n",
    "    'subsample': [  0.9, 1],\n",
    "    'subsample_freq': [1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test grid search for all quantiles\n",
    "for q in QUANTILES:\n",
    "    \n",
    "    # of course, update quantile in params\n",
    "    params['alpha'] = q\n",
    "    best_res, res = grid_search(params, param_grid, features_train, targets_train, 1)\n",
    "    logger.info(best_res['params'])\n",
    "    \n",
    "    mod = lgb.train(best_res['params'],\n",
    "        train_set = lgb.Dataset(features_train, targets_train)\n",
    "    )\n",
    "    predictions = mod.predict(features_validation)\n",
    "    plt.plot(predictions, label = str(q))\n",
    "\n",
    "plt.scatter(range(len(targets_validation)), targets_validation)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train + Predict submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_level_all_quantiles(\n",
    "    agg_level: str, \n",
    "    type_of: str, \n",
    "    sub_d_start: int, \n",
    "    exclude_columns: list = [], \n",
    "    include_columns: list = None,\n",
    "    test: bool = False, \n",
    "    do_grid_search: bool = False, \n",
    "    store_submissions_path: str = 'temp_submissions/', \n",
    "    normalize: bool = False\n",
    "    ):\n",
    "    \"\"\" \n",
    "    Train, for a specific aggregation level, models for all quantiles.\n",
    "    For aggregation levels 10, 11 and 12, undersampling is used to drastically reduce training time.\n",
    "    \"\"\"\n",
    "    ALWAYS_KEEP_COLUMNS = ['days_fwd', 'sold', 'd']\n",
    "    \n",
    "    agg_columns = AGG_LEVEL_COLUMNS[agg_level]\n",
    "    if len(agg_columns) == 0:\n",
    "        agg_str: str = 'Total_X'\n",
    "    elif len(agg_columns) == 1:\n",
    "        agg_str: str = f'{agg_columns[0]}_X'\n",
    "    else:\n",
    "        agg_str: str = '_'.join(agg_columns)\n",
    "\n",
    "    # try:\n",
    "    #     features = pd.DataFrame(features)\n",
    "    # except Exception:\n",
    "    # loading features\n",
    "    logger.info('(re)loading features')\n",
    "    features = pd.read_parquet(f'../data/uncertainty/fold_{sub_d_start}/features/' + (TEST_PATH if test else '') + f'features_{type_of}_{agg_str}.parquet')\n",
    "    features = _down_cast(features)\n",
    "    features_gr = features.copy()\n",
    "\n",
    "    group_columns = agg_columns\n",
    "    res: list = []\n",
    "\n",
    "    # select features\n",
    "    if USE_ALL:\n",
    "        columns = features_gr.columns\n",
    "    elif SPARSE_FEATURES:\n",
    "        columns = [c for c in features_gr.columns if c in SPARSE_FEATURES]\n",
    "    else:\n",
    "        if include_columns == None:\n",
    "            # exclude features from exclusion prefix list\n",
    "            exclude_prefix_list = exclude_columns \n",
    "            columns = [c for c in features_gr.columns if c.split('_')[0] not in exclude_prefix_list]\n",
    "        elif isinstance(include_columns, list):\n",
    "            include_prefix_list = include_columns\n",
    "            columns = [c for c in features_gr.columns if c.split('_')[0] in include_prefix_list]\n",
    "\n",
    "    for column in ALWAYS_KEEP_COLUMNS + group_columns:\n",
    "        if column not in columns:\n",
    "            columns.append(column)    \n",
    "    logger.info(f'feature: {str(columns)}')\n",
    "    features_gr = features_gr[columns]\n",
    "    \n",
    "    # preparations\n",
    "    # sub_d_start = SUB_D_START_VAL if type_of == 'val' else SUB_D_START_EVAL\n",
    "    train_idx = features_gr['sold'].notna() & features_gr['d'].isin([f'd_{sub_d_start - 1 - i}' for i in range(1000)])\n",
    "    pred_idx = features_gr['d'].isin([f'd_{sub_d_start + i}' for i in range(DAYS)])\n",
    "    \n",
    "    # separate train/pred indices\n",
    "    df_train = features_gr[train_idx]\n",
    "    df_pred = features_gr[pred_idx]\n",
    "    features_train: pd.DataFrame = df_train.drop(DROP_FEATURE_COLUMNS, axis = 1, errors = 'ignore')\n",
    "    targets_train: pd.Series = df_train['sold']\n",
    "    features_predict: pd.DataFrame = df_pred.drop(DROP_FEATURE_COLUMNS, axis = 1, errors = 'ignore')\n",
    "    targets_test: pd.Series = df_pred['sold']\n",
    "    \n",
    "    # undersample data\n",
    "    if agg_level in undersampling_dict and HIGH_UNDERSAMPLING:\n",
    "        undersampling_pct = undersampling_dict[agg_level]\n",
    "        features_train, _, targets_train, _ = train_test_split(features_train, targets_train, train_size = undersampling_pct, shuffle=True, random_state=43)\n",
    "\n",
    "    # normalise targets\n",
    "    if normalize:\n",
    "        logger.info('scaling targets')\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        targets_train = scaler.fit_transform(targets_train.values.reshape(-1,1))\n",
    "        \n",
    "    # REMOVE THIS\n",
    "    import matplotlib.pyplot as plt\n",
    "    if PLOT_PREDICTIONS:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (10,5))\n",
    "        aaa = [i for i in range(targets_test.shape[0])]\n",
    "    # REMOVE THIS\n",
    "        \n",
    "    # train model for all quantiles\n",
    "    for quantile in QUANTILES:\n",
    "        \n",
    "        # perform grid search for best parameters\n",
    "        if do_grid_search == True:\n",
    "            # split data to training and testing\n",
    "            # logger.info('divide for cross validation')\n",
    "            # x_train, x_test, y_train, y_test = train_test_split(features_train, targets_train, train_size=.8, shuffle=False, random_state=42)\n",
    "            # train_data = lgb.Dataset(x_train, y_train)\n",
    "            # validation_data = lgb.Dataset(x_test, y_test)\n",
    "            logger.info('perform gridsearch')\n",
    "            params['alpha'] = quantile\n",
    "            best_combination, results = grid_search(params, param_grid, features_train, targets_train, 1)\n",
    "            # del train_data; del validation_data\n",
    "            params_grid_train = best_combination[\"params\"]\n",
    "        else:\n",
    "            params_grid_train = PARAM_GRID_TRAIN\n",
    "            params_grid_train['alpha'] = quantile\n",
    "\n",
    "        # train_best_model\n",
    "        # logger.info(f'features: {str(features_train.columns)}')\n",
    "        mod = lgb.train(params_grid_train,\n",
    "            train_set = lgb.Dataset(features_train, targets_train)\n",
    "        )\n",
    "        predictions = mod.predict(features_predict)\n",
    "        if normalize:\n",
    "            predictions = scaler.inverse_transform(predictions.reshape(-1,1)).reshape(-1,)\n",
    "        \n",
    "        # REMOVE THIS\n",
    "        if PLOT_PREDICTIONS:\n",
    "            ax.plot(aaa, predictions, label = f'{quantile}')\n",
    "        # lgb.plot_importance(mod)\n",
    "        # REMOVE THIS\n",
    "        \n",
    "        # store predictions\n",
    "        df_p = pd.DataFrame(\n",
    "            {\n",
    "                'pred': predictions,\n",
    "                'd': df_pred['d'],\n",
    "            }\n",
    "        )\n",
    "        df_p['quantile'] = quantile\n",
    "        df_p['Level'] = agg_level\n",
    "        df_p['type_of'] = 'validation' if type_of == 'val' else 'evaluation'\n",
    "        if len(agg_columns) == 0:\n",
    "            df_p['agg_column1'] = 'Total'\n",
    "            df_p['agg_column2'] = 'X'\n",
    "        elif len(agg_columns) == 1:\n",
    "            df_p['agg_column1'] = df_pred[agg_columns[0]].values\n",
    "            df_p['agg_column2'] = 'X'\n",
    "        else:\n",
    "            df_p['agg_column1'] = df_pred[agg_columns[0]].values\n",
    "            df_p['agg_column2'] = df_pred[agg_columns[1]].values\n",
    "            \n",
    "        df_p = df_p[['Level', 'agg_column1', 'agg_column2', 'd', 'quantile', 'pred', 'type_of']]\n",
    "        \n",
    "        res.append(_down_cast(df_p))\n",
    "        \n",
    "    # REMOVE THIS\n",
    "    if PLOT_PREDICTIONS:\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    # REMOVE THIS\n",
    "        \n",
    "    # remove to reduce memory usage asap\n",
    "    del features\n",
    "        \n",
    "    # storing predictions in specified file + folder\n",
    "    df_sub_val = pd.concat(res)\n",
    "    group_names = '_'.join(group_columns)\n",
    "    if group_names == '':\n",
    "        group_names = 'Total_X'\n",
    "        \n",
    "    if USE_ALL:\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_use_all.csv'\n",
    "    elif SPARSE_FEATURES:\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_sparse.csv'  \n",
    "    elif include_columns == None:\n",
    "        exclude_names = 'None' if len(exclude_prefix_list) == 0 else '_'.join(exclude_prefix_list)\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_exclude_{exclude_names}.csv'\n",
    "    elif isinstance(include_columns, list):\n",
    "        exclude_names = 'None' if len(include_prefix_list) == 0 else '_'.join(include_prefix_list)\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_include_{exclude_names}.csv'\n",
    "\n",
    "    df_sub_val.to_csv(file_path, index = False)\n",
    "    logger.info('saved under: ' + file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 22:44:27 - __main__ - INFO - starting with all EXCLUDE_COLUMNS\n",
      "2023-11-16 22:44:27 - __main__ - INFO - Exclude columns: ['auto_sold_ma']\n",
      "2023-11-16 22:44:27 - __main__ - INFO - starting with agg_level: Level1\n",
      "2023-11-16 22:44:27 - __main__ - INFO - (re)loading features\n",
      "2023-11-16 22:44:27 - __main__ - INFO - feature: ['d', 'sold', 'auto_sold_56', 'auto_sold_14', 'auto_sold_7', 'auto_sold_1', 'auto_sold_2', 'auto_sold_28', 'auto_sold_ma_28', 'auto_sold_std_14', 'auto_sold_ma_7', 'auto_sold_ma_56', 'auto_sold_std_56', 'auto_sold_ma_168', 'auto_sold_ma_21', 'auto_sold_std_7', 'auto_sold_ma_14', 'auto_sold_std_28', 'auto_sold_ma_112', 'auto_sold_std_168', 'auto_sold_std_21', 'auto_sold_std_112', 'auto_sold_ewm_7', 'auto_sold_ewm_3', 'auto_sold_ewm_30', 'auto_sold_ewm_15', 'auto_sold_ewm_100', 'autoquantiles_sold_qtile_28_0.1', 'autoquantiles_sold_qtile_28_0.99', 'autoquantiles_sold_qtile_14_0.75', 'autoquantiles_sold_qtile_56_0.99', 'autoquantiles_sold_qtile_112_0.01', 'autoquantiles_sold_qtile_14_0.25', 'autoquantiles_sold_qtile_14_0.9', 'autoquantiles_sold_qtile_56_0.1', 'autoquantiles_sold_qtile_56_0.75', 'autoquantiles_sold_qtile_28_0.75', 'autoquantiles_sold_qtile_14_0.99', 'autoquantiles_sold_qtile_14_0.1', 'autoquantiles_sold_qtile_56_0.25', 'autoquantiles_sold_qtile_112_0.99', 'autoquantiles_sold_qtile_112_0.9', 'autoquantiles_sold_qtile_56_0.9', 'autoquantiles_sold_qtile_28_0.9', 'autoquantiles_sold_qtile_112_0.25', 'autoquantiles_sold_qtile_112_0.75', 'autoquantiles_sold_qtile_14_0.5', 'autoquantiles_sold_qtile_56_0.5', 'autoquantiles_sold_qtile_112_0.1', 'autoquantiles_sold_qtile_28_0.25', 'autoquantiles_sold_qtile_112_0.5', 'autoquantiles_sold_qtile_28_0.5', 'price_momentum_m', 'price_momentum_y', 'price_momentum_w', 'price_auto_std_28', 'price_auto_std_56', 'price_auto_std_112', 'seasonal_weekday_Friday', 'seasonal_weekday_Thursday', 'seasonal_weekday_Monday', 'seasonal_weekday_Tuesday', 'seasonal_weekday_Saturday', 'seasonal_weekday_Wednesday', 'seasonal_weekday_Sunday', 'seasonal_month_1', 'seasonal_month_4', 'seasonal_month_2', 'seasonal_month_3', 'seasonal_month_6', 'seasonal_month_5', 'seasonal_month_9', 'seasonal_month_8', 'seasonal_month_11', 'seasonal_month_7', 'seasonal_month_10', 'seasonal_month_12', 'seasonal_monthday_28', 'seasonal_monthday_1', 'seasonal_monthday_25', 'seasonal_monthday_20', 'seasonal_monthday_10', 'seasonal_monthday_16', 'seasonal_monthday_12', 'seasonal_monthday_2', 'seasonal_monthday_8', 'seasonal_monthday_23', 'seasonal_monthday_29', 'seasonal_monthday_15', 'seasonal_monthday_4', 'seasonal_monthday_17', 'seasonal_monthday_11', 'seasonal_monthday_9', 'seasonal_monthday_13', 'seasonal_monthday_14', 'seasonal_monthday_6', 'seasonal_monthday_31', 'seasonal_monthday_5', 'seasonal_monthday_3', 'seasonal_monthday_22', 'seasonal_monthday_27', 'seasonal_monthday_7', 'seasonal_monthday_30', 'seasonal_monthday_21', 'seasonal_monthday_26', 'seasonal_monthday_18', 'seasonal_monthday_19', 'seasonal_monthday_24', 'days_fwd']\n",
      "2023-11-16 22:44:40 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_Total_X_exclude_auto_sold_ma.csv\n",
      "2023-11-16 22:44:40 - __main__ - INFO - Exclude columns: ['auto_sold_ewm']\n",
      "2023-11-16 22:44:40 - __main__ - INFO - starting with agg_level: Level1\n",
      "2023-11-16 22:44:40 - __main__ - INFO - (re)loading features\n",
      "2023-11-16 22:44:40 - __main__ - INFO - feature: ['d', 'sold', 'auto_sold_56', 'auto_sold_14', 'auto_sold_7', 'auto_sold_1', 'auto_sold_2', 'auto_sold_28', 'auto_sold_ma_28', 'auto_sold_std_14', 'auto_sold_ma_7', 'auto_sold_ma_56', 'auto_sold_std_56', 'auto_sold_ma_168', 'auto_sold_ma_21', 'auto_sold_std_7', 'auto_sold_ma_14', 'auto_sold_std_28', 'auto_sold_ma_112', 'auto_sold_std_168', 'auto_sold_std_21', 'auto_sold_std_112', 'auto_sold_ewm_7', 'auto_sold_ewm_3', 'auto_sold_ewm_30', 'auto_sold_ewm_15', 'auto_sold_ewm_100', 'autoquantiles_sold_qtile_28_0.1', 'autoquantiles_sold_qtile_28_0.99', 'autoquantiles_sold_qtile_14_0.75', 'autoquantiles_sold_qtile_56_0.99', 'autoquantiles_sold_qtile_112_0.01', 'autoquantiles_sold_qtile_14_0.25', 'autoquantiles_sold_qtile_14_0.9', 'autoquantiles_sold_qtile_56_0.1', 'autoquantiles_sold_qtile_56_0.75', 'autoquantiles_sold_qtile_28_0.75', 'autoquantiles_sold_qtile_14_0.99', 'autoquantiles_sold_qtile_14_0.1', 'autoquantiles_sold_qtile_56_0.25', 'autoquantiles_sold_qtile_112_0.99', 'autoquantiles_sold_qtile_112_0.9', 'autoquantiles_sold_qtile_56_0.9', 'autoquantiles_sold_qtile_28_0.9', 'autoquantiles_sold_qtile_112_0.25', 'autoquantiles_sold_qtile_112_0.75', 'autoquantiles_sold_qtile_14_0.5', 'autoquantiles_sold_qtile_56_0.5', 'autoquantiles_sold_qtile_112_0.1', 'autoquantiles_sold_qtile_28_0.25', 'autoquantiles_sold_qtile_112_0.5', 'autoquantiles_sold_qtile_28_0.5', 'price_momentum_m', 'price_momentum_y', 'price_momentum_w', 'price_auto_std_28', 'price_auto_std_56', 'price_auto_std_112', 'seasonal_weekday_Friday', 'seasonal_weekday_Thursday', 'seasonal_weekday_Monday', 'seasonal_weekday_Tuesday', 'seasonal_weekday_Saturday', 'seasonal_weekday_Wednesday', 'seasonal_weekday_Sunday', 'seasonal_month_1', 'seasonal_month_4', 'seasonal_month_2', 'seasonal_month_3', 'seasonal_month_6', 'seasonal_month_5', 'seasonal_month_9', 'seasonal_month_8', 'seasonal_month_11', 'seasonal_month_7', 'seasonal_month_10', 'seasonal_month_12', 'seasonal_monthday_28', 'seasonal_monthday_1', 'seasonal_monthday_25', 'seasonal_monthday_20', 'seasonal_monthday_10', 'seasonal_monthday_16', 'seasonal_monthday_12', 'seasonal_monthday_2', 'seasonal_monthday_8', 'seasonal_monthday_23', 'seasonal_monthday_29', 'seasonal_monthday_15', 'seasonal_monthday_4', 'seasonal_monthday_17', 'seasonal_monthday_11', 'seasonal_monthday_9', 'seasonal_monthday_13', 'seasonal_monthday_14', 'seasonal_monthday_6', 'seasonal_monthday_31', 'seasonal_monthday_5', 'seasonal_monthday_3', 'seasonal_monthday_22', 'seasonal_monthday_27', 'seasonal_monthday_7', 'seasonal_monthday_30', 'seasonal_monthday_21', 'seasonal_monthday_26', 'seasonal_monthday_18', 'seasonal_monthday_19', 'seasonal_monthday_24', 'days_fwd']\n",
      "2023-11-16 22:44:53 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_Total_X_exclude_auto_sold_ewm.csv\n",
      "2023-11-16 22:44:53 - __main__ - INFO - Exclude columns: ['autoquantiles_sold_qtile']\n",
      "2023-11-16 22:44:53 - __main__ - INFO - starting with agg_level: Level1\n",
      "2023-11-16 22:44:53 - __main__ - INFO - (re)loading features\n",
      "2023-11-16 22:44:53 - __main__ - INFO - feature: ['d', 'sold', 'auto_sold_56', 'auto_sold_14', 'auto_sold_7', 'auto_sold_1', 'auto_sold_2', 'auto_sold_28', 'auto_sold_ma_28', 'auto_sold_std_14', 'auto_sold_ma_7', 'auto_sold_ma_56', 'auto_sold_std_56', 'auto_sold_ma_168', 'auto_sold_ma_21', 'auto_sold_std_7', 'auto_sold_ma_14', 'auto_sold_std_28', 'auto_sold_ma_112', 'auto_sold_std_168', 'auto_sold_std_21', 'auto_sold_std_112', 'auto_sold_ewm_7', 'auto_sold_ewm_3', 'auto_sold_ewm_30', 'auto_sold_ewm_15', 'auto_sold_ewm_100', 'autoquantiles_sold_qtile_28_0.1', 'autoquantiles_sold_qtile_28_0.99', 'autoquantiles_sold_qtile_14_0.75', 'autoquantiles_sold_qtile_56_0.99', 'autoquantiles_sold_qtile_112_0.01', 'autoquantiles_sold_qtile_14_0.25', 'autoquantiles_sold_qtile_14_0.9', 'autoquantiles_sold_qtile_56_0.1', 'autoquantiles_sold_qtile_56_0.75', 'autoquantiles_sold_qtile_28_0.75', 'autoquantiles_sold_qtile_14_0.99', 'autoquantiles_sold_qtile_14_0.1', 'autoquantiles_sold_qtile_56_0.25', 'autoquantiles_sold_qtile_112_0.99', 'autoquantiles_sold_qtile_112_0.9', 'autoquantiles_sold_qtile_56_0.9', 'autoquantiles_sold_qtile_28_0.9', 'autoquantiles_sold_qtile_112_0.25', 'autoquantiles_sold_qtile_112_0.75', 'autoquantiles_sold_qtile_14_0.5', 'autoquantiles_sold_qtile_56_0.5', 'autoquantiles_sold_qtile_112_0.1', 'autoquantiles_sold_qtile_28_0.25', 'autoquantiles_sold_qtile_112_0.5', 'autoquantiles_sold_qtile_28_0.5', 'price_momentum_m', 'price_momentum_y', 'price_momentum_w', 'price_auto_std_28', 'price_auto_std_56', 'price_auto_std_112', 'seasonal_weekday_Friday', 'seasonal_weekday_Thursday', 'seasonal_weekday_Monday', 'seasonal_weekday_Tuesday', 'seasonal_weekday_Saturday', 'seasonal_weekday_Wednesday', 'seasonal_weekday_Sunday', 'seasonal_month_1', 'seasonal_month_4', 'seasonal_month_2', 'seasonal_month_3', 'seasonal_month_6', 'seasonal_month_5', 'seasonal_month_9', 'seasonal_month_8', 'seasonal_month_11', 'seasonal_month_7', 'seasonal_month_10', 'seasonal_month_12', 'seasonal_monthday_28', 'seasonal_monthday_1', 'seasonal_monthday_25', 'seasonal_monthday_20', 'seasonal_monthday_10', 'seasonal_monthday_16', 'seasonal_monthday_12', 'seasonal_monthday_2', 'seasonal_monthday_8', 'seasonal_monthday_23', 'seasonal_monthday_29', 'seasonal_monthday_15', 'seasonal_monthday_4', 'seasonal_monthday_17', 'seasonal_monthday_11', 'seasonal_monthday_9', 'seasonal_monthday_13', 'seasonal_monthday_14', 'seasonal_monthday_6', 'seasonal_monthday_31', 'seasonal_monthday_5', 'seasonal_monthday_3', 'seasonal_monthday_22', 'seasonal_monthday_27', 'seasonal_monthday_7', 'seasonal_monthday_30', 'seasonal_monthday_21', 'seasonal_monthday_26', 'seasonal_monthday_18', 'seasonal_monthday_19', 'seasonal_monthday_24', 'days_fwd']\n",
      "2023-11-16 22:45:05 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_Total_X_exclude_autoquantiles_sold_qtile.csv\n",
      "2023-11-16 22:45:05 - __main__ - INFO - Exclude columns: ['price_monentum']\n",
      "2023-11-16 22:45:05 - __main__ - INFO - starting with agg_level: Level1\n",
      "2023-11-16 22:45:05 - __main__ - INFO - (re)loading features\n",
      "2023-11-16 22:45:05 - __main__ - INFO - feature: ['d', 'sold', 'auto_sold_56', 'auto_sold_14', 'auto_sold_7', 'auto_sold_1', 'auto_sold_2', 'auto_sold_28', 'auto_sold_ma_28', 'auto_sold_std_14', 'auto_sold_ma_7', 'auto_sold_ma_56', 'auto_sold_std_56', 'auto_sold_ma_168', 'auto_sold_ma_21', 'auto_sold_std_7', 'auto_sold_ma_14', 'auto_sold_std_28', 'auto_sold_ma_112', 'auto_sold_std_168', 'auto_sold_std_21', 'auto_sold_std_112', 'auto_sold_ewm_7', 'auto_sold_ewm_3', 'auto_sold_ewm_30', 'auto_sold_ewm_15', 'auto_sold_ewm_100', 'autoquantiles_sold_qtile_28_0.1', 'autoquantiles_sold_qtile_28_0.99', 'autoquantiles_sold_qtile_14_0.75', 'autoquantiles_sold_qtile_56_0.99', 'autoquantiles_sold_qtile_112_0.01', 'autoquantiles_sold_qtile_14_0.25', 'autoquantiles_sold_qtile_14_0.9', 'autoquantiles_sold_qtile_56_0.1', 'autoquantiles_sold_qtile_56_0.75', 'autoquantiles_sold_qtile_28_0.75', 'autoquantiles_sold_qtile_14_0.99', 'autoquantiles_sold_qtile_14_0.1', 'autoquantiles_sold_qtile_56_0.25', 'autoquantiles_sold_qtile_112_0.99', 'autoquantiles_sold_qtile_112_0.9', 'autoquantiles_sold_qtile_56_0.9', 'autoquantiles_sold_qtile_28_0.9', 'autoquantiles_sold_qtile_112_0.25', 'autoquantiles_sold_qtile_112_0.75', 'autoquantiles_sold_qtile_14_0.5', 'autoquantiles_sold_qtile_56_0.5', 'autoquantiles_sold_qtile_112_0.1', 'autoquantiles_sold_qtile_28_0.25', 'autoquantiles_sold_qtile_112_0.5', 'autoquantiles_sold_qtile_28_0.5', 'price_momentum_m', 'price_momentum_y', 'price_momentum_w', 'price_auto_std_28', 'price_auto_std_56', 'price_auto_std_112', 'seasonal_weekday_Friday', 'seasonal_weekday_Thursday', 'seasonal_weekday_Monday', 'seasonal_weekday_Tuesday', 'seasonal_weekday_Saturday', 'seasonal_weekday_Wednesday', 'seasonal_weekday_Sunday', 'seasonal_month_1', 'seasonal_month_4', 'seasonal_month_2', 'seasonal_month_3', 'seasonal_month_6', 'seasonal_month_5', 'seasonal_month_9', 'seasonal_month_8', 'seasonal_month_11', 'seasonal_month_7', 'seasonal_month_10', 'seasonal_month_12', 'seasonal_monthday_28', 'seasonal_monthday_1', 'seasonal_monthday_25', 'seasonal_monthday_20', 'seasonal_monthday_10', 'seasonal_monthday_16', 'seasonal_monthday_12', 'seasonal_monthday_2', 'seasonal_monthday_8', 'seasonal_monthday_23', 'seasonal_monthday_29', 'seasonal_monthday_15', 'seasonal_monthday_4', 'seasonal_monthday_17', 'seasonal_monthday_11', 'seasonal_monthday_9', 'seasonal_monthday_13', 'seasonal_monthday_14', 'seasonal_monthday_6', 'seasonal_monthday_31', 'seasonal_monthday_5', 'seasonal_monthday_3', 'seasonal_monthday_22', 'seasonal_monthday_27', 'seasonal_monthday_7', 'seasonal_monthday_30', 'seasonal_monthday_21', 'seasonal_monthday_26', 'seasonal_monthday_18', 'seasonal_monthday_19', 'seasonal_monthday_24', 'days_fwd']\n",
      "2023-11-16 22:45:17 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_Total_X_exclude_price_monentum.csv\n",
      "2023-11-16 22:45:17 - __main__ - INFO - Exclude columns: ['seasonal_weekday']\n",
      "2023-11-16 22:45:17 - __main__ - INFO - starting with agg_level: Level1\n",
      "2023-11-16 22:45:17 - __main__ - INFO - (re)loading features\n",
      "2023-11-16 22:45:17 - __main__ - INFO - feature: ['d', 'sold', 'auto_sold_56', 'auto_sold_14', 'auto_sold_7', 'auto_sold_1', 'auto_sold_2', 'auto_sold_28', 'auto_sold_ma_28', 'auto_sold_std_14', 'auto_sold_ma_7', 'auto_sold_ma_56', 'auto_sold_std_56', 'auto_sold_ma_168', 'auto_sold_ma_21', 'auto_sold_std_7', 'auto_sold_ma_14', 'auto_sold_std_28', 'auto_sold_ma_112', 'auto_sold_std_168', 'auto_sold_std_21', 'auto_sold_std_112', 'auto_sold_ewm_7', 'auto_sold_ewm_3', 'auto_sold_ewm_30', 'auto_sold_ewm_15', 'auto_sold_ewm_100', 'autoquantiles_sold_qtile_28_0.1', 'autoquantiles_sold_qtile_28_0.99', 'autoquantiles_sold_qtile_14_0.75', 'autoquantiles_sold_qtile_56_0.99', 'autoquantiles_sold_qtile_112_0.01', 'autoquantiles_sold_qtile_14_0.25', 'autoquantiles_sold_qtile_14_0.9', 'autoquantiles_sold_qtile_56_0.1', 'autoquantiles_sold_qtile_56_0.75', 'autoquantiles_sold_qtile_28_0.75', 'autoquantiles_sold_qtile_14_0.99', 'autoquantiles_sold_qtile_14_0.1', 'autoquantiles_sold_qtile_56_0.25', 'autoquantiles_sold_qtile_112_0.99', 'autoquantiles_sold_qtile_112_0.9', 'autoquantiles_sold_qtile_56_0.9', 'autoquantiles_sold_qtile_28_0.9', 'autoquantiles_sold_qtile_112_0.25', 'autoquantiles_sold_qtile_112_0.75', 'autoquantiles_sold_qtile_14_0.5', 'autoquantiles_sold_qtile_56_0.5', 'autoquantiles_sold_qtile_112_0.1', 'autoquantiles_sold_qtile_28_0.25', 'autoquantiles_sold_qtile_112_0.5', 'autoquantiles_sold_qtile_28_0.5', 'price_momentum_m', 'price_momentum_y', 'price_momentum_w', 'price_auto_std_28', 'price_auto_std_56', 'price_auto_std_112', 'seasonal_weekday_Friday', 'seasonal_weekday_Thursday', 'seasonal_weekday_Monday', 'seasonal_weekday_Tuesday', 'seasonal_weekday_Saturday', 'seasonal_weekday_Wednesday', 'seasonal_weekday_Sunday', 'seasonal_month_1', 'seasonal_month_4', 'seasonal_month_2', 'seasonal_month_3', 'seasonal_month_6', 'seasonal_month_5', 'seasonal_month_9', 'seasonal_month_8', 'seasonal_month_11', 'seasonal_month_7', 'seasonal_month_10', 'seasonal_month_12', 'seasonal_monthday_28', 'seasonal_monthday_1', 'seasonal_monthday_25', 'seasonal_monthday_20', 'seasonal_monthday_10', 'seasonal_monthday_16', 'seasonal_monthday_12', 'seasonal_monthday_2', 'seasonal_monthday_8', 'seasonal_monthday_23', 'seasonal_monthday_29', 'seasonal_monthday_15', 'seasonal_monthday_4', 'seasonal_monthday_17', 'seasonal_monthday_11', 'seasonal_monthday_9', 'seasonal_monthday_13', 'seasonal_monthday_14', 'seasonal_monthday_6', 'seasonal_monthday_31', 'seasonal_monthday_5', 'seasonal_monthday_3', 'seasonal_monthday_22', 'seasonal_monthday_27', 'seasonal_monthday_7', 'seasonal_monthday_30', 'seasonal_monthday_21', 'seasonal_monthday_26', 'seasonal_monthday_18', 'seasonal_monthday_19', 'seasonal_monthday_24', 'days_fwd']\n",
      "2023-11-16 22:45:29 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_Total_X_exclude_seasonal_weekday.csv\n",
      "2023-11-16 22:45:29 - __main__ - INFO - Exclude columns: ['seasonal_monthday']\n",
      "2023-11-16 22:45:29 - __main__ - INFO - starting with agg_level: Level1\n",
      "2023-11-16 22:45:29 - __main__ - INFO - (re)loading features\n",
      "2023-11-16 22:45:29 - __main__ - INFO - feature: ['d', 'sold', 'auto_sold_56', 'auto_sold_14', 'auto_sold_7', 'auto_sold_1', 'auto_sold_2', 'auto_sold_28', 'auto_sold_ma_28', 'auto_sold_std_14', 'auto_sold_ma_7', 'auto_sold_ma_56', 'auto_sold_std_56', 'auto_sold_ma_168', 'auto_sold_ma_21', 'auto_sold_std_7', 'auto_sold_ma_14', 'auto_sold_std_28', 'auto_sold_ma_112', 'auto_sold_std_168', 'auto_sold_std_21', 'auto_sold_std_112', 'auto_sold_ewm_7', 'auto_sold_ewm_3', 'auto_sold_ewm_30', 'auto_sold_ewm_15', 'auto_sold_ewm_100', 'autoquantiles_sold_qtile_28_0.1', 'autoquantiles_sold_qtile_28_0.99', 'autoquantiles_sold_qtile_14_0.75', 'autoquantiles_sold_qtile_56_0.99', 'autoquantiles_sold_qtile_112_0.01', 'autoquantiles_sold_qtile_14_0.25', 'autoquantiles_sold_qtile_14_0.9', 'autoquantiles_sold_qtile_56_0.1', 'autoquantiles_sold_qtile_56_0.75', 'autoquantiles_sold_qtile_28_0.75', 'autoquantiles_sold_qtile_14_0.99', 'autoquantiles_sold_qtile_14_0.1', 'autoquantiles_sold_qtile_56_0.25', 'autoquantiles_sold_qtile_112_0.99', 'autoquantiles_sold_qtile_112_0.9', 'autoquantiles_sold_qtile_56_0.9', 'autoquantiles_sold_qtile_28_0.9', 'autoquantiles_sold_qtile_112_0.25', 'autoquantiles_sold_qtile_112_0.75', 'autoquantiles_sold_qtile_14_0.5', 'autoquantiles_sold_qtile_56_0.5', 'autoquantiles_sold_qtile_112_0.1', 'autoquantiles_sold_qtile_28_0.25', 'autoquantiles_sold_qtile_112_0.5', 'autoquantiles_sold_qtile_28_0.5', 'price_momentum_m', 'price_momentum_y', 'price_momentum_w', 'price_auto_std_28', 'price_auto_std_56', 'price_auto_std_112', 'seasonal_weekday_Friday', 'seasonal_weekday_Thursday', 'seasonal_weekday_Monday', 'seasonal_weekday_Tuesday', 'seasonal_weekday_Saturday', 'seasonal_weekday_Wednesday', 'seasonal_weekday_Sunday', 'seasonal_month_1', 'seasonal_month_4', 'seasonal_month_2', 'seasonal_month_3', 'seasonal_month_6', 'seasonal_month_5', 'seasonal_month_9', 'seasonal_month_8', 'seasonal_month_11', 'seasonal_month_7', 'seasonal_month_10', 'seasonal_month_12', 'seasonal_monthday_28', 'seasonal_monthday_1', 'seasonal_monthday_25', 'seasonal_monthday_20', 'seasonal_monthday_10', 'seasonal_monthday_16', 'seasonal_monthday_12', 'seasonal_monthday_2', 'seasonal_monthday_8', 'seasonal_monthday_23', 'seasonal_monthday_29', 'seasonal_monthday_15', 'seasonal_monthday_4', 'seasonal_monthday_17', 'seasonal_monthday_11', 'seasonal_monthday_9', 'seasonal_monthday_13', 'seasonal_monthday_14', 'seasonal_monthday_6', 'seasonal_monthday_31', 'seasonal_monthday_5', 'seasonal_monthday_3', 'seasonal_monthday_22', 'seasonal_monthday_27', 'seasonal_monthday_7', 'seasonal_monthday_30', 'seasonal_monthday_21', 'seasonal_monthday_26', 'seasonal_monthday_18', 'seasonal_monthday_19', 'seasonal_monthday_24', 'days_fwd']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m         \u001b[39mfor\u001b[39;00m agg_level \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(AGG_LEVEL_COLUMNS\u001b[39m.\u001b[39mkeys())[TEST_NUMB:TEST_NUMBER]: \u001b[39m# for each aggregation level\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m             logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstarting with agg_level: \u001b[39m\u001b[39m{\u001b[39;00magg_level\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m             train_level_all_quantiles(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m                 agg_level,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m                 sub_d_start\u001b[39m=\u001b[39;49msub_d_start,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m                 type_of\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m                 exclude_columns\u001b[39m=\u001b[39;49mexclude_columns,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m                 do_grid_search\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m                 store_submissions_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtemp_submissions/\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m             )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mfinished all EXCLUDE_COLUMNS\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39m---------------------------------\u001b[39m\u001b[39m'\u001b[39m)            \n",
      "\u001b[1;32m/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     params_grid_train[\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m quantile\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39m# train_best_model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m# logger.info(f'features: {str(features_train.columns)}')\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m mod \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(params_grid_train,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     train_set \u001b[39m=\u001b[39;49m lgb\u001b[39m.\u001b[39;49mDataset(features_train, targets_train)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m predictions \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39mpredict(features_predict)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X20sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39mif\u001b[39;00m normalize:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# all groups: seasonal, auto, autoquantiles, momentum\n",
    "SPARSE_FEATURES = [\n",
    "    'autoquantiles_sold_ma_28_0.5', 'auto_sold_ma_28', 'auto_sold_ma_168', 'autoquantiles_sold_ma_168_0.165','autoquantiles_sold_ma_168_0.835',\n",
    "]\n",
    "SPARSE_FEATURES += [f'seasonal_weekday_{i}' for i in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']]\n",
    "SPARSE_FEATURES += [f'seasonal_monthday_{i}' for i in range(1,32)]\n",
    "SPARSE_FEATURES += [f'seasonal_month_{i}' for i in range(1,13)]\n",
    "# SPARSE_FEATURES += [f'state_{i}' for i in ['CA', 'TX', 'WI']]\n",
    "\n",
    "USE_ALL = False\n",
    "SPARSE_FEATURES = None\n",
    "PLOT_PREDICTIONS = False\n",
    "\n",
    "# sparse_features = ['dayofweek', 'dayofmonth', \n",
    "#                      'qs_30d_ewm', 'qs_100d_ewm',\n",
    "#                     'qs_median_28d', 'qs_mean_28d',# 'qs_stdev_28d',\n",
    "#                     'state_id',\n",
    "#                #     'store_id',\n",
    "#                    'qs_qtile90_28d',\n",
    "#                     'pct_nonzero_days_28d',\n",
    "#                     'days_fwd'\n",
    "#                     ]\n",
    "\n",
    "undersampling_dict = {\n",
    "    # 'Level5': .15,\n",
    "    # 'Level6': .15,\n",
    "    # 'Level7': .15,\n",
    "    # 'Level8': .15,\n",
    "    # 'Level9': .1,\n",
    "    'Level10': .001,\n",
    "    'Level11': .0001,\n",
    "    'Level12': .00001\n",
    "}\n",
    "\n",
    "HIGH_UNDERSAMPLING = True\n",
    "TEST_NUMBER = 1\n",
    "TEST_NUMB = 0\n",
    "PARAM_GRID_TRAIN = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': 4,\n",
    "    # 'subsample': .9,\n",
    "    # 'subsample_freq': 1,\n",
    "    \"num_leaves\": 30,\n",
    "    # \"min_child_weight\": .1,\n",
    "    # \"min_child_samples\": 4,\n",
    "    \"hist_pool_size\": 1000,\n",
    "    # 'feature_fraction': 0.9, #.5\n",
    "    # 'bagging_fraction': .8,\n",
    "    \"learning_rate\": 0.01, # .07\n",
    "    \"n_estimators\": 800,#100\n",
    "    \"max_depth\": 10,\n",
    "    # 'reg_sqrt': True,\n",
    "    # 'req_lambda': .00001,\n",
    "    # 'reg_alpha': .00001,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -100,\n",
    "    'n_jobs': 4,\n",
    "    # 'eval_at': 10,\n",
    "    'hist_pool_size': 1000,\n",
    "    'verbose_eval': -100,\n",
    "    # 'boost_from_average': False,\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    # 'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n",
    "    'min_child_samples': [4],\n",
    "    'min_child_weight': [0,0.1],\n",
    "    'num_leaves': [20, 30], # 50, 70, 90, ],\n",
    "    'learning_rate': [0.04, 0.07, 0.1],   # 0.02, 0.03,        \n",
    "    # 'colsample_bytree': [0.3, 0.5, 0.7, 0.8, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "    # 'colsample_bynode':[0.1, 0.15, 0.2, 0.2, 0.2, 0.25, 0.3, 0.5, 0.65, 0.8, 0.9, 1],\n",
    "    # 'reg_lambda': [0, 1e-5, 1e-5, 1e-4, 1e-2, 1, 10, ],\n",
    "    # 'reg_alpha': [0, 1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 1, 10, 100, 1000,],\n",
    "    'subsample': [ 0.9, 1 ],\n",
    "    'subsample_freq': [1],\n",
    "}\n",
    "\n",
    "ALL_PREFIXES = [\n",
    "    'auto_sold_' # lagged values\n",
    "    'auto_sold_ma',\n",
    "    'auto_sold_std',\n",
    "    'auto_sold_ewm',\n",
    "    'autoquantiles_sold_qtile',\n",
    "    'price_momentum',\n",
    "    'seasonal_',\n",
    "    'seasonal_weekday',\n",
    "    'seasonal_monthday',\n",
    "    'seasonal_month',\n",
    "    # 'days_fwd',\n",
    "]\n",
    "\n",
    "EXCLUDE_COLUMNS_LIST = (\n",
    "    # [],\n",
    "    # ['seasonal'],\n",
    "    # ['momentum'],\n",
    "    # ['autoquantiles'],\n",
    "    # ['auto'],\n",
    "    # ['seasonal', 'auto', 'momentum'],\n",
    "    # ['auto', 'momentum']\n",
    "    ['auto_sold_ma'],\n",
    "    ['auto_sold_ewm'],\n",
    "    ['autoquantiles_sold_qtile'],\n",
    "    ['price_monentum'],\n",
    "    ['seasonal_weekday'],\n",
    "    ['seasonal_monthday'],\n",
    ")\n",
    "INCLUDE_COLUMNS_LIST = (\n",
    "    ['auto_sold_ma'],\n",
    "    ['auto_sold_ewm'],\n",
    "    ['autoquantiles_sold_qtile'],\n",
    "    ['seasonal'],\n",
    ")\n",
    "\n",
    "logger.info('starting with all EXCLUDE_COLUMNS')\n",
    "for exclude_columns in EXCLUDE_COLUMNS_LIST: # for each specified feature combination\n",
    "    logger.info(f'Exclude columns: {str(exclude_columns)}')\n",
    "    # for sub_d_start in D_CROSS_VAL_START_LIST: # for each fold\n",
    "    for sub_d_start in D_CROSS_VAL_START_LIST[-1:]:#[:1 if TEST_NUMBER>0 else 100]:\n",
    "        for agg_level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]: # for each aggregation level\n",
    "            logger.info(f'starting with agg_level: {agg_level}')\n",
    "            train_level_all_quantiles(\n",
    "                agg_level,\n",
    "                sub_d_start=sub_d_start,\n",
    "                type_of='val', \n",
    "                exclude_columns=exclude_columns,\n",
    "                do_grid_search=False,\n",
    "                store_submissions_path='temp_submissions_research/',\n",
    "            )\n",
    "logger.info('finished all EXCLUDE_COLUMNS')\n",
    "logger.info('---------------------------------')            \n",
    "logger.info('starting with all EXCLUDE_COLUMNS')            \n",
    "for include_columns in INCLUDE_COLUMNS_LIST: # for each specified feature combination\n",
    "    logger.info(f'Include columns: {str(include_columns)}')\n",
    "    # for sub_d_start in D_CROSS_VAL_START_LIST: # for each fold\n",
    "    for sub_d_start in D_CROSS_VAL_START_LIST[-1:]:#[:1 if TEST_NUMBER>0 else 100]:\n",
    "        for agg_level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]: # for each aggregation level\n",
    "            logger.info(f'starting with agg_level: {agg_level}')\n",
    "            train_level_all_quantiles(\n",
    "                agg_level,\n",
    "                sub_d_start=sub_d_start,\n",
    "                type_of='val', \n",
    "                exclude_columns=None,\n",
    "                include_columns=include_columns,\n",
    "                do_grid_search=False,\n",
    "                store_submissions_path='temp_submissions_research/',\n",
    "            )\n",
    "logger.info('finished all INCLUDE_COLUMNS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load val + eval prediction files and merge to one submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_columns = '_'.join([])\n",
    "# if exclude_columns == '':\n",
    "#     exclude_columns = 'None'\n",
    "\n",
    "# dfs: list = []\n",
    "# for level in AGG_LEVEL_COLUMNS:\n",
    "#     agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "#     group_names = '_'.join(agg_columns)\n",
    "#     if group_names == '':\n",
    "#         group_names = 'Total_X'\n",
    "#     i = str(1802)\n",
    "#     dfs.append(\n",
    "#         f'../data/uncertainty/fold_{i}/temp_submissions/' + f'lgb_multivariate_val_non_transposed_{group_names}_exclude_{exclude_columns}.csv',\n",
    "#     )\n",
    "\n",
    "# df_sub_val = ensemble_submissions_uncertainty(dfs)\n",
    "# transpose = True\n",
    "# if transpose == True:\n",
    "#     sub_validation = df_sub_val.pivot(index='id', columns='d', values='pred').reset_index(drop=False)\n",
    "#     sub_validation.columns = [\"id\"] + [f\"F{i}\" for i in range(1,DAYS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_concat_predictions(fold_name: int, exclude_columns: list = [], include_columns: list = [], sparse = False, use_all = False, load_submissions_path: str = 'temp_submissions/'):\n",
    "    \"\"\" \n",
    "    For specified fold, read the predictions for all aggregation levels and stack them together in one dataframe.\n",
    "    \"\"\"\n",
    "    # D_CV_START_LIST\n",
    "    # if fold_name not in D_CV_START_LIST:\n",
    "        # raise ValueError('fold_name must be a value in D_CV_START_LIST')\n",
    "        \n",
    "    exclude_columns = '_'.join(exclude_columns)\n",
    "    if exclude_columns == '':\n",
    "        exclude_columns = 'None'\n",
    "\n",
    "    logger.info('loading files under path:' + f'../data/uncertainty/fold_{fold_name}/' + load_submissions_path)\n",
    "\n",
    "    dfs: list = []\n",
    "    for level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]:\n",
    "        agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "        group_names = '_'.join(agg_columns)\n",
    "        if group_names == '':\n",
    "            group_names = 'Total_X'\n",
    "        \n",
    "        file_path = f'../data/uncertainty/fold_{str(fold_name)}/' + load_submissions_path \n",
    "        file_path += f'lgb_val_nt_{group_names}_'\n",
    "        if use_all:\n",
    "            file_path += f'use_all.csv'  \n",
    "        elif include_columns == None:\n",
    "            file_path += f'exclude_{exclude_columns}.csv'            \n",
    "        elif isinstance(include_columns, list) == None:\n",
    "            file_path += f'include_{include_columns}.csv'\n",
    "        \n",
    "        dfs.append(file_path)\n",
    "    return ensemble_submissions_uncertainty(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_columns = '_'.join([])\n",
    "# if exclude_columns == '':\n",
    "#     exclude_columns = 'None'\n",
    "\n",
    "# dfs: list = []\n",
    "# for level in AGG_LEVEL_COLUMNS:\n",
    "#     agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "#     group_names = '_'.join(agg_columns)\n",
    "#     if group_names == '':\n",
    "#         group_names = 'Total_X'\n",
    "        \n",
    "#     dfs.append(\n",
    "#         PREDICTION_BASE_PATH + f'lgb_multivariate_eval_non_transposed_{group_names}_exclude_{exclude_columns}.csv',\n",
    "#     )\n",
    "\n",
    "# df_sub_eval = ensemble_submissions_uncertainty(dfs)\n",
    "# transpose = True\n",
    "# if transpose == True:\n",
    "#     sub_evaluation = df_sub_eval.pivot(index='id', columns='d', values='pred').reset_index(drop=False)\n",
    "#     sub_evaluation.columns = [\"id\"] + [f\"F{i}\" for i in range(1,DAYS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sub_evaluation = pd.read_csv('../submissions/submission_baseline_evaluation.csv').drop(['Unnamed: 0'], axis=1)\n",
    "# pd.concat([sub_validation, sub_evaluation]).to_csv(SUBMISSION_BASE_PATH + f'submission_lgb_ensemble{exclude_columns}.csv', index=False)\n",
    "# del sub_validation; del sub_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation Prediction, we can compute WRMSSE locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these variables are used later on\n",
    "FORCE_RELOAD = False\n",
    "try:\n",
    "    # simple code to check if variable exists\n",
    "    d_int + 1\n",
    "    if FORCE_RELOAD:\n",
    "        raise Exception()\n",
    "except:\n",
    "    # if not, load again\n",
    "    # takes about 2-3 minutes to reload and parse\n",
    "    d = pd.read_parquet('../data/uncertainty/cv_template/temp.parquet')\n",
    "    d_int = pd.read_parquet('../data/uncertainty/cv_template/temp_d_int.parquet')['d_int']\n",
    "    # d_int = d['d'].str.split('_').apply(lambda x: int(x[1]))\n",
    "    # d_int.to_frame('d_int').to_parquet('../data/uncertainty/cv_template/temp_d_int.parquet', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cv(df: pd.DataFrame, df_sub: pd.DataFrame):\n",
    "    \n",
    "    # to be able to merge\n",
    "    df_sub['id_merge'] = df_sub['id'].str.split('.')\\\n",
    "        .apply(lambda x: x[0])\n",
    "    df_sub['quantile'] = df_sub['id'].str.split('.')\\\n",
    "        .apply(lambda x: float('.'.join([x[-2], x[-1].split('_')[0]])))\n",
    "\n",
    "    # merge predictions in cv template\n",
    "    p = pd.merge(\n",
    "        df,\n",
    "        df_sub,\n",
    "        how='left',\n",
    "        on=['id_merge', 'd']\n",
    "    )\n",
    "    # del df; del df_sub_val\n",
    "    p['id_merge'] = p['id_merge'].astype(str)\n",
    "\n",
    "    for c in ['sold', 'revenue']:\n",
    "        p[c] = p[c].astype(np.float32)\n",
    "    # d = d[d_int < (D_CV_START + 28)]\n",
    "\n",
    "    return WSPL(p, [f'd_{i}' for i in range(D_CV_START, D_CV_START + 500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 22:50:56 - __main__ - INFO - --------------- ['auto_sold_ma'] ---------------\n",
      "2023-11-16 22:50:59 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1914/temp_submissions/\n",
      "2023-11-16 22:51:23 - utils.metrics - INFO - reading weights file\n",
      "2023-11-16 22:51:23 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-11-16 22:51:55 - utils.metrics - INFO - entering loop ...\n",
      "2023-11-16 22:52:01 - utils.metrics - INFO - Level1 - 0.2625163478240969\n",
      "2023-11-16 22:52:03 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-11-16 22:52:08 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-11-16 22:52:28 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-11-16 22:52:29 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-11-16 22:52:29 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-11-16 22:52:29 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-11-16 22:52:29 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-11-16 22:52:29 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-11-16 22:52:29 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-11-16 22:52:29 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-11-16 22:52:29 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-11-16 22:52:32 - __main__ - INFO - 1914 - wspl: 0.2625163478240969\n",
      "2023-11-16 22:52:32 - __main__ - INFO - 1914 - mean wspl: 0.2625163478240969 +/- 0.0\n",
      "2023-11-16 22:52:32 - __main__ - INFO - 1914 - raw results: [0.2625163478240969]\n",
      "2023-11-16 22:52:32 - __main__ - INFO - --------------- ['auto_sold_ewm'] ---------------\n",
      "2023-11-16 22:52:36 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1914/temp_submissions/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m D_CV_START \u001b[39min\u001b[39;00m D_CROSS_VAL_START_LIST[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]:\u001b[39m#[:1 if TEST_NUMBER>0 else 10]:\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     mean_wspl \u001b[39m=\u001b[39m perform_cv(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         _down_cast(d)[d_int \u001b[39m<\u001b[39;49m (D_CV_START \u001b[39m+\u001b[39;49m DAYS)], \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         read_concat_predictions(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m             fold_name \u001b[39m=\u001b[39;49m D_CV_START, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m             exclude_columns \u001b[39m=\u001b[39;49m EXCLUDE_COLUMNS, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m             include_columns \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m             use_all\u001b[39m=\u001b[39;49mUSE_ALL,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m             load_submissions_path\u001b[39m=\u001b[39;49mFOLDER\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     res\u001b[39m.\u001b[39mappend(mean_wspl)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mstr\u001b[39m(D_CV_START) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m - wspl: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(mean_wspl))\n",
      "\u001b[1;32m/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df_sub[\u001b[39m'\u001b[39m\u001b[39mquantile\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_sub[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([x[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], x[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]])))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# merge predictions in cv template\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m p \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mmerge(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     df,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     df_sub,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mleft\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     on\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mid_merge\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39md\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# del df; del df_sub_val\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joeybesseling/projects/m5-forecasting-uncertainty/code/un_training_model_quantiles.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m p[\u001b[39m'\u001b[39m\u001b[39mid_merge\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m p[\u001b[39m'\u001b[39m\u001b[39mid_merge\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/reshape/merge.py:162\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m    148\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[1;32m    149\u001b[0m         left,\n\u001b[1;32m    150\u001b[0m         right,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         validate\u001b[39m=\u001b[39mvalidate,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result(copy\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/reshape/merge.py:809\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindicator:\n\u001b[1;32m    807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indicator_pre_merge(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright)\n\u001b[0;32m--> 809\u001b[0m join_index, left_indexer, right_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_join_info()\n\u001b[1;32m    811\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_and_concat(\n\u001b[1;32m    812\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    813\u001b[0m )\n\u001b[1;32m    814\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_type)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1065\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     join_index, right_indexer, left_indexer \u001b[39m=\u001b[39m _left_join_on_index(\n\u001b[1;32m   1062\u001b[0m         right_ax, left_ax, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys, sort\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort\n\u001b[1;32m   1063\u001b[0m     )\n\u001b[1;32m   1064\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1065\u001b[0m     (left_indexer, right_indexer) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_join_indexers()\n\u001b[1;32m   1067\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_index:\n\u001b[1;32m   1068\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1038\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_join_indexers\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp], npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp]]:\n\u001b[1;32m   1037\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"return the join indexers\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m     \u001b[39mreturn\u001b[39;00m get_join_indexers(\n\u001b[1;32m   1039\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mleft_join_keys, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mright_join_keys, sort\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msort, how\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhow\n\u001b[1;32m   1040\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1669\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[0;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[1;32m   1666\u001b[0m llab, rlab, shape \u001b[39m=\u001b[39m (\u001b[39mlist\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m zipped)\n\u001b[1;32m   1668\u001b[0m \u001b[39m# get flat i8 keys from label lists\u001b[39;00m\n\u001b[0;32m-> 1669\u001b[0m lkey, rkey \u001b[39m=\u001b[39m _get_join_keys(llab, rlab, \u001b[39mtuple\u001b[39;49m(shape), sort)\n\u001b[1;32m   1671\u001b[0m \u001b[39m# factorize keys to a dense i8 space\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[39m# `count` is the num. of unique keys\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[39m# set(lkey) | set(rkey) == range(count)\u001b[39;00m\n\u001b[1;32m   1675\u001b[0m lkey, rkey, count \u001b[39m=\u001b[39m _factorize_keys(lkey, rkey, sort\u001b[39m=\u001b[39msort, how\u001b[39m=\u001b[39mhow)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_tf_env/lib/python3.9/site-packages/pandas/core/reshape/merge.py:2527\u001b[0m, in \u001b[0;36m_get_join_keys\u001b[0;34m(llab, rlab, shape, sort)\u001b[0m\n\u001b[1;32m   2525\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(divide\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2526\u001b[0m         stride \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m=\u001b[39m shape[i]\n\u001b[0;32m-> 2527\u001b[0m     lkey \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m llab[i] \u001b[39m*\u001b[39m stride\n\u001b[1;32m   2528\u001b[0m     rkey \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rlab[i] \u001b[39m*\u001b[39m stride\n\u001b[1;32m   2530\u001b[0m \u001b[39mif\u001b[39;00m nlev \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(shape):  \u001b[39m# all done!\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "EXCLUDE_COLUMNS_LIST = (\n",
    "    # [],\n",
    "    # ['seasonal'],\n",
    "    # ['momentum'],\n",
    "    # ['autoquantiles'],\n",
    "    # ['auto'],\n",
    "    # ['seasonal', 'auto', 'momentum'],\n",
    "    # ['auto', 'momentum']\n",
    "    ['auto_sold_ma'],\n",
    "    ['auto_sold_ewm'],\n",
    "    ['autoquantiles_sold_qtile'],\n",
    "    ['price_monentum'],\n",
    "    ['seasonal_weekday'],\n",
    "    ['seasonal_monthday'],\n",
    ")\n",
    "\n",
    "# EXCLUDE_COLUMNS_LIST = (\n",
    "#     ['use_all'],\n",
    "# )\n",
    "\n",
    "FOLDER = 'temp_submissions/'\n",
    "\n",
    "logger.info('start evaluating exclude columns')\n",
    "for EXCLUDE_COLUMNS in EXCLUDE_COLUMNS_LIST:\n",
    "    logger.info('--------------- ' + str(EXCLUDE_COLUMNS) + ' ---------------')\n",
    "    res = []\n",
    "    for D_CV_START in D_CROSS_VAL_START_LIST[-1:]:#[:1 if TEST_NUMBER>0 else 10]:\n",
    "        \n",
    "        mean_wspl = perform_cv(\n",
    "            _down_cast(d)[d_int < (D_CV_START + DAYS)], \n",
    "            read_concat_predictions(\n",
    "                fold_name = D_CV_START, \n",
    "                exclude_columns = EXCLUDE_COLUMNS, \n",
    "                include_columns = None,\n",
    "                use_all=USE_ALL,\n",
    "                load_submissions_path=FOLDER\n",
    "            )\n",
    "        )\n",
    "        res.append(mean_wspl)\n",
    "        logger.info(str(D_CV_START) + ' - wspl: ' + str(mean_wspl))\n",
    "\n",
    "    logger.info(str(D_CV_START) + ' - mean wspl: ' + str(np.mean(res)) + ' +/- ' + str(np.std(res)))\n",
    "    logger.info(str(D_CV_START) + ' - raw results: ' + str(res))\n",
    "    \n",
    "logger.info('start evaluating include columns')\n",
    "for INCLUDE_COLUMNS in INCLUDE_COLUMNS_LIST:\n",
    "    logger.info('--------------- ' + str(INCLUDE_COLUMNS) + ' ---------------')\n",
    "    res = []\n",
    "    for D_CV_START in D_CROSS_VAL_START_LIST[-1:]:#[:1 if TEST_NUMBER>0 else 10]:\n",
    "        \n",
    "        mean_wspl = perform_cv(\n",
    "            _down_cast(d)[d_int < (D_CV_START + DAYS)], \n",
    "            read_concat_predictions(\n",
    "                fold_name = D_CV_START, \n",
    "                exclude_columns = [], \n",
    "                include_columns = INCLUDE_COLUMNS,\n",
    "                use_all=USE_ALL,\n",
    "                load_submissions_path=FOLDER\n",
    "            )\n",
    "        )\n",
    "        res.append(mean_wspl)\n",
    "        logger.info(str(D_CV_START) + ' - wspl: ' + str(mean_wspl))\n",
    "\n",
    "    logger.info(str(D_CV_START) + ' - mean wspl: ' + str(np.mean(res)) + ' +/- ' + str(np.std(res)))\n",
    "    logger.info(str(D_CV_START) + ' - raw results: ' + str(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Beneath can be used to create submission template\n",
    "The submission template can be used to quickly insert your predictions.\n",
    "It also contains all other (historical) sales to be able to compute the WRMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_validation = pd.read_csv(DATA_BASE_PATH + SALES_VALIDATION)\n",
    "sales_evaluation = pd.read_csv(DATA_BASE_PATH + SALES_EVALUATION)\n",
    "calendar = pd.read_csv(DATA_BASE_PATH + CALENDAR)\n",
    "sell_prices = pd.read_csv(DATA_BASE_PATH + SELL_PRICES)\n",
    "\n",
    "df_val, submission_idx_val = data_preprocessing(sales_validation, calendar, sell_prices)\n",
    "del sales_validation\n",
    "df_eval, submission_idx_eval = data_preprocessing(sales_evaluation, calendar, sell_prices)\n",
    "del sales_evaluation\n",
    "\n",
    "df_val_after_release = df_val[(df_val.wm_yr_wk > df_val.release)]# & (df_val[\"sold\"].notna())]\n",
    "del df_val\n",
    "df_eval_after_release = df_eval[(df_eval.wm_yr_wk > df_eval.release)]# & (df_eval[\"sold\"].notna())]\n",
    "del df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 17:27:39 - __main__ - INFO - Level1\n",
      "2023-08-17 17:27:40 - __main__ - INFO - Level2\n",
      "2023-08-17 17:27:44 - __main__ - INFO - Level3\n",
      "2023-08-17 17:27:48 - __main__ - INFO - Level4\n",
      "2023-08-17 17:27:51 - __main__ - INFO - Level5\n",
      "2023-08-17 17:27:55 - __main__ - INFO - Level6\n",
      "2023-08-17 17:28:00 - __main__ - INFO - Level7\n",
      "2023-08-17 17:28:05 - __main__ - INFO - Level8\n",
      "2023-08-17 17:28:10 - __main__ - INFO - Level9\n",
      "2023-08-17 17:28:15 - __main__ - INFO - Level10\n",
      "2023-08-17 17:28:21 - __main__ - INFO - Level11\n",
      "2023-08-17 17:28:33 - __main__ - INFO - Level12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>agg_column1</th>\n",
       "      <th>agg_column2</th>\n",
       "      <th>d</th>\n",
       "      <th>sold</th>\n",
       "      <th>revenue</th>\n",
       "      <th>id_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_10</td>\n",
       "      <td>24858.0</td>\n",
       "      <td>63029.78</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_100</td>\n",
       "      <td>23653.0</td>\n",
       "      <td>65665.71</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1000</td>\n",
       "      <td>29241.0</td>\n",
       "      <td>82351.45</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1001</td>\n",
       "      <td>33804.0</td>\n",
       "      <td>93975.55</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1002</td>\n",
       "      <td>42447.0</td>\n",
       "      <td>118961.96</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1003</td>\n",
       "      <td>40647.0</td>\n",
       "      <td>116052.48</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1004</td>\n",
       "      <td>32039.0</td>\n",
       "      <td>89314.17</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1005</td>\n",
       "      <td>29501.0</td>\n",
       "      <td>81688.96</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1006</td>\n",
       "      <td>31117.0</td>\n",
       "      <td>85754.15</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1007</td>\n",
       "      <td>27018.0</td>\n",
       "      <td>74244.86</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1008</td>\n",
       "      <td>39707.0</td>\n",
       "      <td>108637.04</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1009</td>\n",
       "      <td>47082.0</td>\n",
       "      <td>128940.24</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_101</td>\n",
       "      <td>24982.0</td>\n",
       "      <td>68908.04</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1010</td>\n",
       "      <td>48360.0</td>\n",
       "      <td>133218.73</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1011</td>\n",
       "      <td>32930.0</td>\n",
       "      <td>92274.15</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1012</td>\n",
       "      <td>33990.0</td>\n",
       "      <td>92743.98</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1013</td>\n",
       "      <td>32956.0</td>\n",
       "      <td>90505.80</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1014</td>\n",
       "      <td>31862.0</td>\n",
       "      <td>87172.76</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1015</td>\n",
       "      <td>35365.0</td>\n",
       "      <td>95702.83</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1016</td>\n",
       "      <td>45705.0</td>\n",
       "      <td>125791.89</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1017</td>\n",
       "      <td>43898.0</td>\n",
       "      <td>123256.45</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1018</td>\n",
       "      <td>36385.0</td>\n",
       "      <td>100212.69</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1019</td>\n",
       "      <td>32258.0</td>\n",
       "      <td>87909.01</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_102</td>\n",
       "      <td>22196.0</td>\n",
       "      <td>60000.65</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1020</td>\n",
       "      <td>29242.0</td>\n",
       "      <td>81367.81</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1021</td>\n",
       "      <td>29452.0</td>\n",
       "      <td>79956.86</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1022</td>\n",
       "      <td>35763.0</td>\n",
       "      <td>97645.15</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1023</td>\n",
       "      <td>44579.0</td>\n",
       "      <td>123721.42</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1024</td>\n",
       "      <td>42582.0</td>\n",
       "      <td>121478.81</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1025</td>\n",
       "      <td>32102.0</td>\n",
       "      <td>89627.71</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1026</td>\n",
       "      <td>28521.0</td>\n",
       "      <td>78796.16</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1027</td>\n",
       "      <td>27904.0</td>\n",
       "      <td>78144.16</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1028</td>\n",
       "      <td>28693.0</td>\n",
       "      <td>78581.27</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1029</td>\n",
       "      <td>32847.0</td>\n",
       "      <td>91347.72</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_103</td>\n",
       "      <td>22117.0</td>\n",
       "      <td>61407.00</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1030</td>\n",
       "      <td>40046.0</td>\n",
       "      <td>114533.56</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1031</td>\n",
       "      <td>38445.0</td>\n",
       "      <td>111826.12</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1032</td>\n",
       "      <td>28603.0</td>\n",
       "      <td>81092.24</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1033</td>\n",
       "      <td>31247.0</td>\n",
       "      <td>87799.42</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1034</td>\n",
       "      <td>36053.0</td>\n",
       "      <td>97214.80</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1035</td>\n",
       "      <td>19783.0</td>\n",
       "      <td>53456.88</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1036</td>\n",
       "      <td>26041.0</td>\n",
       "      <td>75086.93</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1037</td>\n",
       "      <td>31539.0</td>\n",
       "      <td>91356.80</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1038</td>\n",
       "      <td>38182.0</td>\n",
       "      <td>108919.39</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1039</td>\n",
       "      <td>37079.0</td>\n",
       "      <td>97503.03</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_104</td>\n",
       "      <td>22347.0</td>\n",
       "      <td>60736.91</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1040</td>\n",
       "      <td>38010.0</td>\n",
       "      <td>100557.02</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1041</td>\n",
       "      <td>31513.0</td>\n",
       "      <td>83895.82</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1042</td>\n",
       "      <td>35139.0</td>\n",
       "      <td>93359.95</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1043</td>\n",
       "      <td>36894.0</td>\n",
       "      <td>99430.98</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Level agg_column1 agg_column2       d     sold    revenue id_merge\n",
       "0   Level1       Total           X    d_10  24858.0   63029.78  Total_X\n",
       "1   Level1       Total           X   d_100  23653.0   65665.71  Total_X\n",
       "2   Level1       Total           X  d_1000  29241.0   82351.45  Total_X\n",
       "3   Level1       Total           X  d_1001  33804.0   93975.55  Total_X\n",
       "4   Level1       Total           X  d_1002  42447.0  118961.96  Total_X\n",
       "5   Level1       Total           X  d_1003  40647.0  116052.48  Total_X\n",
       "6   Level1       Total           X  d_1004  32039.0   89314.17  Total_X\n",
       "7   Level1       Total           X  d_1005  29501.0   81688.96  Total_X\n",
       "8   Level1       Total           X  d_1006  31117.0   85754.15  Total_X\n",
       "9   Level1       Total           X  d_1007  27018.0   74244.86  Total_X\n",
       "10  Level1       Total           X  d_1008  39707.0  108637.04  Total_X\n",
       "11  Level1       Total           X  d_1009  47082.0  128940.24  Total_X\n",
       "12  Level1       Total           X   d_101  24982.0   68908.04  Total_X\n",
       "13  Level1       Total           X  d_1010  48360.0  133218.73  Total_X\n",
       "14  Level1       Total           X  d_1011  32930.0   92274.15  Total_X\n",
       "15  Level1       Total           X  d_1012  33990.0   92743.98  Total_X\n",
       "16  Level1       Total           X  d_1013  32956.0   90505.80  Total_X\n",
       "17  Level1       Total           X  d_1014  31862.0   87172.76  Total_X\n",
       "18  Level1       Total           X  d_1015  35365.0   95702.83  Total_X\n",
       "19  Level1       Total           X  d_1016  45705.0  125791.89  Total_X\n",
       "20  Level1       Total           X  d_1017  43898.0  123256.45  Total_X\n",
       "21  Level1       Total           X  d_1018  36385.0  100212.69  Total_X\n",
       "22  Level1       Total           X  d_1019  32258.0   87909.01  Total_X\n",
       "23  Level1       Total           X   d_102  22196.0   60000.65  Total_X\n",
       "24  Level1       Total           X  d_1020  29242.0   81367.81  Total_X\n",
       "25  Level1       Total           X  d_1021  29452.0   79956.86  Total_X\n",
       "26  Level1       Total           X  d_1022  35763.0   97645.15  Total_X\n",
       "27  Level1       Total           X  d_1023  44579.0  123721.42  Total_X\n",
       "28  Level1       Total           X  d_1024  42582.0  121478.81  Total_X\n",
       "29  Level1       Total           X  d_1025  32102.0   89627.71  Total_X\n",
       "30  Level1       Total           X  d_1026  28521.0   78796.16  Total_X\n",
       "31  Level1       Total           X  d_1027  27904.0   78144.16  Total_X\n",
       "32  Level1       Total           X  d_1028  28693.0   78581.27  Total_X\n",
       "33  Level1       Total           X  d_1029  32847.0   91347.72  Total_X\n",
       "34  Level1       Total           X   d_103  22117.0   61407.00  Total_X\n",
       "35  Level1       Total           X  d_1030  40046.0  114533.56  Total_X\n",
       "36  Level1       Total           X  d_1031  38445.0  111826.12  Total_X\n",
       "37  Level1       Total           X  d_1032  28603.0   81092.24  Total_X\n",
       "38  Level1       Total           X  d_1033  31247.0   87799.42  Total_X\n",
       "39  Level1       Total           X  d_1034  36053.0   97214.80  Total_X\n",
       "40  Level1       Total           X  d_1035  19783.0   53456.88  Total_X\n",
       "41  Level1       Total           X  d_1036  26041.0   75086.93  Total_X\n",
       "42  Level1       Total           X  d_1037  31539.0   91356.80  Total_X\n",
       "43  Level1       Total           X  d_1038  38182.0  108919.39  Total_X\n",
       "44  Level1       Total           X  d_1039  37079.0   97503.03  Total_X\n",
       "45  Level1       Total           X   d_104  22347.0   60736.91  Total_X\n",
       "46  Level1       Total           X  d_1040  38010.0  100557.02  Total_X\n",
       "47  Level1       Total           X  d_1041  31513.0   83895.82  Total_X\n",
       "48  Level1       Total           X  d_1042  35139.0   93359.95  Total_X\n",
       "49  Level1       Total           X  d_1043  36894.0   99430.98  Total_X"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "df_eval_after_release['revenue'] = df_eval_after_release['sold'] * df_eval_after_release['sell_price']\n",
    "for level in list(AGG_LEVEL_COLUMNS.keys()):\n",
    "    c = AGG_LEVEL_COLUMNS[level]\n",
    "    logger.info(level)\n",
    "    agg_dict = {\n",
    "        'sold': 'sum',\n",
    "        'revenue': 'sum'\n",
    "    }\n",
    "    d1 = df_eval_after_release.groupby(c + ['d']).agg(agg_dict).reset_index(drop=False)\n",
    "    d = pd.DataFrame({\n",
    "        'd': d1['d'],\n",
    "        'sold': d1['sold'],\n",
    "        'revenue': d1['revenue']\n",
    "    })\n",
    "    if len(c) == 0:\n",
    "        d['agg_column1'] = 'Total'\n",
    "        d['agg_column2'] = 'X'\n",
    "    elif len(c) == 1:\n",
    "        d['agg_column1'] = d1[c[0]]\n",
    "        d['agg_column2'] = 'X'\n",
    "    else:\n",
    "        d['agg_column1'] = d1[c[0]]\n",
    "        d['agg_column2'] = d1[c[1]]\n",
    "    d['id_merge'] = d['agg_column1'] + '_' + d['agg_column2']\n",
    "    d['Level'] = level\n",
    "    dfs.append(d[['Level', 'agg_column1', 'agg_column2', 'd', 'sold', 'revenue', 'id_merge']])\n",
    "d = pd.concat(dfs)\n",
    "d.head(50)\n",
    "d.to_parquet('temp.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('test.txt')\n",
    "# file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
