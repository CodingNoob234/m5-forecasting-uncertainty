{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from utils.utils import WRMSSE, RMSSE, _down_cast, data_preprocessing, diff_lists, log_status\n",
    "from utils.utils import cross_validation_on_validation_set, ensemble_submissions, ensemble_submissions_uncertainty\n",
    "from utils.metrics import WSPL\n",
    "from utils.configure_logger import configure_logger\n",
    "from utils.utils import prefixes_in_column\n",
    "from utils import constants\n",
    "\n",
    "configure_logger()\n",
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = constants.DATA_BASE_PATH #'../data/m5-forecasting-accuracy/'\n",
    "DATA_BASE_PATH_UNCERTAINTY = constants.DATA_BASE_PATH_UNCERTAINTY #'../data/m5-forecasting-uncertainty/'\n",
    "SALES_EVALUATION = constants.SALES_EVALUATION \n",
    "SALES_VALIDATION = constants.SALES_VALIDATION\n",
    "CALENDAR = constants.CALENDAR \n",
    "SAMPLE_SUBMISSION = constants.SAMPLE_SUBMISSION \n",
    "SELL_PRICES = constants.SELL_PRICES\n",
    "\n",
    "PRECOMPUTED_BASE_PATH = constants.PRECOMPUTED_BASE_PATH #'../data/uncertainty/features/'\n",
    "\n",
    "DAYS: int = constants.DAYS #28\n",
    "QUANTILES: int = constants.QUANTILES \n",
    "\n",
    "AGG_LEVEL_COLUMNS = constants.AGG_LEVEL_COLUMNS\n",
    "D_CROSS_VAL_START_LIST = constants.D_CROSS_VAL_START_LIST\n",
    "\n",
    "# to simple get the precomputed name\n",
    "precomputed_name = lambda store, eval_val: f'processed_{store}_{eval_val}.pkl'\n",
    "\n",
    "TEST_PATH = constants.TEST_PATH#'test/'\n",
    "PREDICTION_BASE_PATH = constants.PREDICTION_BASE_PATH #'../data/uncertainty/temp_submissions/'\n",
    "SUBMISSION_BASE_PATH = constants.SUBMISSION_BASE_PATH #'../data/uncertainty/final_submissions/'\n",
    "\n",
    "SUB_D_START_VAL: int = constants.SUB_D_START_VAL\n",
    "SUB_D_START_EVAL: int = constants.SUB_D_START_EVAL\n",
    "\n",
    "# the columns are always included after feature processing\n",
    "# because they are required in the training and submission format\n",
    "DROP_FEATURE_COLUMNS: list = constants.DROP_FEATURE_COLUMNS #['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'sold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GridSearch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_status\n",
    "def grid_search(params: dict, param_grid: dict, features, targets, n_folds: int = 1):\n",
    "    \"\"\" \n",
    "    Given a grid with parameters, train lgb model for all possible combinations.\n",
    "    Returns the parameter set with the best score and the dictionary with all results.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    \n",
    "    # to be sure\n",
    "    features = features.reset_index(drop=True)\n",
    "    targets = targets.reset_index(drop=True)\n",
    "\n",
    "    param_combinations = list(itertools.product(*param_grid.values()))\n",
    "    results = {}\n",
    "    for i, param_combination in enumerate(param_combinations,1):\n",
    "        \n",
    "        # create dictionary with all parameters\n",
    "        param_combination = {k:v for k,v in zip(param_grid.keys(), param_combination)}\n",
    "        param_combination.update(params)\n",
    "                \n",
    "        # init dict\n",
    "        results[f\"combination_{i}\"] = {\n",
    "            'params': param_combination,\n",
    "            'res': []\n",
    "        }\n",
    "        \n",
    "        # perform n_folds\n",
    "        for j in range(n_folds):\n",
    "            \n",
    "            # kfold\n",
    "            features_train, features_validation, targets_train, targets_validation =\\\n",
    "                train_test_split(features, targets, train_size = .8, shuffle=True)#, random_state=42)\n",
    "\n",
    "            # train lgb model        \n",
    "            temp_dict = {} # this dict object will be used to add all (intermediate) evaluation scores during the training process\n",
    "            mod: lgb.Booster = lgb.train(param_combination, \n",
    "                train_set = lgb.Dataset(features_train, targets_train),\n",
    "                valid_sets = lgb.Dataset(features_validation, targets_validation),\n",
    "                evals_result = temp_dict,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            \n",
    "            # store results\n",
    "            results[f\"combination_{i}\"]['res']\\\n",
    "                .append(temp_dict[\"valid_0\"][\"quantile\"][-1],\n",
    "                )\n",
    "\n",
    "        # compute average results\n",
    "        results[f\"combination_{i}\"]['validation_score'] = \\\n",
    "            np.mean(results[f\"combination_{i}\"]['res'])\n",
    "        \n",
    "    # sort the results based on evaluation score\n",
    "    sorted_results = dict(sorted(results.items(), key=lambda item: item[1][\"validation_score\"]))\n",
    "    return list(sorted_results.values())[0], results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData:\n",
    "    \"\"\" Class to load data \"\"\"\n",
    "    def __init__(self):\n",
    "        self.level = None\n",
    "        \n",
    "    def prep_data(self,level, sub_d_start):\n",
    "        \"\"\" read the precomputed features and targets for specified aggregation level,  \"\"\"\n",
    "        # define params\n",
    "        agg_level = level\n",
    "        # sub_d_start: int = int(1886)\n",
    "        exclude_columns = []\n",
    "        test = False\n",
    "        type_of = 'val'\n",
    "\n",
    "        # read file\n",
    "        agg_columns = AGG_LEVEL_COLUMNS[agg_level]\n",
    "        if len(agg_columns) == 0:\n",
    "            agg_str: str = 'Total_X'\n",
    "        elif len(agg_columns) == 1:\n",
    "            agg_str: str = f'{agg_columns[0]}_X'\n",
    "        else:\n",
    "            agg_str: str = '_'.join(agg_columns)\n",
    "\n",
    "        if self.level == level:\n",
    "            pass\n",
    "        else:\n",
    "            logger.info('(re)loading features')\n",
    "            features = pd.read_parquet(f'../data/uncertainty/fold_{sub_d_start}/features/' + (TEST_PATH if test else '') + f'features_{type_of}_{agg_str}.parquet')\n",
    "            features = _down_cast(features)\n",
    "\n",
    "        group_columns = agg_columns\n",
    "        exclude_prefix_list = exclude_columns # unconditional, auto, momentum, seasonal\n",
    "        \n",
    "        features_gr = features.copy()\n",
    "        features_gr = features_gr[[c for c in features_gr if c.split('_')[0] not in exclude_prefix_list]]\n",
    "\n",
    "        # preparations\n",
    "        train_idx = features_gr['sold'].notna() & features_gr['d'].isin([f'd_{sub_d_start - 1 - i}' for i in range(1460)])\n",
    "        df_train = features_gr[train_idx]\n",
    "        features_train: pd.DataFrame = df_train.drop(DROP_FEATURE_COLUMNS, axis = 1, errors = 'ignore')\n",
    "        targets_train: pd.Series = df_train['sold']\n",
    "        return features_train, targets_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Training a Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 18:12:39 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['state_id',\n",
       " 'auto_sold_2',\n",
       " 'auto_sold_1',\n",
       " 'auto_sold_56',\n",
       " 'auto_sold_14',\n",
       " 'auto_sold_7',\n",
       " 'auto_sold_28',\n",
       " 'auto_sold_qtile_112_0.25',\n",
       " 'auto_sold_qtile_168_0.25',\n",
       " 'auto_sold_ma_7',\n",
       " 'auto_sold_qtile_7_0.75',\n",
       " 'auto_sold_qtile_56_0.75',\n",
       " 'auto_sold_qtile_168_0.1',\n",
       " 'auto_sold_qtile_21_0.25',\n",
       " 'auto_sold_ma_14',\n",
       " 'auto_sold_qtile_14_0.5',\n",
       " 'auto_sold_qtile_14_0.1',\n",
       " 'auto_sold_qtile_112_0.5',\n",
       " 'auto_sold_qtile_112_0.99',\n",
       " 'auto_sold_qtile_168_0.9',\n",
       " 'auto_sold_qtile_56_0.9',\n",
       " 'auto_sold_qtile_21_0.5',\n",
       " 'auto_sold_ma_28',\n",
       " 'auto_sold_qtile_28_0.25',\n",
       " 'auto_sold_ewm_168',\n",
       " 'auto_sold_std_7',\n",
       " 'auto_sold_ewm_28',\n",
       " 'auto_sold_std_28',\n",
       " 'auto_sold_qtile_28_0.75',\n",
       " 'auto_sold_qtile_56_0.1',\n",
       " 'auto_sold_qtile_168_0.5',\n",
       " 'auto_sold_qtile_28_0.9',\n",
       " 'auto_sold_ma_112',\n",
       " 'auto_sold_std_112',\n",
       " 'auto_sold_std_168',\n",
       " 'auto_sold_qtile_21_0.9',\n",
       " 'auto_sold_qtile_168_0.99',\n",
       " 'auto_sold_ma_21',\n",
       " 'auto_sold_qtile_28_0.5',\n",
       " 'auto_sold_qtile_112_0.9',\n",
       " 'auto_sold_qtile_168_0.01',\n",
       " 'auto_sold_qtile_14_0.9',\n",
       " 'auto_sold_qtile_7_0.25',\n",
       " 'auto_sold_qtile_168_0.75',\n",
       " 'auto_sold_qtile_21_0.75',\n",
       " 'auto_sold_qtile_14_0.25',\n",
       " 'auto_sold_ewm_14',\n",
       " 'auto_sold_std_56',\n",
       " 'auto_sold_ewm_7',\n",
       " 'auto_sold_qtile_56_0.25',\n",
       " 'auto_sold_ewm_3',\n",
       " 'auto_sold_ma_168',\n",
       " 'auto_sold_std_14',\n",
       " 'auto_sold_qtile_28_0.1',\n",
       " 'auto_sold_ewm_112',\n",
       " 'auto_sold_qtile_112_0.1',\n",
       " 'auto_sold_ewm_56',\n",
       " 'auto_sold_qtile_21_0.1',\n",
       " 'auto_sold_qtile_56_0.5',\n",
       " 'auto_sold_std_3',\n",
       " 'auto_sold_qtile_7_0.5',\n",
       " 'auto_sold_qtile_14_0.75',\n",
       " 'auto_sold_std_21',\n",
       " 'auto_sold_qtile_112_0.75',\n",
       " 'auto_sold_qtile_112_0.01',\n",
       " 'auto_sold_ewm_21',\n",
       " 'auto_sold_ma_56',\n",
       " 'auto_sold_qtile_3_0.5',\n",
       " 'auto_sold_ma_3',\n",
       " 'price_uncond_median',\n",
       " 'price_uncond_std',\n",
       " 'price_uncond_avg',\n",
       " 'price_momentum_w',\n",
       " 'price_momentum_m',\n",
       " 'price_momentum_y',\n",
       " 'price_auto_std_112',\n",
       " 'price_auto_std_28',\n",
       " 'price_auto_std_56',\n",
       " 'seasonal_weekday_Friday',\n",
       " 'seasonal_weekday_Sunday',\n",
       " 'seasonal_weekday_Wednesday',\n",
       " 'seasonal_weekday_Tuesday',\n",
       " 'seasonal_weekday_Saturday',\n",
       " 'seasonal_weekday_Monday',\n",
       " 'seasonal_weekday_Thursday',\n",
       " 'seasonal_month_12',\n",
       " 'seasonal_month_5',\n",
       " 'seasonal_month_1',\n",
       " 'seasonal_month_6',\n",
       " 'seasonal_month_7',\n",
       " 'seasonal_month_9',\n",
       " 'seasonal_month_4',\n",
       " 'seasonal_month_10',\n",
       " 'seasonal_month_2',\n",
       " 'seasonal_month_8',\n",
       " 'seasonal_month_11',\n",
       " 'seasonal_month_3',\n",
       " 'seasonal_monthday_6',\n",
       " 'seasonal_monthday_20',\n",
       " 'seasonal_monthday_7',\n",
       " 'seasonal_monthday_11',\n",
       " 'seasonal_monthday_3',\n",
       " 'seasonal_monthday_28',\n",
       " 'seasonal_monthday_21',\n",
       " 'seasonal_monthday_26',\n",
       " 'seasonal_monthday_12',\n",
       " 'seasonal_monthday_29',\n",
       " 'seasonal_monthday_1',\n",
       " 'seasonal_monthday_19',\n",
       " 'seasonal_monthday_24',\n",
       " 'seasonal_monthday_17',\n",
       " 'seasonal_monthday_23',\n",
       " 'seasonal_monthday_30',\n",
       " 'seasonal_monthday_16',\n",
       " 'seasonal_monthday_25',\n",
       " 'seasonal_monthday_2',\n",
       " 'seasonal_monthday_31',\n",
       " 'seasonal_monthday_4',\n",
       " 'seasonal_monthday_8',\n",
       " 'seasonal_monthday_10',\n",
       " 'seasonal_monthday_18',\n",
       " 'seasonal_monthday_15',\n",
       " 'seasonal_monthday_14',\n",
       " 'seasonal_monthday_9',\n",
       " 'seasonal_monthday_22',\n",
       " 'seasonal_monthday_5',\n",
       " 'seasonal_monthday_13',\n",
       " 'seasonal_monthday_27',\n",
       " 'state_CA',\n",
       " 'state_WI',\n",
       " 'state_TX',\n",
       " 'days_fwd']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data example, to investigate which features are computed, among other things\n",
    "level = 'Level2'\n",
    "dataLoader = LoadData()\n",
    "features, targets = dataLoader.prep_data(level, 1914)\n",
    "list(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold\n",
    "prefixes = ['seasonal', 'auto_sold_ewm']\n",
    "features_train, features_validation, targets_train, targets_validation =\\\n",
    "    train_test_split(features, targets, test_size = 28, shuffle=False, random_state=42)\n",
    "    # train_test_split(features[[c for c in features.columns if prefixes_in_column(c, prefixes)]], targets, train_size = .8, shuffle=False, random_state=42)\n",
    "\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -100,\n",
    "    'n_jobs': 4,\n",
    "    # 'subsample': .9,\n",
    "    # 'subsample_freq': 1,\n",
    "    \"num_leaves\": 30,\n",
    "    \"min_child_weight\": .1,\n",
    "    \"min_child_samples\": 4,\n",
    "    \"hist_pool_size\": 1000,\n",
    "    'feature_fraction': 0.9, #.5\n",
    "    # 'bagging_fraction': .8,\n",
    "    \"learning_rate\": 0.005, #0.07,\n",
    "    \"n_estimators\": 2000,#100\n",
    "    \"max_depth\": 10,\n",
    "    # 'reg_sqrt': True,\n",
    "    # 'req_lambda': .00001,\n",
    "    # 'reg_alpha': .00001,\n",
    "    'alpha': .25,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# train lgb model       \n",
    "for q in [0.005, 0.025, 0.135, 0.25, 0.5, 0.75, 0.865, 0.975, 0.995]:\n",
    "    params['alpha'] = q \n",
    "    temp_dict = {}\n",
    "    mod: lgb.Booster = lgb.train(params, \n",
    "        train_set = lgb.Dataset(features_train, targets_train),\n",
    "        valid_sets = lgb.Dataset(features_validation, targets_validation),\n",
    "        evals_result = temp_dict,\n",
    "        verbose_eval = False\n",
    "    )\n",
    "    plt.plot(mod.predict(features_validation), label = f'{q}')\n",
    "\n",
    "plt.scatter(range(len(targets_validation.index)), targets_validation, label = 'true', s = 10)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total ~280 seconds\n",
    "lgb_quantile_params = {     # fairly well tuned, with high runtimes \n",
    "    'max_depth': [10, 20],\n",
    "    'n_estimators': [ 150, 200, 200],  # 300, 350, 400, ],   \n",
    "    'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n",
    "    'min_child_samples': [ 2, 4, 7, 10, 14, 20, 30, 40, 60, 80, 100, 100, 100, \n",
    "                                        130, 170, 200, 300, 500, 700, 1000 ],\n",
    "    'min_child_weight': [0, 0, 0, 0, 1e-4, 1e-3, 1e-3, 1e-3, 5e-3, 2e-2, 0.1 ],\n",
    "    'num_leaves': [ 20, 30, 50, 50 ], # 50, 70, 90, ],\n",
    "    'learning_rate': [  0.04, 0.05, 0.07, 0.07, 0.07, 0.1, 0.1, 0.1 ],   # 0.02, 0.03,        \n",
    "    'colsample_bytree': [0.3, 0.5, 0.7, 0.8, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "    'colsample_bynode':[0.1, 0.15, 0.2, 0.2, 0.2, 0.25, 0.3, 0.5, 0.65, 0.8, 0.9, 1],\n",
    "    'reg_lambda': [0, 0, 0, 0, 1e-5, 1e-5, 1e-5, 1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100   ],\n",
    "    'reg_alpha': [0, 1e-5, 3e-5, 1e-4, 1e-4, 1e-3, 3e-3, 1e-2, 0.1, 1, 1, 10, 10, 100, 1000,],\n",
    "    'subsample': [  0.9, 1],\n",
    "    'subsample_freq': [1],\n",
    "    'cat_smooth': [0.1, 0.2, 0.5, 1, 2, 5, 7, 10],\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': 4,\n",
    "    # 'eval_at': 10,\n",
    "    'hist_pool_size': 1000,\n",
    "    # 'verbose_eval': 0\n",
    "    # 'subsample': 0.5,\n",
    "    # 'subsample_freq': 1,\n",
    "    # 'feature_fraction': 0.5,\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10],\n",
    "    'n_estimators': [200, 800],\n",
    "    # 'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n",
    "    'min_child_samples': [4],\n",
    "    'min_child_weight': [0.1 ],\n",
    "    'num_leaves': [30], # 50, 70, 90, ],\n",
    "    'learning_rate': [0.001, 0.005, 0.01 ],   # 0.02, 0.03,        \n",
    "    # 'colsample_bytree': [0.3, 0.5, 0.7, 0.8, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "    # 'colsample_bynode':[0.1, 0.15, 0.2, 0.2, 0.2, 0.25, 0.3, 0.5, 0.65, 0.8, 0.9, 1],\n",
    "    # 'reg_lambda': [0, 1e-5, 1e-5, 1e-4, 1e-2, 1, 10, ],\n",
    "    # 'reg_alpha': [0, 1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 1, 10, 100, 1000,],\n",
    "    'subsample': [  0.9, 1],\n",
    "    'subsample_freq': [1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test grid search for all quantiles\n",
    "# for q in QUANTILES[:5]:\n",
    "    \n",
    "#     # of course, update quantile in params\n",
    "#     params['alpha'] = q\n",
    "#     best_res, res = grid_search(params, param_grid, features_train, targets_train, 1)\n",
    "#     logger.info(best_res['params'])\n",
    "    \n",
    "#     mod = lgb.train(best_res['params'],\n",
    "#         train_set = lgb.Dataset(features_train, targets_train)\n",
    "#     )\n",
    "#     predictions = mod.predict(features_validation)\n",
    "#     plt.plot(predictions, label = str(q))\n",
    "\n",
    "# plt.scatter(range(len(targets_validation)), targets_validation)\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train + Predict submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_level_all_quantiles(\n",
    "    agg_level: str, \n",
    "    type_of: str, \n",
    "    sub_d_start: int, \n",
    "    exclude_columns: list = [], \n",
    "    include_columns: list = None,\n",
    "    test: bool = False, \n",
    "    do_grid_search: bool = False, \n",
    "    store_submissions_path: str = 'temp_submissions/', \n",
    "    normalize: bool = False,\n",
    "):\n",
    "    \"\"\" \n",
    "    Train, for a specific aggregation level, models for all quantiles.\n",
    "    For aggregation levels 10, 11 and 12, undersampling is used to drastically reduce training time.\n",
    "    \"\"\"\n",
    "    ALWAYS_KEEP_COLUMNS = ['days_fwd', 'sold', 'd']\n",
    "    \n",
    "    agg_columns = AGG_LEVEL_COLUMNS[agg_level]\n",
    "    if len(agg_columns) == 0:\n",
    "        agg_str: str = 'Total_X'\n",
    "    elif len(agg_columns) == 1:\n",
    "        agg_str: str = f'{agg_columns[0]}_X'\n",
    "    else:\n",
    "        agg_str: str = '_'.join(agg_columns)\n",
    "\n",
    "    # try:\n",
    "    #     features = pd.DataFrame(features)\n",
    "    # except Exception:\n",
    "    # loading features\n",
    "    logger.info('(re)loading features')\n",
    "    features = pd.read_parquet(f'../data/uncertainty/fold_{sub_d_start}/features/' + (TEST_PATH if test else '') + f'features_{type_of}_{agg_str}.parquet')\n",
    "    features = _down_cast(features)\n",
    "    features_gr = features.copy()\n",
    "    \n",
    "    # preparations\n",
    "    # sub_d_start = SUB_D_START_VAL if type_of == 'val' else SUB_D_START_EVAL\n",
    "    train_idx = features_gr['sold'].notna() & features_gr['d'].isin([f'd_{sub_d_start - 1 - i}' for i in range(1300)])\n",
    "    pred_idx = features_gr['d'].isin([f'd_{sub_d_start + i}' for i in range(DAYS)])\n",
    "\n",
    "    group_columns = agg_columns\n",
    "    res: list = []\n",
    "    \n",
    "    def check_any_prefix_matches(column, prefixes):\n",
    "        \"\"\" Return true if any prefix is in column \"\"\"\n",
    "        # print(column, prefixes, prefixes[0] in column)\n",
    "        for prefix in prefixes:\n",
    "            if prefix in column:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # select features\n",
    "    if USE_ALL or 'kbest' in include_columns:\n",
    "        columns = features_gr.columns\n",
    "    elif SPARSE_FEATURES:\n",
    "        columns = [c for c in features_gr.columns if c in SPARSE_FEATURES]\n",
    "    elif include_columns == None:\n",
    "        # exclude features from exclusion prefix list\n",
    "        exclude_prefix_list = exclude_columns \n",
    "        # columns = [c for c in features_gr.columns if c.split('_')[0] not in exclude_prefix_list]\n",
    "        columns = [c for c in features_gr.columns if not check_any_prefix_matches(c, exclude_prefix_list)]\n",
    "    elif isinstance(include_columns, list):\n",
    "        include_prefix_list = include_columns\n",
    "        # columns = [c for c in features_gr.columns if c.split('_')[0] in include_prefix_list]\n",
    "        columns = [c for c in features_gr.columns if check_any_prefix_matches(c, include_prefix_list)]\n",
    "\n",
    "    for column in ALWAYS_KEEP_COLUMNS + group_columns:\n",
    "        if column not in columns:\n",
    "            columns.append(column) \n",
    "\n",
    "    features_gr = features_gr[columns]\n",
    "    df_pred = features_gr[pred_idx]\n",
    "    df_train = features_gr[train_idx]\n",
    "    if agg_level not in ['Level9', 'Level10', 'Level11', 'Level12']:\n",
    "        df_train = df_train[df_train['sold'] >= 50]\n",
    "\n",
    "    from copy import deepcopy\n",
    "    temp_drop_feature_columns = deepcopy(DROP_FEATURE_COLUMNS)\n",
    "    if not USE_ALL and 'kbest' not in include_columns:\n",
    "        if 'state_id' in include_prefix_list:\n",
    "            temp_drop_feature_columns.remove('state_id')\n",
    "        if 'store_id' in include_prefix_list:\n",
    "            temp_drop_feature_columns.remove('store_id')\n",
    "    if USE_ALL or 'kbest' in include_columns:\n",
    "        temp_drop_feature_columns.remove('state_id')\n",
    "        temp_drop_feature_columns.remove('store_id')\n",
    "        \n",
    "    features_train: pd.DataFrame = df_train.drop(temp_drop_feature_columns, axis = 1, errors = 'ignore')\n",
    "    # logger.info(f'feature: {str(features_train.columns)}')\n",
    "    targets_train: pd.Series = df_train['sold']\n",
    "    features_predict: pd.DataFrame = df_pred.drop(temp_drop_feature_columns, axis = 1, errors = 'ignore')\n",
    "    targets_test: pd.Series = df_pred['sold']\n",
    "    \n",
    "    #### SELECT FEATURES ####\n",
    "    if 'kbest' in include_columns:\n",
    "        # cannot do selectkbest for category variables\n",
    "        exclude_from_kbest = ['state_id', 'store_id', 'seasonal_weekday', 'seasonal_monthday', 'seasonal_month', 'days_fwd']\n",
    "        # temp_drop_idx = features_train.notna().all(axis=1)\n",
    "        temp_drop_idx = features_train.drop(exclude_from_kbest, axis=1, errors='ignore').fillna(0).notna().all(axis=1)\n",
    "        from sklearn import metrics\n",
    "        from sklearn import feature_selection\n",
    "        fit = SelectKBest(\n",
    "                k=9,\n",
    "                # score_func=metrics.mean_pinball_loss\n",
    "                score_func=feature_selection.f_regression\n",
    "            ).fit(\n",
    "                features_train.drop(exclude_from_kbest, axis=1, errors='ignore').fillna(0)[temp_drop_idx], \n",
    "                targets_train[temp_drop_idx]\n",
    "            )\n",
    "        print(fit.get_feature_names_out())\n",
    "        features_keep = list(fit.get_feature_names_out())\n",
    "        for c in exclude_from_kbest:\n",
    "            if c in features_train.columns:\n",
    "                features_keep.append(c)\n",
    "        features_train = features_train[features_keep]\n",
    "        features_predict = features_predict[features_keep]\n",
    "    #### SELECT FEATURES ####\n",
    "    \n",
    "    # undersample data\n",
    "    if agg_level in undersampling_dict.keys() and HIGH_UNDERSAMPLING:\n",
    "        undersampling_pct = undersampling_dict[agg_level]\n",
    "        features_train, _, targets_train, _ = train_test_split(features_train, targets_train, train_size = undersampling_pct, shuffle=True, random_state=43)\n",
    "\n",
    "    # normalise targets\n",
    "    if normalize:\n",
    "        logger.info('scaling targets')\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        targets_train = scaler.fit_transform(targets_train.values.reshape(-1,1))\n",
    "        \n",
    "    # REMOVE THIS\n",
    "    import matplotlib.pyplot as plt\n",
    "    if PLOT_PREDICTIONS:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (10,5))\n",
    "        aaa = [i for i in range(targets_test.shape[0])]\n",
    "    # REMOVE THIS\n",
    "        \n",
    "    # train model for all quantiles\n",
    "    for quantile in QUANTILES:\n",
    "        \n",
    "        # perform grid search for best parameters\n",
    "        if do_grid_search == True:\n",
    "            # split data to training and testing\n",
    "            # logger.info('divide for cross validation')\n",
    "            # x_train, x_test, y_train, y_test = train_test_split(features_train, targets_train, train_size=.8, shuffle=False, random_state=42)\n",
    "            # train_data = lgb.Dataset(x_train, y_train)\n",
    "            # validation_data = lgb.Dataset(x_test, y_test)\n",
    "            logger.info('perform gridsearch')\n",
    "            params['alpha'] = quantile\n",
    "            best_combination, results = grid_search(params, param_grid, features_train, targets_train, 1)\n",
    "            # del train_data; del validation_data\n",
    "            params_grid_train = best_combination[\"params\"]\n",
    "            logger.info(f'q: {quantile} - cv best params: {params_grid_train}')\n",
    "        else:\n",
    "            params_grid_train = PARAM_GRID_TRAIN\n",
    "            params_grid_train['alpha'] = quantile\n",
    "\n",
    "        # train_best_model\n",
    "        # logger.info(f'features: {str(features_train.columns)}')\n",
    "        mod = lgb.train(params_grid_train,\n",
    "            train_set = lgb.Dataset(features_train, targets_train)\n",
    "        )\n",
    "        # save model\n",
    "        group_names = '_'.join(group_columns)\n",
    "        if group_names == '':\n",
    "            group_names = 'Total_X'\n",
    "        if USE_ALL:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_use_all_q={quantile}.joblib'\n",
    "        elif 'kbest' in include_columns:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_include_k_best_q={quantile}.joblib'\n",
    "        elif SPARSE_FEATURES:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_sparse_q={quantile}.joblib' \n",
    "        elif include_columns == None:\n",
    "            exclude_names = 'None' if len(exclude_prefix_list) == 0 else '_'.join(exclude_prefix_list)\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_exclude_{exclude_names}_q={quantile}.joblib'\n",
    "        elif isinstance(include_columns, list):\n",
    "            exclude_names = 'None' if len(include_prefix_list) == 0 else '_'.join(include_prefix_list)\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_include_{exclude_names}_q={quantile}.joblib'\n",
    "\n",
    "        import joblib\n",
    "        joblib.dump(mod, file_path)\n",
    "        \n",
    "        predictions = mod.predict(features_predict)\n",
    "        if normalize:\n",
    "            predictions = scaler.inverse_transform(predictions.reshape(-1,1)).reshape(-1,)\n",
    "        \n",
    "        # REMOVE THIS\n",
    "        if PLOT_PREDICTIONS:\n",
    "            ax.plot(aaa, predictions, label = f'{quantile}')\n",
    "        # lgb.plot_importance(mod)\n",
    "        # REMOVE THIS\n",
    "        \n",
    "        # store predictions\n",
    "        df_p = pd.DataFrame(\n",
    "            {\n",
    "                'pred': predictions,\n",
    "                'd': df_pred['d'],\n",
    "            }\n",
    "        )\n",
    "        df_p['quantile'] = quantile\n",
    "        df_p['Level'] = agg_level\n",
    "        df_p['type_of'] = 'validation' if type_of == 'val' else 'evaluation'\n",
    "        if len(agg_columns) == 0:\n",
    "            df_p['agg_column1'] = 'Total'\n",
    "            df_p['agg_column2'] = 'X'\n",
    "        elif len(agg_columns) == 1:\n",
    "            df_p['agg_column1'] = df_pred[agg_columns[0]].values\n",
    "            df_p['agg_column2'] = 'X'\n",
    "        else:\n",
    "            df_p['agg_column1'] = df_pred[agg_columns[0]].values\n",
    "            df_p['agg_column2'] = df_pred[agg_columns[1]].values\n",
    "            \n",
    "        df_p = df_p[['Level', 'agg_column1', 'agg_column2', 'd', 'quantile', 'pred', 'type_of']]\n",
    "        \n",
    "        res.append(_down_cast(df_p))\n",
    "        \n",
    "    # REMOVE THIS\n",
    "    if PLOT_PREDICTIONS:\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "        plt.show()\n",
    "    # REMOVE THIS\n",
    "        \n",
    "    # remove to reduce memory usage asap\n",
    "    del features\n",
    "        \n",
    "    # storing predictions in specified file + folder\n",
    "    df_sub_val = pd.concat(res)\n",
    "    group_names = '_'.join(group_columns)\n",
    "    if group_names == '':\n",
    "        group_names = 'Total_X'\n",
    "        \n",
    "    if USE_ALL:\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_use_all.csv'\n",
    "    elif 'kbest' in include_columns:\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_include_k_best.csv'\n",
    "    elif SPARSE_FEATURES:\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_sparse.csv'  \n",
    "    elif include_columns == None:\n",
    "        exclude_names = 'None' if len(exclude_prefix_list) == 0 else '_'.join(exclude_prefix_list)\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_exclude_{exclude_names}.csv'\n",
    "    elif isinstance(include_columns, list):\n",
    "        exclude_names = 'None' if len(include_prefix_list) == 0 else '_'.join(include_prefix_list)\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_include_{exclude_names}.csv'\n",
    "\n",
    "    df_sub_val.to_csv(file_path, index = False)\n",
    "    logger.info('saved under: ' + file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:53:05 - __main__ - INFO - starting with all EXCLUDE_COLUMNS\n",
      "2023-12-10 18:53:05 - __main__ - INFO - finished all EXCLUDE_COLUMNS\n",
      "2023-12-10 18:53:05 - __main__ - INFO - ---------------------------------\n",
      "2023-12-10 18:53:05 - __main__ - INFO - starting with all INCLUDE_COLUMNS\n",
      "2023-12-10 18:53:05 - __main__ - INFO - Include columns: ['seasonal', 'auto_sold_std_3', 'auto_sold_std_56', 'auto_sold_std_168', 'auto_sold_ma_7', 'auto_sold_ma_28', 'auto_sold_ma_56', 'auto_sold_qtile_28_0.25', 'auto_sold_qtile_168_0.25', 'auto_sold_qtile_56_0.1', 'state_id', 'store_id']\n",
      "2023-12-10 18:53:05 - __main__ - INFO - starting with agg_level: Level1\n",
      "2023-12-10 18:53:05 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['auto_sold_ma_7' 'auto_sold_qtile_28_0.9' 'auto_sold_std_112'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:53:17 - __main__ - INFO - saved under: ../data/uncertainty/fold_1802/temp_submissions/lgb_val_nt_Total_X_include_k_best.csv\n",
      "2023-12-10 18:53:17 - __main__ - INFO - starting with agg_level: Level2\n",
      "2023-12-10 18:53:17 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:53:31 - __main__ - INFO - saved under: ../data/uncertainty/fold_1802/temp_submissions/lgb_val_nt_state_id_include_k_best.csv\n",
      "2023-12-10 18:53:31 - __main__ - INFO - starting with agg_level: Level3\n",
      "2023-12-10 18:53:31 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:53:49 - __main__ - INFO - saved under: ../data/uncertainty/fold_1802/temp_submissions/lgb_val_nt_store_id_include_k_best.csv\n",
      "2023-12-10 18:53:49 - __main__ - INFO - starting with agg_level: Level4\n",
      "2023-12-10 18:53:49 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:54:03 - __main__ - INFO - saved under: ../data/uncertainty/fold_1802/temp_submissions/lgb_val_nt_cat_id_include_k_best.csv\n",
      "2023-12-10 18:54:03 - __main__ - INFO - starting with agg_level: Level5\n",
      "2023-12-10 18:54:03 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:54:19 - __main__ - INFO - saved under: ../data/uncertainty/fold_1802/temp_submissions/lgb_val_nt_dept_id_include_k_best.csv\n",
      "2023-12-10 18:54:19 - __main__ - INFO - starting with agg_level: Level6\n",
      "2023-12-10 18:54:19 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:54:37 - __main__ - INFO - saved under: ../data/uncertainty/fold_1802/temp_submissions/lgb_val_nt_state_id_cat_id_include_k_best.csv\n",
      "2023-12-10 18:54:37 - __main__ - INFO - starting with agg_level: Level7\n",
      "2023-12-10 18:54:37 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:54:59 - __main__ - INFO - saved under: ../data/uncertainty/fold_1802/temp_submissions/lgb_val_nt_state_id_dept_id_include_k_best.csv\n",
      "2023-12-10 18:54:59 - __main__ - INFO - starting with agg_level: Level8\n",
      "2023-12-10 18:54:59 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:55:29 - __main__ - INFO - saved under: ../data/uncertainty/fold_1802/temp_submissions/lgb_val_nt_store_id_cat_id_include_k_best.csv\n",
      "2023-12-10 18:55:29 - __main__ - INFO - starting with agg_level: Level9\n",
      "2023-12-10 18:55:29 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:56:13 - __main__ - INFO - saved under: ../data/uncertainty/fold_1802/temp_submissions/lgb_val_nt_store_id_dept_id_include_k_best.csv\n",
      "2023-12-10 18:56:13 - __main__ - INFO - starting with agg_level: Level1\n",
      "2023-12-10 18:56:13 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['auto_sold_ma_7' 'auto_sold_qtile_28_0.9' 'auto_sold_std_112'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:56:25 - __main__ - INFO - saved under: ../data/uncertainty/fold_1830/temp_submissions/lgb_val_nt_Total_X_include_k_best.csv\n",
      "2023-12-10 18:56:25 - __main__ - INFO - starting with agg_level: Level2\n",
      "2023-12-10 18:56:25 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:56:41 - __main__ - INFO - saved under: ../data/uncertainty/fold_1830/temp_submissions/lgb_val_nt_state_id_include_k_best.csv\n",
      "2023-12-10 18:56:41 - __main__ - INFO - starting with agg_level: Level3\n",
      "2023-12-10 18:56:41 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:57:00 - __main__ - INFO - saved under: ../data/uncertainty/fold_1830/temp_submissions/lgb_val_nt_store_id_include_k_best.csv\n",
      "2023-12-10 18:57:00 - __main__ - INFO - starting with agg_level: Level4\n",
      "2023-12-10 18:57:00 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:57:12 - __main__ - INFO - saved under: ../data/uncertainty/fold_1830/temp_submissions/lgb_val_nt_cat_id_include_k_best.csv\n",
      "2023-12-10 18:57:12 - __main__ - INFO - starting with agg_level: Level5\n",
      "2023-12-10 18:57:12 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:57:27 - __main__ - INFO - saved under: ../data/uncertainty/fold_1830/temp_submissions/lgb_val_nt_dept_id_include_k_best.csv\n",
      "2023-12-10 18:57:27 - __main__ - INFO - starting with agg_level: Level6\n",
      "2023-12-10 18:57:27 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:57:42 - __main__ - INFO - saved under: ../data/uncertainty/fold_1830/temp_submissions/lgb_val_nt_state_id_cat_id_include_k_best.csv\n",
      "2023-12-10 18:57:42 - __main__ - INFO - starting with agg_level: Level7\n",
      "2023-12-10 18:57:42 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:58:03 - __main__ - INFO - saved under: ../data/uncertainty/fold_1830/temp_submissions/lgb_val_nt_state_id_dept_id_include_k_best.csv\n",
      "2023-12-10 18:58:03 - __main__ - INFO - starting with agg_level: Level8\n",
      "2023-12-10 18:58:03 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:58:26 - __main__ - INFO - saved under: ../data/uncertainty/fold_1830/temp_submissions/lgb_val_nt_store_id_cat_id_include_k_best.csv\n",
      "2023-12-10 18:58:26 - __main__ - INFO - starting with agg_level: Level9\n",
      "2023-12-10 18:58:26 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:59:00 - __main__ - INFO - saved under: ../data/uncertainty/fold_1830/temp_submissions/lgb_val_nt_store_id_dept_id_include_k_best.csv\n",
      "2023-12-10 18:59:00 - __main__ - INFO - starting with agg_level: Level1\n",
      "2023-12-10 18:59:00 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['auto_sold_ma_7' 'auto_sold_qtile_28_0.9' 'auto_sold_std_112'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:59:11 - __main__ - INFO - saved under: ../data/uncertainty/fold_1858/temp_submissions/lgb_val_nt_Total_X_include_k_best.csv\n",
      "2023-12-10 18:59:11 - __main__ - INFO - starting with agg_level: Level2\n",
      "2023-12-10 18:59:11 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:59:24 - __main__ - INFO - saved under: ../data/uncertainty/fold_1858/temp_submissions/lgb_val_nt_state_id_include_k_best.csv\n",
      "2023-12-10 18:59:24 - __main__ - INFO - starting with agg_level: Level3\n",
      "2023-12-10 18:59:24 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:59:41 - __main__ - INFO - saved under: ../data/uncertainty/fold_1858/temp_submissions/lgb_val_nt_store_id_include_k_best.csv\n",
      "2023-12-10 18:59:41 - __main__ - INFO - starting with agg_level: Level4\n",
      "2023-12-10 18:59:41 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:59:53 - __main__ - INFO - saved under: ../data/uncertainty/fold_1858/temp_submissions/lgb_val_nt_cat_id_include_k_best.csv\n",
      "2023-12-10 18:59:53 - __main__ - INFO - starting with agg_level: Level5\n",
      "2023-12-10 18:59:53 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:00:08 - __main__ - INFO - saved under: ../data/uncertainty/fold_1858/temp_submissions/lgb_val_nt_dept_id_include_k_best.csv\n",
      "2023-12-10 19:00:08 - __main__ - INFO - starting with agg_level: Level6\n",
      "2023-12-10 19:00:08 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:00:23 - __main__ - INFO - saved under: ../data/uncertainty/fold_1858/temp_submissions/lgb_val_nt_state_id_cat_id_include_k_best.csv\n",
      "2023-12-10 19:00:23 - __main__ - INFO - starting with agg_level: Level7\n",
      "2023-12-10 19:00:23 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:00:45 - __main__ - INFO - saved under: ../data/uncertainty/fold_1858/temp_submissions/lgb_val_nt_state_id_dept_id_include_k_best.csv\n",
      "2023-12-10 19:00:45 - __main__ - INFO - starting with agg_level: Level8\n",
      "2023-12-10 19:00:45 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:01:07 - __main__ - INFO - saved under: ../data/uncertainty/fold_1858/temp_submissions/lgb_val_nt_store_id_cat_id_include_k_best.csv\n",
      "2023-12-10 19:01:07 - __main__ - INFO - starting with agg_level: Level9\n",
      "2023-12-10 19:01:07 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:01:42 - __main__ - INFO - saved under: ../data/uncertainty/fold_1858/temp_submissions/lgb_val_nt_store_id_dept_id_include_k_best.csv\n",
      "2023-12-10 19:01:42 - __main__ - INFO - starting with agg_level: Level1\n",
      "2023-12-10 19:01:42 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['auto_sold_ma_7' 'auto_sold_qtile_28_0.9' 'auto_sold_std_112'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:01:52 - __main__ - INFO - saved under: ../data/uncertainty/fold_1886/temp_submissions/lgb_val_nt_Total_X_include_k_best.csv\n",
      "2023-12-10 19:01:52 - __main__ - INFO - starting with agg_level: Level2\n",
      "2023-12-10 19:01:52 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:02:04 - __main__ - INFO - saved under: ../data/uncertainty/fold_1886/temp_submissions/lgb_val_nt_state_id_include_k_best.csv\n",
      "2023-12-10 19:02:04 - __main__ - INFO - starting with agg_level: Level3\n",
      "2023-12-10 19:02:04 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:02:21 - __main__ - INFO - saved under: ../data/uncertainty/fold_1886/temp_submissions/lgb_val_nt_store_id_include_k_best.csv\n",
      "2023-12-10 19:02:21 - __main__ - INFO - starting with agg_level: Level4\n",
      "2023-12-10 19:02:21 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:02:33 - __main__ - INFO - saved under: ../data/uncertainty/fold_1886/temp_submissions/lgb_val_nt_cat_id_include_k_best.csv\n",
      "2023-12-10 19:02:33 - __main__ - INFO - starting with agg_level: Level5\n",
      "2023-12-10 19:02:33 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:02:48 - __main__ - INFO - saved under: ../data/uncertainty/fold_1886/temp_submissions/lgb_val_nt_dept_id_include_k_best.csv\n",
      "2023-12-10 19:02:48 - __main__ - INFO - starting with agg_level: Level6\n",
      "2023-12-10 19:02:48 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:03:03 - __main__ - INFO - saved under: ../data/uncertainty/fold_1886/temp_submissions/lgb_val_nt_state_id_cat_id_include_k_best.csv\n",
      "2023-12-10 19:03:03 - __main__ - INFO - starting with agg_level: Level7\n",
      "2023-12-10 19:03:03 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:03:23 - __main__ - INFO - saved under: ../data/uncertainty/fold_1886/temp_submissions/lgb_val_nt_state_id_dept_id_include_k_best.csv\n",
      "2023-12-10 19:03:23 - __main__ - INFO - starting with agg_level: Level8\n",
      "2023-12-10 19:03:23 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:03:46 - __main__ - INFO - saved under: ../data/uncertainty/fold_1886/temp_submissions/lgb_val_nt_store_id_cat_id_include_k_best.csv\n",
      "2023-12-10 19:03:46 - __main__ - INFO - starting with agg_level: Level9\n",
      "2023-12-10 19:03:46 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:04:20 - __main__ - INFO - saved under: ../data/uncertainty/fold_1886/temp_submissions/lgb_val_nt_store_id_dept_id_include_k_best.csv\n",
      "2023-12-10 19:04:20 - __main__ - INFO - starting with agg_level: Level1\n",
      "2023-12-10 19:04:20 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['auto_sold_ma_7' 'auto_sold_qtile_28_0.9' 'auto_sold_std_112'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:04:30 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_Total_X_include_k_best.csv\n",
      "2023-12-10 19:04:30 - __main__ - INFO - starting with agg_level: Level2\n",
      "2023-12-10 19:04:30 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:04:43 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_state_id_include_k_best.csv\n",
      "2023-12-10 19:04:43 - __main__ - INFO - starting with agg_level: Level3\n",
      "2023-12-10 19:04:43 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:05:00 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_store_id_include_k_best.csv\n",
      "2023-12-10 19:05:00 - __main__ - INFO - starting with agg_level: Level4\n",
      "2023-12-10 19:05:00 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:05:12 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_cat_id_include_k_best.csv\n",
      "2023-12-10 19:05:12 - __main__ - INFO - starting with agg_level: Level5\n",
      "2023-12-10 19:05:12 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:05:26 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_dept_id_include_k_best.csv\n",
      "2023-12-10 19:05:26 - __main__ - INFO - starting with agg_level: Level6\n",
      "2023-12-10 19:05:26 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:05:42 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_state_id_cat_id_include_k_best.csv\n",
      "2023-12-10 19:05:42 - __main__ - INFO - starting with agg_level: Level7\n",
      "2023-12-10 19:05:42 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:06:02 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_state_id_dept_id_include_k_best.csv\n",
      "2023-12-10 19:06:02 - __main__ - INFO - starting with agg_level: Level8\n",
      "2023-12-10 19:06:02 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:06:25 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_store_id_cat_id_include_k_best.csv\n",
      "2023-12-10 19:06:25 - __main__ - INFO - starting with agg_level: Level9\n",
      "2023-12-10 19:06:25 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price_uncond_std' 'price_uncond_median' 'price_uncond_avg'\n",
      " 'price_momentum_w' 'price_momentum_m' 'price_momentum_y'\n",
      " 'price_auto_std_28' 'price_auto_std_56' 'price_auto_std_112']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:06:58 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_store_id_dept_id_include_k_best.csv\n",
      "2023-12-10 19:06:58 - __main__ - INFO - finished all INCLUDE_COLUMNS\n"
     ]
    }
   ],
   "source": [
    "# all groups: seasonal, auto, autoquantiles, momentum\n",
    "SPARSE_FEATURES = [\n",
    "    'auto_sold_qtile_28_0.5', \n",
    "    'auto_sold_ma_28', \n",
    "    'auto_sold_ma_168', \n",
    "    'auto_sold_qtile_168_0.1',\n",
    "    'auto_sold_qtile_168_0.9',\n",
    "]\n",
    "SPARSE_FEATURES += [f'seasonal_weekday_{i}' for i in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']]\n",
    "SPARSE_FEATURES += [f'seasonal_monthday_{i}' for i in range(1,32)]\n",
    "SPARSE_FEATURES += [f'seasonal_month_{i}' for i in range(1,13)]\n",
    "SPARSE_FEATURES += ['seasonal_weekday', 'seasonal_monthday', 'seasonal_month']\n",
    "# SPARSE_FEATURES += [f'state_{i}' for i in ['CA', 'TX', 'WI']]\n",
    "# SPARSE_FEATURES = [ 'seasonal_weekday', 'seasonal_monthday', 'auto_sold_ewm_112', 'auto_sold_ewm_28',\n",
    "#   'autoquantiles_sold_qtile_28_0.5', 'auto_sold_ma_28',\n",
    "#   'auto_sold_qtile_28_0.9',\n",
    "#    'auto_sold_qtile_28_0.1'\n",
    "# ],\n",
    "\n",
    "USE_ALL = False\n",
    "SPARSE_FEATURES = None\n",
    "PLOT_PREDICTIONS = False\n",
    "\n",
    "undersampling_dict = {\n",
    "    # 'Level5': .15,\n",
    "    # 'Level6': .15,\n",
    "    # 'Level7': .15,\n",
    "    # 'Level8': .15,\n",
    "    # 'Level9': .1,\n",
    "    'Level10': .1, #.001\n",
    "    'Level11': .3, #.0001\n",
    "    'Level12': .1 #.00001\n",
    "}\n",
    "\n",
    "HIGH_UNDERSAMPLING = True\n",
    "TEST_NUMBER = 9\n",
    "TEST_NUMB = 0\n",
    "PARAM_GRID_TRAIN = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': 4,\n",
    "    \"num_leaves\": 30,\n",
    "    \"hist_pool_size\": 300,\n",
    "    # 'feature_fraction': 0.9, #.5\n",
    "    # 'bagging_fraction': .8,\n",
    "    \"learning_rate\": .01, # .01\n",
    "    \"n_estimators\": 1000, #1000\n",
    "    \"max_depth\": 10,\n",
    "}\n",
    "PARAM_GRID_TRAIN_HIGH_LEVEL = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': 4,\n",
    "    \"num_leaves\": 30,\n",
    "    \"hist_pool_size\": 300,\n",
    "    # 'feature_fraction': 0.9, #.5\n",
    "    # 'bagging_fraction': .8,\n",
    "    \"learning_rate\": .01, # .07\n",
    "    \"n_estimators\": 3000,#100\n",
    "    \"max_depth\": 10,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -100,\n",
    "    'n_jobs': 4,\n",
    "    # 'eval_at': 10,\n",
    "    'hist_pool_size': 1000,\n",
    "    'verbose_eval': -100,\n",
    "}\n",
    "param_grid = {\n",
    "    'max_depth': [10,],\n",
    "    'n_estimators': [200, 500, 1000, 2000, 4000],\n",
    "    # 'min_child_samples': [4],\n",
    "    # 'min_child_weight': [0,0.1],\n",
    "    'num_leaves': [30], # 50, 70, 90, ],\n",
    "    'learning_rate': [.001, .005, .01, .02],#[0.04, 0.07, 0.1],   # 0.02, 0.03,        \n",
    "    # 'subsample': [ 0.9, 1 ],\n",
    "    # 'subsample_freq': [1],\n",
    "}\n",
    "\n",
    "ALL_PREFIXES = ['auto_sold', 'auto_sold_ma', 'auto_sold_std', 'auto_sold_ewm', 'auto_sold_qtile',\n",
    "    'price_momentum', 'price_uncond', 'price_auto_std','seasonal_', 'state_', 'store_',\n",
    "]\n",
    "\n",
    "EXCLUDE_COLUMNS_LIST = ()\n",
    "\n",
    "# DESELECT WHICH EXPERIMENT TO PERFORM\n",
    "# LATER ON I'LL ADD A DICTIONARY FOR EASY SWITCHING BETWEEN EXPERIMENTS\n",
    "\n",
    "# EXPERIMENT: SEASONALITY FEATURES >>>>>>>>>>>>>>> DONE\n",
    "# BASE = []\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_ewm'],\n",
    "#     ['seasonal_weekday','auto_sold_ewm'],\n",
    "#     ['seasonal_monthday','auto_sold_ewm'],\n",
    "#     ['seasonal_weekday','seasonal_monthday','auto_sold_ewm'],\n",
    "#     ['seasonal','auto_sold_ewm'],\n",
    "# ]\n",
    "\n",
    "# # EXPERIMENT: COMPARE STORE_ID VS STATE_ID >>>>>>>>>>>>>>> DONE\n",
    "# BASE = ['seasonal', 'auto_sold_ma']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     [],\n",
    "#     ['state_id',],\n",
    "#     ['store_id',],\n",
    "#     ['state_id', 'store_id']\n",
    "# ]\n",
    "\n",
    "# # EXPERIMENT: AUTO_SOLD FEATURES >>>>>>>>>>>>>>> DONE\n",
    "# BASE = ['seasonal']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_ewm'],\n",
    "#     ['auto_sold_ma'],\n",
    "#     ['auto_sold_ewm', 'auto_sold_ma'],\n",
    "# ]\n",
    "\n",
    "# EXPERIMENT: QUANTILE VS. STD FEATURES >>>>>>>>>>>>>>> DONE\n",
    "# BASE = ['seasonal', 'auto_sold_ma']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_qtile'],\n",
    "#     ['auto_sold_std'],\n",
    "#     ['auto_sold_qtile','auto_sold_std'],\n",
    "# ]\n",
    "\n",
    "# # EXPERIMENT: PRICE MOMENTUM / PRICE AUTO >>>>>>>>>>>>>>> DONE\n",
    "# BASE = ['seasonal', 'auto_sold_ma']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['price_auto_std'],\n",
    "#     ['price_momentum'],\n",
    "#     ['price_uncond'],\n",
    "#     ['price_auto_std', 'price_momentum'],\n",
    "#     ['price_auto_std', 'price_momentum', 'price_uncond']\n",
    "# ]\n",
    "\n",
    "# # EXPERIMENT: BEST MODEL / ALL IMPORTANT FEATURES >>>>>>>>>>>>>>> DONE\n",
    "# BASE = ['seasonal']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_ma', 'state_id', 'store_id']\n",
    "#     ['auto_sold_ma', 'auto_sold_std', 'state_id', 'store_id'],\n",
    "# ]\n",
    "\n",
    "# EXPERIMENT: SPARSE MA VS. FULL MA\n",
    "BASE = ['seasonal']\n",
    "INCLUDE_COLUMNS_LIST = [\n",
    "    ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm', 'state_id', 'store_id'],\n",
    "    ['auto_sold_ma_28', 'auto_sold_ma_56', 'auto_sold_ma_168', 'state_id', 'store_id']\n",
    "]\n",
    "\n",
    "# EXPERIMENT: SPARSE VS. FULL\n",
    "BASE = ['seasonal',]\n",
    "INCLUDE_COLUMNS_LIST = [\n",
    "    # ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm', 'state_id', 'store_id'],\n",
    "    ['auto_sold_std_3', 'auto_sold_std_56', 'auto_sold_std_168', \n",
    "     'auto_sold_ma_7',  'auto_sold_ma_28', 'auto_sold_ma_56', \n",
    "     'auto_sold_qtile_28_0.25', 'auto_sold_qtile_168_0.25', 'auto_sold_qtile_56_0.1', \n",
    "     'state_id', 'store_id'],\n",
    "    # ['kbest']\n",
    "]\n",
    "\n",
    "# # EXPERIMENT: SPARSE VS. FULL\n",
    "# BASE = ['seasonal']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm'],\n",
    "#     [ 'auto_sold_ewm_112', 'auto_sold_ewm_28',\n",
    "#       'auto_sold_qtile_28_0.5', 'auto_sold_ma_28',\n",
    "#       'auto_sold_qtile_28_0.9', 'auto_sold_qtile_28_0.1'\n",
    "#     ],\n",
    "# ]\n",
    "\n",
    "INCLUDE_COLUMNS_LIST = [BASE + i for i in INCLUDE_COLUMNS_LIST]\n",
    "DO_GRID_SEARCH = False\n",
    "\n",
    "logger.info('starting with all EXCLUDE_COLUMNS')\n",
    "for exclude_columns in EXCLUDE_COLUMNS_LIST: # for each specified feature combination\n",
    "    logger.info(f'Exclude columns: {str(exclude_columns)}')\n",
    "    # for sub_d_start in D_CROSS_VAL_START_LIST: # for each fold\n",
    "    for sub_d_start in D_CROSS_VAL_START_LIST:#[:1 if TEST_NUMBER>0 else 100]:\n",
    "        for agg_level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]: # for each aggregation level\n",
    "            logger.info(f'starting with agg_level: {agg_level}')\n",
    "            train_level_all_quantiles(\n",
    "                agg_level,\n",
    "                sub_d_start=sub_d_start,\n",
    "                type_of='val', \n",
    "                exclude_columns=exclude_columns,\n",
    "                do_grid_search=DO_GRID_SEARCH,\n",
    "                store_submissions_path='temp_submissions/',\n",
    "            )\n",
    "logger.info('finished all EXCLUDE_COLUMNS')\n",
    "\n",
    "logger.info('---------------------------------')            \n",
    "logger.info('starting with all INCLUDE_COLUMNS')            \n",
    "for include_columns in INCLUDE_COLUMNS_LIST: # for each specified feature combination\n",
    "    logger.info(f'Include columns: {str(include_columns)}')\n",
    "    for sub_d_start in D_CROSS_VAL_START_LIST:\n",
    "        for agg_level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]: # for each aggregation level\n",
    "            logger.info(f'starting with agg_level: {agg_level}')\n",
    "            train_level_all_quantiles(\n",
    "                agg_level,\n",
    "                sub_d_start=sub_d_start,\n",
    "                type_of='val', \n",
    "                exclude_columns=None,\n",
    "                include_columns=include_columns,\n",
    "                do_grid_search=DO_GRID_SEARCH,\n",
    "                store_submissions_path='temp_submissions/',\n",
    "            )\n",
    "logger.info('finished all INCLUDE_COLUMNS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load val + eval prediction files and merge to one submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_columns = '_'.join([])\n",
    "# if exclude_columns == '':\n",
    "#     exclude_columns = 'None'\n",
    "\n",
    "# dfs: list = []\n",
    "# for level in AGG_LEVEL_COLUMNS:\n",
    "#     agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "#     group_names = '_'.join(agg_columns)\n",
    "#     if group_names == '':\n",
    "#         group_names = 'Total_X'\n",
    "#     i = str(1802)\n",
    "#     dfs.append(\n",
    "#         f'../data/uncertainty/fold_{i}/temp_submissions/' + f'lgb_multivariate_val_non_transposed_{group_names}_exclude_{exclude_columns}.csv',\n",
    "#     )\n",
    "\n",
    "# df_sub_val = ensemble_submissions_uncertainty(dfs)\n",
    "# transpose = True\n",
    "# if transpose == True:\n",
    "#     sub_validation = df_sub_val.pivot(index='id', columns='d', values='pred').reset_index(drop=False)\n",
    "#     sub_validation.columns = [\"id\"] + [f\"F{i}\" for i in range(1,DAYS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_concat_predictions(fold_name: int, exclude_columns: list = [], include_columns: list = [], sparse = False, use_all = False, load_submissions_path: str = 'temp_submissions/'):\n",
    "    \"\"\" \n",
    "    For specified fold, read the predictions for all aggregation levels and stack them together in one dataframe.\n",
    "    \"\"\"\n",
    "    # D_CV_START_LIST\n",
    "    # if fold_name not in D_CV_START_LIST:\n",
    "        # raise ValueError('fold_name must be a value in D_CV_START_LIST')\n",
    "        \n",
    "    exclude_columns = '_'.join(exclude_columns)\n",
    "    if exclude_columns == '':\n",
    "        exclude_columns = 'None'\n",
    "\n",
    "    logger.info('loading files under path:' + f'../data/uncertainty/fold_{fold_name}/' + load_submissions_path)\n",
    "\n",
    "    dfs: list = []\n",
    "    for level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]:\n",
    "        agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "        group_names = '_'.join(agg_columns)\n",
    "        if group_names == '':\n",
    "            group_names = 'Total_X'\n",
    "        \n",
    "        file_path = f'../data/uncertainty/fold_{str(fold_name)}/' + load_submissions_path \n",
    "        file_path += f'lgb_val_nt_{group_names}_'\n",
    "        if use_all:\n",
    "            file_path += f'use_all.csv'  \n",
    "        elif include_columns == None:\n",
    "            file_path += f'exclude_{\"_\".join(exclude_columns)}.csv'            \n",
    "        elif isinstance(include_columns, list):\n",
    "            file_path += f'include_{\"_\".join(include_columns)}.csv'\n",
    "        \n",
    "        dfs.append(file_path)\n",
    "    return ensemble_submissions_uncertainty(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_columns = '_'.join([])\n",
    "# if exclude_columns == '':\n",
    "#     exclude_columns = 'None'\n",
    "\n",
    "# dfs: list = []\n",
    "# for level in AGG_LEVEL_COLUMNS:\n",
    "#     agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "#     group_names = '_'.join(agg_columns)\n",
    "#     if group_names == '':\n",
    "#         group_names = 'Total_X'\n",
    "        \n",
    "#     dfs.append(\n",
    "#         PREDICTION_BASE_PATH + f'lgb_multivariate_eval_non_transposed_{group_names}_exclude_{exclude_columns}.csv',\n",
    "#     )\n",
    "\n",
    "# df_sub_eval = ensemble_submissions_uncertainty(dfs)\n",
    "# transpose = True\n",
    "# if transpose == True:\n",
    "#     sub_evaluation = df_sub_eval.pivot(index='id', columns='d', values='pred').reset_index(drop=False)\n",
    "#     sub_evaluation.columns = [\"id\"] + [f\"F{i}\" for i in range(1,DAYS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sub_evaluation = pd.read_csv('../submissions/submission_baseline_evaluation.csv').drop(['Unnamed: 0'], axis=1)\n",
    "# pd.concat([sub_validation, sub_evaluation]).to_csv(SUBMISSION_BASE_PATH + f'submission_lgb_ensemble{exclude_columns}.csv', index=False)\n",
    "# del sub_validation; del sub_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation Prediction, we can compute WRMSSE locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these variables are used later on\n",
    "FORCE_RELOAD = False\n",
    "try:\n",
    "    # simple code to check if variable exists\n",
    "    d_int + 1\n",
    "    if FORCE_RELOAD:\n",
    "        raise Exception()\n",
    "except:\n",
    "    # if not, load again\n",
    "    # takes about 2-3 minutes to reload and parse\n",
    "    d = pd.read_parquet('../data/uncertainty/cv_template/temp.parquet')\n",
    "    d_int = pd.read_parquet('../data/uncertainty/cv_template/temp_d_int.parquet')['d_int']\n",
    "    # d_int = d['d'].str.split('_').apply(lambda x: int(x[1]))\n",
    "    # d_int.to_frame('d_int').to_parquet('../data/uncertainty/cv_template/temp_d_int.parquet', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cv(df: pd.DataFrame, df_sub: pd.DataFrame):\n",
    "    \n",
    "    # to be able to merge\n",
    "    df_sub['id_merge'] = df_sub['id'].str.split('.')\\\n",
    "        .apply(lambda x: x[0])\n",
    "    df_sub['quantile'] = df_sub['id'].str.split('.')\\\n",
    "        .apply(lambda x: float('.'.join([x[-2], x[-1].split('_')[0]])))\n",
    "\n",
    "    # merge predictions in cv template\n",
    "    p = pd.merge(\n",
    "        df,\n",
    "        df_sub,\n",
    "        how='left',\n",
    "        on=['id_merge', 'd']\n",
    "    )\n",
    "    # del df; del df_sub_val\n",
    "    p['id_merge'] = p['id_merge'].astype(str)\n",
    "\n",
    "    for c in ['sold', 'revenue']:\n",
    "        p[c] = p[c].astype(np.float32)\n",
    "    # d = d[d_int < (D_CV_START + 28)]\n",
    "\n",
    "    return WSPL(p, [f'd_{i}' for i in range(D_CV_START, D_CV_START + 500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 19:06:59 - __main__ - INFO - start evaluating exclude columns\n",
      "2023-12-10 19:06:59 - __main__ - INFO - start evaluating include columns\n",
      "2023-12-10 19:06:59 - __main__ - INFO - --------------- ['k_best'] ---------------\n",
      "2023-12-10 19:07:02 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1802/temp_submissions/\n",
      "2023-12-10 19:07:24 - utils.metrics - INFO - reading weights file\n",
      "2023-12-10 19:07:24 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-10 19:07:52 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-10 19:07:58 - utils.metrics - INFO - Level1 - 0.19791906409995028\n",
      "2023-12-10 19:07:59 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:08:03 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:08:20 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:08:21 - utils.metrics - INFO - Level2 - 0.23093397792358752\n",
      "2023-12-10 19:08:21 - utils.metrics - INFO - Level3 - 0.25515156818126256\n",
      "2023-12-10 19:08:21 - utils.metrics - INFO - Level4 - 0.3793385803203012\n",
      "2023-12-10 19:08:21 - utils.metrics - INFO - Level5 - 0.449225879837354\n",
      "2023-12-10 19:08:21 - utils.metrics - INFO - Level6 - 0.23140186349115438\n",
      "2023-12-10 19:08:21 - utils.metrics - INFO - Level7 - 0.270817404877664\n",
      "2023-12-10 19:08:21 - utils.metrics - INFO - Level8 - 0.22325648497428507\n",
      "2023-12-10 19:08:22 - utils.metrics - INFO - Level9 - 0.2857980580473939\n",
      "2023-12-10 19:08:23 - __main__ - INFO - 1802 - wspl: 0.28042698686143924\n",
      "2023-12-10 19:08:27 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1830/temp_submissions/\n",
      "2023-12-10 19:08:45 - utils.metrics - INFO - reading weights file\n",
      "2023-12-10 19:08:46 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-10 19:09:11 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-10 19:09:16 - utils.metrics - INFO - Level1 - 0.19325950798217306\n",
      "2023-12-10 19:09:18 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:09:22 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:09:39 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:09:40 - utils.metrics - INFO - Level2 - 0.23109535644469195\n",
      "2023-12-10 19:09:40 - utils.metrics - INFO - Level3 - 0.2965782360537755\n",
      "2023-12-10 19:09:40 - utils.metrics - INFO - Level4 - 0.3115579769508974\n",
      "2023-12-10 19:09:40 - utils.metrics - INFO - Level5 - 0.6182682491613661\n",
      "2023-12-10 19:09:40 - utils.metrics - INFO - Level6 - 0.2443757913649675\n",
      "2023-12-10 19:09:40 - utils.metrics - INFO - Level7 - 0.27369159822492\n",
      "2023-12-10 19:09:40 - utils.metrics - INFO - Level8 - 0.2440491406634519\n",
      "2023-12-10 19:09:40 - utils.metrics - INFO - Level9 - 0.32779641914394575\n",
      "2023-12-10 19:09:42 - __main__ - INFO - 1830 - wspl: 0.30451914177668765\n",
      "2023-12-10 19:09:46 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1858/temp_submissions/\n",
      "2023-12-10 19:10:05 - utils.metrics - INFO - reading weights file\n",
      "2023-12-10 19:10:05 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-10 19:10:31 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-10 19:10:36 - utils.metrics - INFO - Level1 - 0.12383007440370752\n",
      "2023-12-10 19:10:37 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:10:42 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:10:59 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:11:00 - utils.metrics - INFO - Level2 - 0.2368466824715423\n",
      "2023-12-10 19:11:00 - utils.metrics - INFO - Level3 - 0.2923884671889886\n",
      "2023-12-10 19:11:00 - utils.metrics - INFO - Level4 - 0.22870539011510224\n",
      "2023-12-10 19:11:00 - utils.metrics - INFO - Level5 - 0.25475815075490166\n",
      "2023-12-10 19:11:00 - utils.metrics - INFO - Level6 - 0.22880594585627803\n",
      "2023-12-10 19:11:00 - utils.metrics - INFO - Level7 - 0.26615235562234324\n",
      "2023-12-10 19:11:00 - utils.metrics - INFO - Level8 - 0.24633209383804924\n",
      "2023-12-10 19:11:01 - utils.metrics - INFO - Level9 - 0.2992331976366823\n",
      "2023-12-10 19:11:02 - __main__ - INFO - 1858 - wspl: 0.24189470643195504\n",
      "2023-12-10 19:11:06 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1886/temp_submissions/\n",
      "2023-12-10 19:11:25 - utils.metrics - INFO - reading weights file\n",
      "2023-12-10 19:11:25 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-10 19:11:52 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-10 19:11:57 - utils.metrics - INFO - Level1 - 0.13183358690511604\n",
      "2023-12-10 19:11:59 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:12:03 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:12:21 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:12:22 - utils.metrics - INFO - Level2 - 0.21488225072963624\n",
      "2023-12-10 19:12:22 - utils.metrics - INFO - Level3 - 0.27499686693837844\n",
      "2023-12-10 19:12:22 - utils.metrics - INFO - Level4 - 0.1436688942839731\n",
      "2023-12-10 19:12:22 - utils.metrics - INFO - Level5 - 0.257758499640582\n",
      "2023-12-10 19:12:22 - utils.metrics - INFO - Level6 - 0.18009606791479313\n",
      "2023-12-10 19:12:22 - utils.metrics - INFO - Level7 - 0.2475946087056493\n",
      "2023-12-10 19:12:22 - utils.metrics - INFO - Level8 - 0.2284360712836697\n",
      "2023-12-10 19:12:22 - utils.metrics - INFO - Level9 - 0.29635496548276286\n",
      "2023-12-10 19:12:24 - __main__ - INFO - 1886 - wspl: 0.21951353465384008\n",
      "2023-12-10 19:12:28 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1914/temp_submissions/\n",
      "2023-12-10 19:12:47 - utils.metrics - INFO - reading weights file\n",
      "2023-12-10 19:12:47 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-10 19:13:14 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-10 19:13:20 - utils.metrics - INFO - Level1 - 0.30066256234296695\n",
      "2023-12-10 19:13:21 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:13:27 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:13:46 - utils.metrics - ERROR - Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "2023-12-10 19:13:46 - utils.metrics - INFO - Level2 - 0.3864353492667945\n",
      "2023-12-10 19:13:46 - utils.metrics - INFO - Level3 - 0.3323749732954694\n",
      "2023-12-10 19:13:46 - utils.metrics - INFO - Level4 - 0.23446515574593224\n",
      "2023-12-10 19:13:46 - utils.metrics - INFO - Level5 - 0.32251590197070945\n",
      "2023-12-10 19:13:46 - utils.metrics - INFO - Level6 - 0.2248833160743541\n",
      "2023-12-10 19:13:47 - utils.metrics - INFO - Level7 - 0.35768250732932594\n",
      "2023-12-10 19:13:47 - utils.metrics - INFO - Level8 - 0.27822528252136375\n",
      "2023-12-10 19:13:47 - utils.metrics - INFO - Level9 - 0.3459089427997574\n",
      "2023-12-10 19:13:49 - __main__ - INFO - 1914 - wspl: 0.3092393323718527\n",
      "2023-12-10 19:13:49 - __main__ - INFO - 1914 - mean wspl: 0.27111874041915496 +/- 0.03513559795439422\n",
      "2023-12-10 19:13:49 - __main__ - INFO - 1914 - raw results: [0.28042698686143924, 0.30451914177668765, 0.24189470643195504, 0.21951353465384008, 0.3092393323718527]\n"
     ]
    }
   ],
   "source": [
    "FILE_NAME_ALL_RESULTS = '../data/uncertainty/all_results.json'\n",
    "USE_ALL = False\n",
    "sparse_features = ['dayofweek', 'dayofmonth', \n",
    "                     'qs_30d_ewm', 'qs_100d_ewm',\n",
    "                    'qs_median_28d', 'qs_mean_28d',# 'qs_stdev_28d',\n",
    "                    'state_id',\n",
    "               #     'store_id',\n",
    "                   'qs_qtile90_28d',\n",
    "                    'pct_nonzero_days_28d',\n",
    "                    'days_fwd'\n",
    "                    ]\n",
    "FOLDER = 'temp_submissions/'\n",
    "\n",
    "# load dict to store results in\n",
    "from utils.utils import load_results_as_json\n",
    "results = load_results_as_json(FILE_NAME_ALL_RESULTS)\n",
    "\n",
    "logger.info('start evaluating exclude columns')\n",
    "for EXCLUDE_COLUMNS in EXCLUDE_COLUMNS_LIST:\n",
    "    \n",
    "    results['exclude_' + ' '.join(EXCLUDE_COLUMNS)] = {}\n",
    "    \n",
    "    logger.info('--------------- ' + str(EXCLUDE_COLUMNS) + ' ---------------')\n",
    "    res = []\n",
    "    for D_CV_START in D_CROSS_VAL_START_LIST:#[:1 if TEST_NUMBER>0 else 10]:\n",
    "\n",
    "        mean_wspl, res_dict = perform_cv(\n",
    "            _down_cast(d)[d_int < (D_CV_START + DAYS)], \n",
    "            read_concat_predictions(\n",
    "                fold_name = D_CV_START, \n",
    "                exclude_columns = EXCLUDE_COLUMNS, \n",
    "                include_columns = None,\n",
    "                use_all=USE_ALL,\n",
    "                load_submissions_path=FOLDER\n",
    "            )\n",
    "        )\n",
    "        res.append(mean_wspl)\n",
    "        results['exclude_' + ' '.join(EXCLUDE_COLUMNS)]['fold_' + str(D_CV_START)] = res_dict \n",
    "        logger.info(str(D_CV_START) + ' - wspl: ' + str(mean_wspl))\n",
    "\n",
    "    logger.info(' - mean wspl: ' + str(np.mean(res)) + ' +/- ' + str(np.std(res)))\n",
    "    logger.info(str(D_CV_START) + ' - raw results: ' + str(res))\n",
    "    \n",
    "logger.info('start evaluating include columns')\n",
    "# for INCLUDE_COLUMNS in [['k_best']]:\n",
    "for INCLUDE_COLUMNS in INCLUDE_COLUMNS_LIST:\n",
    "    results['include_' + ' '.join(INCLUDE_COLUMNS)] = {}\n",
    "    \n",
    "    logger.info('--------------- ' + str(INCLUDE_COLUMNS) + ' ---------------')\n",
    "    res = []\n",
    "    for D_CV_START in D_CROSS_VAL_START_LIST:#[:1 if TEST_NUMBER>0 else 10]:\n",
    "        \n",
    "        mean_wspl, res_dict = perform_cv(\n",
    "            _down_cast(d)[d_int < (D_CV_START + DAYS)], \n",
    "            read_concat_predictions(\n",
    "                fold_name = D_CV_START, \n",
    "                exclude_columns = [], \n",
    "                include_columns = INCLUDE_COLUMNS,\n",
    "                use_all=USE_ALL,\n",
    "                load_submissions_path=FOLDER\n",
    "            )\n",
    "        )\n",
    "        res.append(mean_wspl)\n",
    "\n",
    "        results['include_' + ' '.join(INCLUDE_COLUMNS)]['fold_' + str(D_CV_START)] = res_dict \n",
    "        logger.info(str(D_CV_START) + ' - wspl: ' + str(mean_wspl))\n",
    "\n",
    "    logger.info(str(D_CV_START) + ' - mean wspl: ' + str(np.mean(res)) + ' +/- ' + str(np.std(res)))\n",
    "    logger.info(str(D_CV_START) + ' - raw results: ' + str(res))\n",
    "\n",
    "from utils.utils import store_results_as_json\n",
    "store_results_as_json(results, FILE_NAME_ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Beneath can be used to create submission template\n",
    "The submission template can be used to quickly insert your predictions.\n",
    "It also contains all other (historical) sales to be able to compute the WRMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_validation = pd.read_csv(DATA_BASE_PATH + SALES_VALIDATION)\n",
    "# sales_evaluation = pd.read_csv(DATA_BASE_PATH + SALES_EVALUATION)\n",
    "# calendar = pd.read_csv(DATA_BASE_PATH + CALENDAR)\n",
    "# sell_prices = pd.read_csv(DATA_BASE_PATH + SELL_PRICES)\n",
    "\n",
    "# df_val, submission_idx_val = data_preprocessing(sales_validation, calendar, sell_prices)\n",
    "# del sales_validation\n",
    "# df_eval, submission_idx_eval = data_preprocessing(sales_evaluation, calendar, sell_prices)\n",
    "# del sales_evaluation\n",
    "\n",
    "# df_val_after_release = df_val[(df_val.wm_yr_wk > df_val.release)]# & (df_val[\"sold\"].notna())]\n",
    "# del df_val\n",
    "# df_eval_after_release = df_eval[(df_eval.wm_yr_wk > df_eval.release)]# & (df_eval[\"sold\"].notna())]\n",
    "# del df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 17:27:39 - __main__ - INFO - Level1\n",
      "2023-08-17 17:27:40 - __main__ - INFO - Level2\n",
      "2023-08-17 17:27:44 - __main__ - INFO - Level3\n",
      "2023-08-17 17:27:48 - __main__ - INFO - Level4\n",
      "2023-08-17 17:27:51 - __main__ - INFO - Level5\n",
      "2023-08-17 17:27:55 - __main__ - INFO - Level6\n",
      "2023-08-17 17:28:00 - __main__ - INFO - Level7\n",
      "2023-08-17 17:28:05 - __main__ - INFO - Level8\n",
      "2023-08-17 17:28:10 - __main__ - INFO - Level9\n",
      "2023-08-17 17:28:15 - __main__ - INFO - Level10\n",
      "2023-08-17 17:28:21 - __main__ - INFO - Level11\n",
      "2023-08-17 17:28:33 - __main__ - INFO - Level12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>agg_column1</th>\n",
       "      <th>agg_column2</th>\n",
       "      <th>d</th>\n",
       "      <th>sold</th>\n",
       "      <th>revenue</th>\n",
       "      <th>id_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_10</td>\n",
       "      <td>24858.0</td>\n",
       "      <td>63029.78</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_100</td>\n",
       "      <td>23653.0</td>\n",
       "      <td>65665.71</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1000</td>\n",
       "      <td>29241.0</td>\n",
       "      <td>82351.45</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1001</td>\n",
       "      <td>33804.0</td>\n",
       "      <td>93975.55</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1002</td>\n",
       "      <td>42447.0</td>\n",
       "      <td>118961.96</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1003</td>\n",
       "      <td>40647.0</td>\n",
       "      <td>116052.48</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1004</td>\n",
       "      <td>32039.0</td>\n",
       "      <td>89314.17</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1005</td>\n",
       "      <td>29501.0</td>\n",
       "      <td>81688.96</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1006</td>\n",
       "      <td>31117.0</td>\n",
       "      <td>85754.15</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1007</td>\n",
       "      <td>27018.0</td>\n",
       "      <td>74244.86</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1008</td>\n",
       "      <td>39707.0</td>\n",
       "      <td>108637.04</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1009</td>\n",
       "      <td>47082.0</td>\n",
       "      <td>128940.24</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_101</td>\n",
       "      <td>24982.0</td>\n",
       "      <td>68908.04</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1010</td>\n",
       "      <td>48360.0</td>\n",
       "      <td>133218.73</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1011</td>\n",
       "      <td>32930.0</td>\n",
       "      <td>92274.15</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1012</td>\n",
       "      <td>33990.0</td>\n",
       "      <td>92743.98</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1013</td>\n",
       "      <td>32956.0</td>\n",
       "      <td>90505.80</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1014</td>\n",
       "      <td>31862.0</td>\n",
       "      <td>87172.76</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1015</td>\n",
       "      <td>35365.0</td>\n",
       "      <td>95702.83</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1016</td>\n",
       "      <td>45705.0</td>\n",
       "      <td>125791.89</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1017</td>\n",
       "      <td>43898.0</td>\n",
       "      <td>123256.45</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1018</td>\n",
       "      <td>36385.0</td>\n",
       "      <td>100212.69</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1019</td>\n",
       "      <td>32258.0</td>\n",
       "      <td>87909.01</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_102</td>\n",
       "      <td>22196.0</td>\n",
       "      <td>60000.65</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1020</td>\n",
       "      <td>29242.0</td>\n",
       "      <td>81367.81</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1021</td>\n",
       "      <td>29452.0</td>\n",
       "      <td>79956.86</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1022</td>\n",
       "      <td>35763.0</td>\n",
       "      <td>97645.15</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1023</td>\n",
       "      <td>44579.0</td>\n",
       "      <td>123721.42</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1024</td>\n",
       "      <td>42582.0</td>\n",
       "      <td>121478.81</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1025</td>\n",
       "      <td>32102.0</td>\n",
       "      <td>89627.71</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1026</td>\n",
       "      <td>28521.0</td>\n",
       "      <td>78796.16</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1027</td>\n",
       "      <td>27904.0</td>\n",
       "      <td>78144.16</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1028</td>\n",
       "      <td>28693.0</td>\n",
       "      <td>78581.27</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1029</td>\n",
       "      <td>32847.0</td>\n",
       "      <td>91347.72</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_103</td>\n",
       "      <td>22117.0</td>\n",
       "      <td>61407.00</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1030</td>\n",
       "      <td>40046.0</td>\n",
       "      <td>114533.56</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1031</td>\n",
       "      <td>38445.0</td>\n",
       "      <td>111826.12</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1032</td>\n",
       "      <td>28603.0</td>\n",
       "      <td>81092.24</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1033</td>\n",
       "      <td>31247.0</td>\n",
       "      <td>87799.42</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1034</td>\n",
       "      <td>36053.0</td>\n",
       "      <td>97214.80</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1035</td>\n",
       "      <td>19783.0</td>\n",
       "      <td>53456.88</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1036</td>\n",
       "      <td>26041.0</td>\n",
       "      <td>75086.93</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1037</td>\n",
       "      <td>31539.0</td>\n",
       "      <td>91356.80</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1038</td>\n",
       "      <td>38182.0</td>\n",
       "      <td>108919.39</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1039</td>\n",
       "      <td>37079.0</td>\n",
       "      <td>97503.03</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_104</td>\n",
       "      <td>22347.0</td>\n",
       "      <td>60736.91</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1040</td>\n",
       "      <td>38010.0</td>\n",
       "      <td>100557.02</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1041</td>\n",
       "      <td>31513.0</td>\n",
       "      <td>83895.82</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1042</td>\n",
       "      <td>35139.0</td>\n",
       "      <td>93359.95</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Level1</td>\n",
       "      <td>Total</td>\n",
       "      <td>X</td>\n",
       "      <td>d_1043</td>\n",
       "      <td>36894.0</td>\n",
       "      <td>99430.98</td>\n",
       "      <td>Total_X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Level agg_column1 agg_column2       d     sold    revenue id_merge\n",
       "0   Level1       Total           X    d_10  24858.0   63029.78  Total_X\n",
       "1   Level1       Total           X   d_100  23653.0   65665.71  Total_X\n",
       "2   Level1       Total           X  d_1000  29241.0   82351.45  Total_X\n",
       "3   Level1       Total           X  d_1001  33804.0   93975.55  Total_X\n",
       "4   Level1       Total           X  d_1002  42447.0  118961.96  Total_X\n",
       "5   Level1       Total           X  d_1003  40647.0  116052.48  Total_X\n",
       "6   Level1       Total           X  d_1004  32039.0   89314.17  Total_X\n",
       "7   Level1       Total           X  d_1005  29501.0   81688.96  Total_X\n",
       "8   Level1       Total           X  d_1006  31117.0   85754.15  Total_X\n",
       "9   Level1       Total           X  d_1007  27018.0   74244.86  Total_X\n",
       "10  Level1       Total           X  d_1008  39707.0  108637.04  Total_X\n",
       "11  Level1       Total           X  d_1009  47082.0  128940.24  Total_X\n",
       "12  Level1       Total           X   d_101  24982.0   68908.04  Total_X\n",
       "13  Level1       Total           X  d_1010  48360.0  133218.73  Total_X\n",
       "14  Level1       Total           X  d_1011  32930.0   92274.15  Total_X\n",
       "15  Level1       Total           X  d_1012  33990.0   92743.98  Total_X\n",
       "16  Level1       Total           X  d_1013  32956.0   90505.80  Total_X\n",
       "17  Level1       Total           X  d_1014  31862.0   87172.76  Total_X\n",
       "18  Level1       Total           X  d_1015  35365.0   95702.83  Total_X\n",
       "19  Level1       Total           X  d_1016  45705.0  125791.89  Total_X\n",
       "20  Level1       Total           X  d_1017  43898.0  123256.45  Total_X\n",
       "21  Level1       Total           X  d_1018  36385.0  100212.69  Total_X\n",
       "22  Level1       Total           X  d_1019  32258.0   87909.01  Total_X\n",
       "23  Level1       Total           X   d_102  22196.0   60000.65  Total_X\n",
       "24  Level1       Total           X  d_1020  29242.0   81367.81  Total_X\n",
       "25  Level1       Total           X  d_1021  29452.0   79956.86  Total_X\n",
       "26  Level1       Total           X  d_1022  35763.0   97645.15  Total_X\n",
       "27  Level1       Total           X  d_1023  44579.0  123721.42  Total_X\n",
       "28  Level1       Total           X  d_1024  42582.0  121478.81  Total_X\n",
       "29  Level1       Total           X  d_1025  32102.0   89627.71  Total_X\n",
       "30  Level1       Total           X  d_1026  28521.0   78796.16  Total_X\n",
       "31  Level1       Total           X  d_1027  27904.0   78144.16  Total_X\n",
       "32  Level1       Total           X  d_1028  28693.0   78581.27  Total_X\n",
       "33  Level1       Total           X  d_1029  32847.0   91347.72  Total_X\n",
       "34  Level1       Total           X   d_103  22117.0   61407.00  Total_X\n",
       "35  Level1       Total           X  d_1030  40046.0  114533.56  Total_X\n",
       "36  Level1       Total           X  d_1031  38445.0  111826.12  Total_X\n",
       "37  Level1       Total           X  d_1032  28603.0   81092.24  Total_X\n",
       "38  Level1       Total           X  d_1033  31247.0   87799.42  Total_X\n",
       "39  Level1       Total           X  d_1034  36053.0   97214.80  Total_X\n",
       "40  Level1       Total           X  d_1035  19783.0   53456.88  Total_X\n",
       "41  Level1       Total           X  d_1036  26041.0   75086.93  Total_X\n",
       "42  Level1       Total           X  d_1037  31539.0   91356.80  Total_X\n",
       "43  Level1       Total           X  d_1038  38182.0  108919.39  Total_X\n",
       "44  Level1       Total           X  d_1039  37079.0   97503.03  Total_X\n",
       "45  Level1       Total           X   d_104  22347.0   60736.91  Total_X\n",
       "46  Level1       Total           X  d_1040  38010.0  100557.02  Total_X\n",
       "47  Level1       Total           X  d_1041  31513.0   83895.82  Total_X\n",
       "48  Level1       Total           X  d_1042  35139.0   93359.95  Total_X\n",
       "49  Level1       Total           X  d_1043  36894.0   99430.98  Total_X"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfs = []\n",
    "# df_eval_after_release['revenue'] = df_eval_after_release['sold'] * df_eval_after_release['sell_price']\n",
    "# for level in list(AGG_LEVEL_COLUMNS.keys()):\n",
    "#     c = AGG_LEVEL_COLUMNS[level]\n",
    "#     logger.info(level)\n",
    "#     agg_dict = {\n",
    "#         'sold': 'sum',\n",
    "#         'revenue': 'sum'\n",
    "#     }\n",
    "#     d1 = df_eval_after_release.groupby(c + ['d']).agg(agg_dict).reset_index(drop=False)\n",
    "#     d = pd.DataFrame({\n",
    "#         'd': d1['d'],\n",
    "#         'sold': d1['sold'],\n",
    "#         'revenue': d1['revenue']\n",
    "#     })\n",
    "#     if len(c) == 0:\n",
    "#         d['agg_column1'] = 'Total'\n",
    "#         d['agg_column2'] = 'X'\n",
    "#     elif len(c) == 1:\n",
    "#         d['agg_column1'] = d1[c[0]]\n",
    "#         d['agg_column2'] = 'X'\n",
    "#     else:\n",
    "#         d['agg_column1'] = d1[c[0]]\n",
    "#         d['agg_column2'] = d1[c[1]]\n",
    "#     d['id_merge'] = d['agg_column1'] + '_' + d['agg_column2']\n",
    "#     d['Level'] = level\n",
    "#     dfs.append(d[['Level', 'agg_column1', 'agg_column2', 'd', 'sold', 'revenue', 'id_merge']])\n",
    "# d = pd.concat(dfs)\n",
    "# d.head(50)\n",
    "# d.to_parquet('temp.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
