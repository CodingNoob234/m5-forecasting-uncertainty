{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from utils.utils import WRMSSE, RMSSE, _down_cast, data_preprocessing, diff_lists, log_status\n",
    "from utils.utils import cross_validation_on_validation_set, ensemble_submissions, ensemble_submissions_uncertainty\n",
    "from utils.metrics import WSPL\n",
    "from utils.configure_logger import configure_logger\n",
    "from utils.utils import prefixes_in_column\n",
    "from utils import constants\n",
    "\n",
    "configure_logger()\n",
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH = constants.DATA_BASE_PATH #'../data/m5-forecasting-accuracy/'\n",
    "DATA_BASE_PATH_UNCERTAINTY = constants.DATA_BASE_PATH_UNCERTAINTY #'../data/m5-forecasting-uncertainty/'\n",
    "SALES_EVALUATION = constants.SALES_EVALUATION \n",
    "SALES_VALIDATION = constants.SALES_VALIDATION\n",
    "CALENDAR = constants.CALENDAR \n",
    "SAMPLE_SUBMISSION = constants.SAMPLE_SUBMISSION \n",
    "SELL_PRICES = constants.SELL_PRICES\n",
    "\n",
    "PRECOMPUTED_BASE_PATH = constants.PRECOMPUTED_BASE_PATH #'../data/uncertainty/features/'\n",
    "\n",
    "DAYS: int = constants.DAYS #28\n",
    "QUANTILES: int = constants.QUANTILES \n",
    "\n",
    "AGG_LEVEL_COLUMNS = constants.AGG_LEVEL_COLUMNS\n",
    "D_CROSS_VAL_START_LIST = constants.D_CROSS_VAL_START_LIST\n",
    "\n",
    "# to simple get the precomputed name\n",
    "precomputed_name = lambda store, eval_val: f'processed_{store}_{eval_val}.pkl'\n",
    "\n",
    "TEST_PATH = constants.TEST_PATH#'test/'\n",
    "PREDICTION_BASE_PATH = constants.PREDICTION_BASE_PATH #'../data/uncertainty/temp_submissions/'\n",
    "SUBMISSION_BASE_PATH = constants.SUBMISSION_BASE_PATH #'../data/uncertainty/final_submissions/'\n",
    "\n",
    "SUB_D_START_VAL: int = constants.SUB_D_START_VAL\n",
    "SUB_D_START_EVAL: int = constants.SUB_D_START_EVAL\n",
    "\n",
    "# the columns are always included after feature processing\n",
    "# because they are required in the training and submission format\n",
    "DROP_FEATURE_COLUMNS: list = constants.DROP_FEATURE_COLUMNS #['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'd', 'sold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GridSearch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_status\n",
    "def grid_search(params: dict, param_grid: dict, features, targets, n_folds: int = 1):\n",
    "    \"\"\" \n",
    "    Given a grid with parameters, train lgb model for all possible combinations.\n",
    "    Returns the parameter set with the best score and the dictionary with all results.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    \n",
    "    # to be sure\n",
    "    features = features.reset_index(drop=True)\n",
    "    targets = targets.reset_index(drop=True)\n",
    "\n",
    "    param_combinations = list(itertools.product(*param_grid.values()))\n",
    "    results = {}\n",
    "    for i, param_combination in enumerate(param_combinations,1):\n",
    "        \n",
    "        # create dictionary with all parameters\n",
    "        param_combination = {k:v for k,v in zip(param_grid.keys(), param_combination)}\n",
    "        param_combination.update(params)\n",
    "                \n",
    "        # init dict\n",
    "        results[f\"combination_{i}\"] = {\n",
    "            'params': param_combination,\n",
    "            'res': []\n",
    "        }\n",
    "        \n",
    "        # perform n_folds\n",
    "        for j in range(n_folds):\n",
    "            \n",
    "            # kfold\n",
    "            features_train, features_validation, targets_train, targets_validation =\\\n",
    "                train_test_split(features, targets, train_size = .8, shuffle=True)#, random_state=42)\n",
    "\n",
    "            # train lgb model        \n",
    "            temp_dict = {} # this dict object will be used to add all (intermediate) evaluation scores during the training process\n",
    "            mod: lgb.Booster = lgb.train(param_combination, \n",
    "                train_set = lgb.Dataset(features_train, targets_train),\n",
    "                valid_sets = lgb.Dataset(features_validation, targets_validation),\n",
    "                evals_result = temp_dict,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            \n",
    "            # store results\n",
    "            results[f\"combination_{i}\"]['res']\\\n",
    "                .append(temp_dict[\"valid_0\"][\"quantile\"][-1],\n",
    "                )\n",
    "\n",
    "        # compute average results\n",
    "        results[f\"combination_{i}\"]['validation_score'] = \\\n",
    "            np.mean(results[f\"combination_{i}\"]['res'])\n",
    "        \n",
    "    # sort the results based on evaluation score\n",
    "    sorted_results = dict(sorted(results.items(), key=lambda item: item[1][\"validation_score\"]))\n",
    "    return list(sorted_results.values())[0], results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData:\n",
    "    \"\"\" Class to load data \"\"\"\n",
    "    def __init__(self):\n",
    "        self.level = None\n",
    "        \n",
    "    def prep_data(self,level, sub_d_start):\n",
    "        \"\"\" read the precomputed features and targets for specified aggregation level,  \"\"\"\n",
    "        # define params\n",
    "        agg_level = level\n",
    "        # sub_d_start: int = int(1886)\n",
    "        exclude_columns = []\n",
    "        test = False\n",
    "        type_of = 'val'\n",
    "\n",
    "        # read file\n",
    "        agg_columns = AGG_LEVEL_COLUMNS[agg_level]\n",
    "        if len(agg_columns) == 0:\n",
    "            agg_str: str = 'Total_X'\n",
    "        elif len(agg_columns) == 1:\n",
    "            agg_str: str = f'{agg_columns[0]}_X'\n",
    "        else:\n",
    "            agg_str: str = '_'.join(agg_columns)\n",
    "\n",
    "        if self.level == level:\n",
    "            pass\n",
    "        else:\n",
    "            logger.info('(re)loading features')\n",
    "            features = pd.read_parquet(f'../data/uncertainty/fold_{sub_d_start}/features/' + (TEST_PATH if test else '') + f'features_{type_of}_{agg_str}.parquet')\n",
    "            features = _down_cast(features)\n",
    "\n",
    "        group_columns = agg_columns\n",
    "        exclude_prefix_list = exclude_columns # unconditional, auto, momentum, seasonal\n",
    "        \n",
    "        features_gr = features.copy()\n",
    "        features_gr = features_gr[[c for c in features_gr if c.split('_')[0] not in exclude_prefix_list]]\n",
    "\n",
    "        # preparations\n",
    "        train_idx = features_gr['sold'].notna() & features_gr['d'].isin([f'd_{sub_d_start - 1 - i}' for i in range(1460)])\n",
    "        df_train = features_gr[train_idx]\n",
    "        features_train: pd.DataFrame = df_train.drop(DROP_FEATURE_COLUMNS, axis = 1, errors = 'ignore')\n",
    "        targets_train: pd.Series = df_train['sold']\n",
    "        return features_train, targets_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Training a Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 21:32:32 - __main__ - INFO - (re)loading features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['auto_sold_56',\n",
       " 'auto_sold_1',\n",
       " 'auto_sold_2',\n",
       " 'auto_sold_28',\n",
       " 'auto_sold_7',\n",
       " 'auto_sold_14',\n",
       " 'auto_sold_qtile_168_0.75',\n",
       " 'auto_sold_qtile_112_0.99',\n",
       " 'auto_sold_qtile_14_0.25',\n",
       " 'auto_sold_qtile_14_0.75',\n",
       " 'auto_sold_ewm_56',\n",
       " 'auto_sold_ewm_7',\n",
       " 'auto_sold_qtile_7_0.25',\n",
       " 'auto_sold_qtile_112_0.5',\n",
       " 'auto_sold_qtile_14_0.1',\n",
       " 'auto_sold_qtile_56_0.75',\n",
       " 'auto_sold_qtile_168_0.99',\n",
       " 'auto_sold_qtile_7_0.75',\n",
       " 'auto_sold_std_168',\n",
       " 'auto_sold_ewm_3',\n",
       " 'auto_sold_qtile_56_0.25',\n",
       " 'auto_sold_qtile_168_0.01',\n",
       " 'auto_sold_ma_3',\n",
       " 'auto_sold_qtile_14_0.5',\n",
       " 'auto_sold_qtile_56_0.5',\n",
       " 'auto_sold_ewm_14',\n",
       " 'auto_sold_ewm_168',\n",
       " 'auto_sold_ma_21',\n",
       " 'auto_sold_qtile_3_0.5',\n",
       " 'auto_sold_ewm_28',\n",
       " 'auto_sold_qtile_21_0.75',\n",
       " 'auto_sold_qtile_168_0.1',\n",
       " 'auto_sold_qtile_112_0.01',\n",
       " 'auto_sold_qtile_168_0.9',\n",
       " 'auto_sold_qtile_28_0.75',\n",
       " 'auto_sold_qtile_112_0.75',\n",
       " 'auto_sold_std_3',\n",
       " 'auto_sold_qtile_21_0.9',\n",
       " 'auto_sold_qtile_21_0.25',\n",
       " 'auto_sold_ma_168',\n",
       " 'auto_sold_qtile_21_0.1',\n",
       " 'auto_sold_ma_14',\n",
       " 'auto_sold_qtile_168_0.5',\n",
       " 'auto_sold_qtile_56_0.1',\n",
       " 'auto_sold_qtile_21_0.5',\n",
       " 'auto_sold_std_7',\n",
       " 'auto_sold_qtile_112_0.25',\n",
       " 'auto_sold_qtile_28_0.1',\n",
       " 'auto_sold_std_21',\n",
       " 'auto_sold_qtile_28_0.5',\n",
       " 'auto_sold_std_14',\n",
       " 'auto_sold_std_28',\n",
       " 'auto_sold_ma_28',\n",
       " 'auto_sold_ewm_112',\n",
       " 'auto_sold_qtile_56_0.9',\n",
       " 'auto_sold_qtile_112_0.1',\n",
       " 'auto_sold_qtile_14_0.9',\n",
       " 'auto_sold_qtile_168_0.25',\n",
       " 'auto_sold_ma_112',\n",
       " 'auto_sold_qtile_112_0.9',\n",
       " 'auto_sold_std_56',\n",
       " 'auto_sold_qtile_7_0.5',\n",
       " 'auto_sold_ma_56',\n",
       " 'auto_sold_ewm_21',\n",
       " 'auto_sold_qtile_28_0.25',\n",
       " 'auto_sold_ma_7',\n",
       " 'auto_sold_qtile_28_0.9',\n",
       " 'auto_sold_std_112',\n",
       " 'price_uncond_std',\n",
       " 'price_uncond_median',\n",
       " 'price_uncond_avg',\n",
       " 'price_momentum_w',\n",
       " 'price_momentum_m',\n",
       " 'price_momentum_y',\n",
       " 'price_auto_std_28',\n",
       " 'price_auto_std_56',\n",
       " 'price_auto_std_112',\n",
       " 'seasonal_weekday',\n",
       " 'seasonal_monthday',\n",
       " 'seasonal_month',\n",
       " 'days_fwd']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data example, to investigate which features are computed, among other things\n",
    "level = 'Level2'\n",
    "dataLoader = LoadData()\n",
    "features, targets = dataLoader.prep_data(level, 1914)\n",
    "list(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold\n",
    "prefixes = ['seasonal', 'auto_sold_ewm']\n",
    "features_train, features_validation, targets_train, targets_validation =\\\n",
    "    train_test_split(features, targets, test_size = 28, shuffle=False, random_state=42)\n",
    "    # train_test_split(features[[c for c in features.columns if prefixes_in_column(c, prefixes)]], targets, train_size = .8, shuffle=False, random_state=42)\n",
    "\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -100,\n",
    "    'n_jobs': 4,\n",
    "    # 'subsample': .9,\n",
    "    # 'subsample_freq': 1,\n",
    "    \"num_leaves\": 30,\n",
    "    \"min_child_weight\": .1,\n",
    "    \"min_child_samples\": 4,\n",
    "    \"hist_pool_size\": 1000,\n",
    "    'feature_fraction': 0.9, #.5\n",
    "    # 'bagging_fraction': .8,\n",
    "    \"learning_rate\": 0.005, #0.07,\n",
    "    \"n_estimators\": 2000,#100\n",
    "    \"max_depth\": 10,\n",
    "    # 'reg_sqrt': True,\n",
    "    # 'req_lambda': .00001,\n",
    "    # 'reg_alpha': .00001,\n",
    "    'alpha': .25,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# # train lgb model       \n",
    "# for q in [0.005, 0.025, 0.135, 0.25, 0.5, 0.75, 0.865, 0.975, 0.995]:\n",
    "#     params['alpha'] = q \n",
    "#     temp_dict = {}\n",
    "#     mod: lgb.Booster = lgb.train(params, \n",
    "#         train_set = lgb.Dataset(features_train, targets_train),\n",
    "#         valid_sets = lgb.Dataset(features_validation, targets_validation),\n",
    "#         evals_result = temp_dict,\n",
    "#         verbose_eval = False\n",
    "#     )\n",
    "#     plt.plot(mod.predict(features_validation), label = f'{q}')\n",
    "\n",
    "# plt.scatter(range(len(targets_validation.index)), targets_validation, label = 'true', s = 10)\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total ~280 seconds\n",
    "lgb_quantile_params = {     # fairly well tuned, with high runtimes \n",
    "    'max_depth': [10, 20],\n",
    "    'n_estimators': [ 150, 200, 200],  # 300, 350, 400, ],   \n",
    "    'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n",
    "    'min_child_samples': [ 2, 4, 7, 10, 14, 20, 30, 40, 60, 80, 100, 100, 100, \n",
    "                                        130, 170, 200, 300, 500, 700, 1000 ],\n",
    "    'min_child_weight': [0, 0, 0, 0, 1e-4, 1e-3, 1e-3, 1e-3, 5e-3, 2e-2, 0.1 ],\n",
    "    'num_leaves': [ 20, 30, 50, 50 ], # 50, 70, 90, ],\n",
    "    'learning_rate': [  0.04, 0.05, 0.07, 0.07, 0.07, 0.1, 0.1, 0.1 ],   # 0.02, 0.03,        \n",
    "    'colsample_bytree': [0.3, 0.5, 0.7, 0.8, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "    'colsample_bynode':[0.1, 0.15, 0.2, 0.2, 0.2, 0.25, 0.3, 0.5, 0.65, 0.8, 0.9, 1],\n",
    "    'reg_lambda': [0, 0, 0, 0, 1e-5, 1e-5, 1e-5, 1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 100   ],\n",
    "    'reg_alpha': [0, 1e-5, 3e-5, 1e-4, 1e-4, 1e-3, 3e-3, 1e-2, 0.1, 1, 1, 10, 10, 100, 1000,],\n",
    "    'subsample': [  0.9, 1],\n",
    "    'subsample_freq': [1],\n",
    "    'cat_smooth': [0.1, 0.2, 0.5, 1, 2, 5, 7, 10],\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': 4,\n",
    "    # 'eval_at': 10,\n",
    "    'hist_pool_size': 1000,\n",
    "    # 'verbose_eval': 0\n",
    "    # 'subsample': 0.5,\n",
    "    # 'subsample_freq': 1,\n",
    "    # 'feature_fraction': 0.5,\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10],\n",
    "    'n_estimators': [200, 800],\n",
    "    # 'min_split_gain': [0, 0, 0, 0, 1e-4, 1e-3, 1e-2, 0.1],\n",
    "    'min_child_samples': [4],\n",
    "    'min_child_weight': [0.1 ],\n",
    "    'num_leaves': [30], # 50, 70, 90, ],\n",
    "    'learning_rate': [0.001, 0.005, 0.01 ],   # 0.02, 0.03,        \n",
    "    # 'colsample_bytree': [0.3, 0.5, 0.7, 0.8, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "    # 'colsample_bynode':[0.1, 0.15, 0.2, 0.2, 0.2, 0.25, 0.3, 0.5, 0.65, 0.8, 0.9, 1],\n",
    "    # 'reg_lambda': [0, 1e-5, 1e-5, 1e-4, 1e-2, 1, 10, ],\n",
    "    # 'reg_alpha': [0, 1e-5, 3e-5, 1e-4, 1e-3, 1e-2, 1, 10, 100, 1000,],\n",
    "    'subsample': [  0.9, 1],\n",
    "    'subsample_freq': [1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test grid search for all quantiles\n",
    "# for q in QUANTILES[:5]:\n",
    "    \n",
    "#     # of course, update quantile in params\n",
    "#     params['alpha'] = q\n",
    "#     best_res, res = grid_search(params, param_grid, features_train, targets_train, 1)\n",
    "#     logger.info(best_res['params'])\n",
    "    \n",
    "#     mod = lgb.train(best_res['params'],\n",
    "#         train_set = lgb.Dataset(features_train, targets_train)\n",
    "#     )\n",
    "#     predictions = mod.predict(features_validation)\n",
    "#     plt.plot(predictions, label = str(q))\n",
    "\n",
    "# plt.scatter(range(len(targets_validation)), targets_validation)\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train + Predict submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_level_all_quantiles(\n",
    "    agg_level: str, \n",
    "    type_of: str, \n",
    "    sub_d_start: int, \n",
    "    exclude_columns: list = [], \n",
    "    include_columns: list = None,\n",
    "    test: bool = False, \n",
    "    do_grid_search: bool = False, \n",
    "    store_submissions_path: str = 'temp_submissions/', \n",
    "    normalize: bool = False,\n",
    "):\n",
    "    \"\"\" \n",
    "    Train, for a specific aggregation level, models for all quantiles.\n",
    "    For aggregation levels 10, 11 and 12, undersampling is used to drastically reduce training time.\n",
    "    \"\"\"\n",
    "    ALWAYS_KEEP_COLUMNS = ['days_fwd', 'sold', 'd']\n",
    "    \n",
    "    agg_columns = AGG_LEVEL_COLUMNS[agg_level]\n",
    "    if len(agg_columns) == 0:\n",
    "        agg_str: str = 'Total_X'\n",
    "    elif len(agg_columns) == 1:\n",
    "        agg_str: str = f'{agg_columns[0]}_X'\n",
    "    else:\n",
    "        agg_str: str = '_'.join(agg_columns)\n",
    "\n",
    "    # try:\n",
    "    #     features = pd.DataFrame(features)\n",
    "    # except Exception:\n",
    "    # loading features\n",
    "    logger.info('(re)loading features')\n",
    "    features = pd.read_parquet(f'../data/uncertainty/fold_{sub_d_start}/features/' + (TEST_PATH if test else '') + f'features_{type_of}_{agg_str}.parquet')\n",
    "    features = _down_cast(features)\n",
    "    features_gr = features.copy()\n",
    "    \n",
    "    # preparations\n",
    "    # sub_d_start = SUB_D_START_VAL if type_of == 'val' else SUB_D_START_EVAL\n",
    "    train_idx = features_gr['sold'].notna() & features_gr['d'].isin([f'd_{sub_d_start - 1 - i}' for i in range(1300)])\n",
    "    pred_idx = features_gr['d'].isin([f'd_{sub_d_start + i}' for i in range(DAYS)])\n",
    "\n",
    "    group_columns = agg_columns\n",
    "    res: list = []\n",
    "    \n",
    "    def check_any_prefix_matches(column, prefixes):\n",
    "        \"\"\" Return true if any prefix is in column \"\"\"\n",
    "        # print(column, prefixes, prefixes[0] in column)\n",
    "        for prefix in prefixes:\n",
    "            if prefix in column:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # select features\n",
    "    if USE_ALL or 'kbest' in include_columns:\n",
    "        columns = features_gr.columns\n",
    "    elif SPARSE_FEATURES:\n",
    "        columns = [c for c in features_gr.columns if c in SPARSE_FEATURES]\n",
    "    elif include_columns == None:\n",
    "        # exclude features from exclusion prefix list\n",
    "        exclude_prefix_list = exclude_columns \n",
    "        # columns = [c for c in features_gr.columns if c.split('_')[0] not in exclude_prefix_list]\n",
    "        columns = [c for c in features_gr.columns if not check_any_prefix_matches(c, exclude_prefix_list)]\n",
    "    elif isinstance(include_columns, list):\n",
    "        include_prefix_list = include_columns\n",
    "        # columns = [c for c in features_gr.columns if c.split('_')[0] in include_prefix_list]\n",
    "        columns = [c for c in features_gr.columns if check_any_prefix_matches(c, include_prefix_list)]\n",
    "\n",
    "    for column in ALWAYS_KEEP_COLUMNS + group_columns:\n",
    "        if column not in columns:\n",
    "            columns.append(column) \n",
    "\n",
    "    features_gr = features_gr[columns]\n",
    "    df_pred = features_gr[pred_idx]\n",
    "    df_train = features_gr[train_idx]\n",
    "    if agg_level not in ['Level9', 'Level10', 'Level11', 'Level12']:\n",
    "        df_train = df_train[df_train['sold'] >= 50]\n",
    "\n",
    "    from copy import deepcopy\n",
    "    temp_drop_feature_columns = deepcopy(DROP_FEATURE_COLUMNS)\n",
    "    if not USE_ALL and 'kbest' not in include_columns:\n",
    "        if 'state_id' in include_prefix_list:\n",
    "            temp_drop_feature_columns.remove('state_id')\n",
    "        if 'store_id' in include_prefix_list:\n",
    "            temp_drop_feature_columns.remove('store_id')\n",
    "    if USE_ALL or 'kbest' in include_columns:\n",
    "        temp_drop_feature_columns.remove('state_id')\n",
    "        temp_drop_feature_columns.remove('store_id')\n",
    "        \n",
    "    features_train: pd.DataFrame = df_train.drop(temp_drop_feature_columns, axis = 1, errors = 'ignore')\n",
    "    # logger.info(f'feature: {str(features_train.columns)}')\n",
    "    targets_train: pd.Series = df_train['sold']\n",
    "    features_predict: pd.DataFrame = df_pred.drop(temp_drop_feature_columns, axis = 1, errors = 'ignore')\n",
    "    targets_test: pd.Series = df_pred['sold']\n",
    "    \n",
    "    #### SELECT FEATURES ####\n",
    "    if 'kbest' in include_columns:\n",
    "        # cannot do selectkbest for category variables\n",
    "        exclude_from_kbest = ['state_id', 'store_id', 'seasonal_weekday', 'seasonal_monthday', 'seasonal_month', 'days_fwd']\n",
    "        # temp_drop_idx = features_train.notna().all(axis=1)\n",
    "        temp_drop_idx = features_train.drop(exclude_from_kbest, axis=1, errors='ignore').fillna(0).notna().all(axis=1)\n",
    "        from sklearn import metrics\n",
    "        from sklearn import feature_selection\n",
    "        fit = SelectKBest(\n",
    "                k=9,\n",
    "                # score_func=metrics.mean_pinball_loss\n",
    "                score_func=feature_selection.f_regression\n",
    "            ).fit(\n",
    "                features_train.drop(exclude_from_kbest, axis=1, errors='ignore').fillna(0)[temp_drop_idx], \n",
    "                targets_train[temp_drop_idx]\n",
    "            )\n",
    "        print(fit.get_feature_names_out())\n",
    "        features_keep = list(fit.get_feature_names_out())\n",
    "        for c in exclude_from_kbest:\n",
    "            if c in features_train.columns:\n",
    "                features_keep.append(c)\n",
    "        features_train = features_train[features_keep]\n",
    "        features_predict = features_predict[features_keep]\n",
    "    #### SELECT FEATURES ####\n",
    "    \n",
    "    # undersample data\n",
    "    if agg_level in undersampling_dict.keys() and HIGH_UNDERSAMPLING:\n",
    "        undersampling_pct = undersampling_dict[agg_level]\n",
    "        features_train, _, targets_train, _ = train_test_split(features_train, targets_train, train_size = undersampling_pct, shuffle=True, random_state=43)\n",
    "\n",
    "    # normalise targets\n",
    "    if normalize:\n",
    "        logger.info('scaling targets')\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        targets_train = scaler.fit_transform(targets_train.values.reshape(-1,1))\n",
    "        \n",
    "    # REMOVE THIS\n",
    "    import matplotlib.pyplot as plt\n",
    "    if PLOT_PREDICTIONS:\n",
    "        fig, ax = plt.subplots(1,1, figsize = (10,5))\n",
    "        aaa = [i for i in range(targets_test.shape[0])]\n",
    "    # REMOVE THIS\n",
    "        \n",
    "    # train model for all quantiles\n",
    "    for quantile in QUANTILES:\n",
    "        \n",
    "        # perform grid search for best parameters\n",
    "        if do_grid_search == True:\n",
    "            # split data to training and testing\n",
    "            # logger.info('divide for cross validation')\n",
    "            # x_train, x_test, y_train, y_test = train_test_split(features_train, targets_train, train_size=.8, shuffle=False, random_state=42)\n",
    "            # train_data = lgb.Dataset(x_train, y_train)\n",
    "            # validation_data = lgb.Dataset(x_test, y_test)\n",
    "            logger.info('perform gridsearch')\n",
    "            params['alpha'] = quantile\n",
    "            best_combination, results = grid_search(params, param_grid, features_train, targets_train, 1)\n",
    "            # del train_data; del validation_data\n",
    "            params_grid_train = best_combination[\"params\"]\n",
    "            logger.info(f'q: {quantile} - cv best params: {params_grid_train}')\n",
    "        else:\n",
    "            params_grid_train = PARAM_GRID_TRAIN\n",
    "            params_grid_train['alpha'] = quantile\n",
    "\n",
    "        # train_best_model\n",
    "        # logger.info(f'features: {str(features_train.columns)}')\n",
    "        mod = lgb.train(params_grid_train,\n",
    "            train_set = lgb.Dataset(features_train, targets_train)\n",
    "        )\n",
    "        # save model\n",
    "        group_names = '_'.join(group_columns)\n",
    "        if group_names == '':\n",
    "            group_names = 'Total_X'\n",
    "        if USE_ALL:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_use_all_q={quantile}.joblib'\n",
    "        elif 'kbest' in include_columns:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_include_k_best_q={quantile}.joblib'\n",
    "        elif SPARSE_FEATURES:\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_sparse_q={quantile}.joblib' \n",
    "        elif include_columns == None:\n",
    "            exclude_names = 'None' if len(exclude_prefix_list) == 0 else '_'.join(exclude_prefix_list)\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_exclude_{exclude_names}_q={quantile}.joblib'\n",
    "        elif isinstance(include_columns, list):\n",
    "            exclude_names = 'None' if len(include_prefix_list) == 0 else '_'.join(include_prefix_list)\n",
    "            file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + 'models/' + f'lgb_{type_of}_nt_{group_names}_include_{exclude_names}_q={quantile}.joblib'\n",
    "\n",
    "        import joblib\n",
    "        joblib.dump(mod, file_path)\n",
    "        \n",
    "        predictions = mod.predict(features_predict)\n",
    "        if normalize:\n",
    "            predictions = scaler.inverse_transform(predictions.reshape(-1,1)).reshape(-1,)\n",
    "        \n",
    "        # REMOVE THIS\n",
    "        if PLOT_PREDICTIONS:\n",
    "            ax.plot(aaa, predictions, label = f'{quantile}')\n",
    "        # lgb.plot_importance(mod)\n",
    "        # REMOVE THIS\n",
    "        \n",
    "        # store predictions\n",
    "        df_p = pd.DataFrame(\n",
    "            {\n",
    "                'pred': predictions,\n",
    "                'd': df_pred['d'],\n",
    "            }\n",
    "        )\n",
    "        df_p['quantile'] = quantile\n",
    "        df_p['Level'] = agg_level\n",
    "        df_p['type_of'] = 'validation' if type_of == 'val' else 'evaluation'\n",
    "        if len(agg_columns) == 0:\n",
    "            df_p['agg_column1'] = 'Total'\n",
    "            df_p['agg_column2'] = 'X'\n",
    "        elif len(agg_columns) == 1:\n",
    "            df_p['agg_column1'] = df_pred[agg_columns[0]].values\n",
    "            df_p['agg_column2'] = 'X'\n",
    "        else:\n",
    "            df_p['agg_column1'] = df_pred[agg_columns[0]].values\n",
    "            df_p['agg_column2'] = df_pred[agg_columns[1]].values\n",
    "            \n",
    "        df_p = df_p[['Level', 'agg_column1', 'agg_column2', 'd', 'quantile', 'pred', 'type_of']]\n",
    "        \n",
    "        res.append(_down_cast(df_p))\n",
    "        \n",
    "    # REMOVE THIS\n",
    "    if PLOT_PREDICTIONS:\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "        plt.show()\n",
    "    # REMOVE THIS\n",
    "        \n",
    "    # remove to reduce memory usage asap\n",
    "    del features\n",
    "        \n",
    "    # storing predictions in specified file + folder\n",
    "    df_sub_val = pd.concat(res)\n",
    "    group_names = '_'.join(group_columns)\n",
    "    if group_names == '':\n",
    "        group_names = 'Total_X'\n",
    "        \n",
    "    if USE_ALL:\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_use_all.csv'\n",
    "    elif 'kbest' in include_columns:\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_include_k_best.csv'\n",
    "    elif SPARSE_FEATURES:\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_sparse.csv'  \n",
    "    elif include_columns == None:\n",
    "        exclude_names = 'None' if len(exclude_prefix_list) == 0 else '_'.join(exclude_prefix_list)\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_exclude_{exclude_names}.csv'\n",
    "    elif isinstance(include_columns, list):\n",
    "        exclude_names = 'None' if len(include_prefix_list) == 0 else '_'.join(include_prefix_list)\n",
    "        file_path = f'../data/uncertainty/fold_{str(sub_d_start)}/' + store_submissions_path + f'lgb_{type_of}_nt_{group_names}_include_{exclude_names}.csv'\n",
    "\n",
    "    df_sub_val.to_csv(file_path, index = False)\n",
    "    logger.info('saved under: ' + file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 22:36:06 - __main__ - INFO - starting with all EXCLUDE_COLUMNS\n",
      "2023-12-12 22:36:06 - __main__ - INFO - finished all EXCLUDE_COLUMNS\n",
      "2023-12-12 22:36:06 - __main__ - INFO - ---------------------------------\n",
      "2023-12-12 22:36:06 - __main__ - INFO - starting with all INCLUDE_COLUMNS\n",
      "2023-12-12 22:36:06 - __main__ - INFO - Include columns: ['seasonal', 'auto_sold_ma_28', 'auto_sold_ma_56', 'auto_sold_ma_168', 'state_id', 'store_id']\n",
      "2023-12-12 22:36:06 - __main__ - INFO - starting with agg_level: Level10\n",
      "2023-12-12 22:36:06 - __main__ - INFO - (re)loading features\n",
      "2023-12-12 22:37:56 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_item_id_include_seasonal_auto_sold_ma_28_auto_sold_ma_56_auto_sold_ma_168_state_id_store_id.csv\n",
      "2023-12-12 22:37:56 - __main__ - INFO - starting with agg_level: Level11\n",
      "2023-12-12 22:37:56 - __main__ - INFO - (re)loading features\n",
      "2023-12-12 22:42:17 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_state_id_item_id_include_seasonal_auto_sold_ma_28_auto_sold_ma_56_auto_sold_ma_168_state_id_store_id.csv\n",
      "2023-12-12 22:42:17 - __main__ - INFO - starting with agg_level: Level12\n",
      "2023-12-12 22:42:17 - __main__ - INFO - (re)loading features\n",
      "2023-12-12 22:57:53 - __main__ - INFO - saved under: ../data/uncertainty/fold_1914/temp_submissions/lgb_val_nt_item_id_store_id_include_seasonal_auto_sold_ma_28_auto_sold_ma_56_auto_sold_ma_168_state_id_store_id.csv\n",
      "2023-12-12 22:57:53 - __main__ - INFO - finished all INCLUDE_COLUMNS\n"
     ]
    }
   ],
   "source": [
    "# all groups: seasonal, auto, autoquantiles, momentum\n",
    "SPARSE_FEATURES = [\n",
    "    'auto_sold_qtile_28_0.5', \n",
    "    'auto_sold_ma_28', \n",
    "    'auto_sold_ma_168', \n",
    "    'auto_sold_qtile_168_0.1',\n",
    "    'auto_sold_qtile_168_0.9',\n",
    "]\n",
    "SPARSE_FEATURES += [f'seasonal_weekday_{i}' for i in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']]\n",
    "SPARSE_FEATURES += [f'seasonal_monthday_{i}' for i in range(1,32)]\n",
    "SPARSE_FEATURES += [f'seasonal_month_{i}' for i in range(1,13)]\n",
    "SPARSE_FEATURES += ['seasonal_weekday', 'seasonal_monthday', 'seasonal_month']\n",
    "# SPARSE_FEATURES = [ 'seasonal_weekday', 'seasonal_monthday', 'auto_sold_ewm_112', 'auto_sold_ewm_28',\n",
    "#   'autoquantiles_sold_qtile_28_0.5', 'auto_sold_ma_28',\n",
    "#   'auto_sold_qtile_28_0.9',\n",
    "#    'auto_sold_qtile_28_0.1'\n",
    "# ],\n",
    "\n",
    "USE_ALL = False\n",
    "SPARSE_FEATURES = None\n",
    "PLOT_PREDICTIONS = False\n",
    "\n",
    "undersampling_dict = {\n",
    "    'Level10': .1, #.001\n",
    "    'Level11': .1, #.0001\n",
    "    'Level12': .1 #.00001\n",
    "}\n",
    "\n",
    "HIGH_UNDERSAMPLING = True\n",
    "TEST_NUMBER = 9 # 9\n",
    "TEST_NUMB = 0 # 0\n",
    "PARAM_GRID_TRAIN = {\n",
    "    'objective': 'quantile',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': 4,\n",
    "    \"num_leaves\": 30,\n",
    "    \"hist_pool_size\": 300,\n",
    "    \"learning_rate\": .01, # .01\n",
    "    \"n_estimators\": 1000, #1000\n",
    "    \"max_depth\": 10,\n",
    "}\n",
    "PARAM_GRID_TRAIN_HIGH_LEVEL = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': 4,\n",
    "    \"num_leaves\": 30,\n",
    "    \"hist_pool_size\": 300,\n",
    "    # 'feature_fraction': 0.9, #.5\n",
    "    # 'bagging_fraction': .8,\n",
    "    \"learning_rate\": .01, # .07\n",
    "    \"n_estimators\": 3000,#100\n",
    "    \"max_depth\": 10,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    # 'metric': 'quantile', # Use Root Mean Squared Error (RMSE) as the evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 43,\n",
    "    'verbose': -100,\n",
    "    'n_jobs': 4,\n",
    "    # 'eval_at': 10,\n",
    "    'hist_pool_size': 1000,\n",
    "    'verbose_eval': -100,\n",
    "}\n",
    "param_grid = {\n",
    "    'max_depth': [10,],\n",
    "    'n_estimators': [200, 500, 1000, 2000, 4000],\n",
    "    # 'min_child_samples': [4],\n",
    "    # 'min_child_weight': [0,0.1],\n",
    "    'num_leaves': [30], # 50, 70, 90, ],\n",
    "    'learning_rate': [.001, .005, .01, .02],#[0.04, 0.07, 0.1],   # 0.02, 0.03,        \n",
    "    # 'subsample': [ 0.9, 1 ],\n",
    "    # 'subsample_freq': [1],\n",
    "}\n",
    "\n",
    "ALL_PREFIXES = ['auto_sold', 'auto_sold_ma', 'auto_sold_std', 'auto_sold_ewm', 'auto_sold_qtile',\n",
    "    'price_momentum', 'price_uncond', 'price_auto_std','seasonal_', 'state_', 'store_',\n",
    "]\n",
    "\n",
    "EXCLUDE_COLUMNS_LIST = ()\n",
    "\n",
    "# DESELECT WHICH EXPERIMENT TO RUN\n",
    "# LATER ON I'LL ADD A DICTIONARY FOR EASIER SWITCHING BETWEEN EXPERIMENTS\n",
    "\n",
    "# EXPERIMENT: SEASONALITY FEATURES >>>>>>>>>>>>>>> DONE\n",
    "# BASE = []\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_ewm'],\n",
    "#     ['seasonal_weekday','auto_sold_ewm'],\n",
    "#     ['seasonal_monthday','auto_sold_ewm'],\n",
    "#     ['seasonal_weekday','seasonal_monthday','auto_sold_ewm'],\n",
    "#     ['seasonal','auto_sold_ewm'],\n",
    "# ]\n",
    "\n",
    "# # EXPERIMENT: COMPARE STORE_ID VS STATE_ID >>>>>>>>>>>>>>> DONE\n",
    "# BASE = ['seasonal', 'auto_sold_ma']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     [],\n",
    "#     ['state_id',],\n",
    "#     ['store_id',],\n",
    "#     ['state_id', 'store_id']\n",
    "# ]\n",
    "\n",
    "# # EXPERIMENT: AUTO_SOLD FEATURES >>>>>>>>>>>>>>> DONE\n",
    "# BASE = ['seasonal']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_ewm'],\n",
    "#     ['auto_sold_ma'],\n",
    "#     ['auto_sold_ewm', 'auto_sold_ma'],\n",
    "# ]\n",
    "\n",
    "# EXPERIMENT: QUANTILE VS. STD FEATURES >>>>>>>>>>>>>>> DONE\n",
    "# BASE = ['seasonal', 'auto_sold_ma']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_qtile'],\n",
    "#     ['auto_sold_std'],\n",
    "#     ['auto_sold_qtile','auto_sold_std'],\n",
    "# ]\n",
    "\n",
    "# # EXPERIMENT: PRICE MOMENTUM / PRICE AUTO >>>>>>>>>>>>>>> DONE\n",
    "# BASE = ['seasonal', 'auto_sold_ma']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['price_auto_std'],\n",
    "#     ['price_momentum'],\n",
    "#     ['price_uncond'],\n",
    "#     ['price_auto_std', 'price_momentum'],\n",
    "#     ['price_auto_std', 'price_momentum', 'price_uncond']\n",
    "# ]\n",
    "\n",
    "# # EXPERIMENT: BEST MODEL / ALL IMPORTANT FEATURES >>>>>>>>>>>>>>> DONE\n",
    "# BASE = ['seasonal']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_ma', 'state_id', 'store_id']\n",
    "#     ['auto_sold_ma', 'auto_sold_std', 'state_id', 'store_id'],\n",
    "# ]\n",
    "\n",
    "# EXPERIMENT: SPARSE MA VS. FULL MA\n",
    "BASE = ['seasonal']\n",
    "INCLUDE_COLUMNS_LIST = [\n",
    "    # ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm', 'state_id', 'store_id'],\n",
    "    ['auto_sold_ma_28', 'auto_sold_ma_56', 'auto_sold_ma_168', 'state_id', 'store_id']\n",
    "]\n",
    "\n",
    "# # EXPERIMENT: SPARSE VS. FULL\n",
    "# BASE = ['seasonal',]\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     # ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm', 'state_id', 'store_id'],\n",
    "#     ['auto_sold_std_3', 'auto_sold_std_56', 'auto_sold_std_168', \n",
    "#      'auto_sold_ma_7',  'auto_sold_ma_28', 'auto_sold_ma_56', \n",
    "#      'auto_sold_qtile_28_0.25', 'auto_sold_qtile_168_0.25', 'auto_sold_qtile_56_0.1', \n",
    "#      'state_id', 'store_id'],\n",
    "#     # ['kbest']\n",
    "# ]\n",
    "\n",
    "# # EXPERIMENT: SPARSE VS. FULL\n",
    "# BASE = ['seasonal']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm'],\n",
    "#     [ 'auto_sold_ewm_112', 'auto_sold_ewm_28',\n",
    "#       'auto_sold_qtile_28_0.5', 'auto_sold_ma_28',\n",
    "#       'auto_sold_qtile_28_0.9', 'auto_sold_qtile_28_0.1'\n",
    "#     ],\n",
    "# ]\n",
    "\n",
    "INCLUDE_COLUMNS_LIST = [BASE + i for i in INCLUDE_COLUMNS_LIST]\n",
    "DO_GRID_SEARCH = False\n",
    "\n",
    "logger.info('starting with all EXCLUDE_COLUMNS')\n",
    "for exclude_columns in EXCLUDE_COLUMNS_LIST: # for each specified feature combination\n",
    "    logger.info(f'Exclude columns: {str(exclude_columns)}')\n",
    "    # for sub_d_start in D_CROSS_VAL_START_LIST: # for each fold\n",
    "    for sub_d_start in D_CROSS_VAL_START_LIST:\n",
    "        for agg_level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]: # for each aggregation level\n",
    "            logger.info(f'starting with agg_level: {agg_level}')\n",
    "            train_level_all_quantiles(\n",
    "                agg_level,\n",
    "                sub_d_start=sub_d_start,\n",
    "                type_of='val', \n",
    "                exclude_columns=exclude_columns,\n",
    "                do_grid_search=DO_GRID_SEARCH,\n",
    "                store_submissions_path='temp_submissions/',\n",
    "            )\n",
    "logger.info('finished all EXCLUDE_COLUMNS')\n",
    "\n",
    "logger.info('---------------------------------')            \n",
    "logger.info('starting with all INCLUDE_COLUMNS')            \n",
    "for include_columns in INCLUDE_COLUMNS_LIST: # for each specified feature combination\n",
    "    logger.info(f'Include columns: {str(include_columns)}')\n",
    "    for sub_d_start in D_CROSS_VAL_START_LIST:\n",
    "        for agg_level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]: # for each aggregation level\n",
    "            logger.info(f'starting with agg_level: {agg_level}')\n",
    "            train_level_all_quantiles(\n",
    "                agg_level,\n",
    "                sub_d_start=sub_d_start,\n",
    "                type_of='val', \n",
    "                exclude_columns=None,\n",
    "                include_columns=include_columns,\n",
    "                do_grid_search=DO_GRID_SEARCH,\n",
    "                store_submissions_path='temp_submissions/',\n",
    "            )\n",
    "logger.info('finished all INCLUDE_COLUMNS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load val + eval prediction files and merge to one submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_columns = '_'.join([])\n",
    "# if exclude_columns == '':\n",
    "#     exclude_columns = 'None'\n",
    "\n",
    "# dfs: list = []\n",
    "# for level in AGG_LEVEL_COLUMNS:\n",
    "#     agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "#     group_names = '_'.join(agg_columns)\n",
    "#     if group_names == '':\n",
    "#         group_names = 'Total_X'\n",
    "#     i = str(1914)\n",
    "#     dfs.append(\n",
    "#         f'../data/uncertainty/fold_{i}/temp_submissions/' + f'lgb_multivariate_val_non_transposed_{group_names}_exclude_{exclude_columns}.csv',\n",
    "#     )\n",
    "\n",
    "# df_sub_val = ensemble_submissions_uncertainty(dfs)\n",
    "# transpose = True\n",
    "# if transpose == True:\n",
    "#     sub_validation = df_sub_val.pivot(index='id', columns='d', values='pred').reset_index(drop=False)\n",
    "#     sub_validation.columns = [\"id\"] + [f\"F{i}\" for i in range(1,DAYS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_concat_predictions(fold_name: int, exclude_columns: list = [], include_columns: list = [], sparse = False, use_all = False, load_submissions_path: str = 'temp_submissions/'):\n",
    "    \"\"\" \n",
    "    For specified fold, read the predictions for all aggregation levels and stack them together in one dataframe.\n",
    "    \"\"\"\n",
    "    # D_CV_START_LIST\n",
    "    # if fold_name not in D_CV_START_LIST:\n",
    "        # raise ValueError('fold_name must be a value in D_CV_START_LIST')\n",
    "        \n",
    "    exclude_columns = '_'.join(exclude_columns)\n",
    "    if exclude_columns == '':\n",
    "        exclude_columns = 'None'\n",
    "\n",
    "    logger.info('loading files under path:' + f'../data/uncertainty/fold_{fold_name}/' + load_submissions_path)\n",
    "\n",
    "    dfs: list = []\n",
    "    for level in list(AGG_LEVEL_COLUMNS.keys())[TEST_NUMB:TEST_NUMBER]:\n",
    "        agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "        group_names = '_'.join(agg_columns)\n",
    "        if group_names == '':\n",
    "            group_names = 'Total_X'\n",
    "        \n",
    "        file_path = f'../data/uncertainty/fold_{str(fold_name)}/' + load_submissions_path \n",
    "        file_path += f'lgb_val_nt_{group_names}_'\n",
    "        if use_all:\n",
    "            file_path += f'use_all.csv'  \n",
    "        elif include_columns == None:\n",
    "            file_path += f'exclude_{\"_\".join(exclude_columns)}.csv'            \n",
    "        elif isinstance(include_columns, list):\n",
    "            file_path += f'include_{\"_\".join(include_columns)}.csv'\n",
    "        \n",
    "        dfs.append(file_path)\n",
    "    return ensemble_submissions_uncertainty(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE = ['seasonal',]\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     # ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm', 'state_id', 'store_id'],\n",
    "#     ['auto_sold_std_3', 'auto_sold_std_56', 'auto_sold_std_168', \n",
    "#      'auto_sold_ma_7',  'auto_sold_ma_28', 'auto_sold_ma_56', \n",
    "#      'auto_sold_qtile_28_0.25', 'auto_sold_qtile_168_0.25', 'auto_sold_qtile_56_0.1', \n",
    "#      'state_id', 'store_id'],\n",
    "#     # ['kbest']\n",
    "# ]\n",
    "# \n",
    "# BASE = ['seasonal']\n",
    "# INCLUDE_COLUMNS_LIST = [\n",
    "#     ['auto_sold_ma', 'auto_sold_std', 'auto_sold_qtile', 'auto_sold_ewm', 'state_id', 'store_id'],\n",
    "#     ['auto_sold_ma_28', 'auto_sold_ma_56', 'auto_sold_ma_168', 'state_id', 'store_id']\n",
    "# ]\n",
    "\n",
    "# include_columns = BASE + INCLUDE_COLUMNS_LIST[1]\n",
    "# include_columns_str = '_'.join(include_columns)\n",
    "\n",
    "# dfs: list = []\n",
    "# for level in AGG_LEVEL_COLUMNS:\n",
    "#     agg_columns = AGG_LEVEL_COLUMNS[level]\n",
    "#     group_names = '_'.join(agg_columns)\n",
    "#     if group_names == '':\n",
    "#         group_names = 'Total_X'\n",
    "        \n",
    "#     dfs.append(\n",
    "#         f'../data/uncertainty/fold_{1914}/temp_submissions/lgb_val_nt_{group_names}_include_{include_columns_str}.csv'\n",
    "#     )\n",
    "\n",
    "# df_sub_eval = ensemble_submissions_uncertainty(dfs)\n",
    "# transpose = True\n",
    "# if transpose == True:\n",
    "#     sub_evaluation = df_sub_eval.pivot(index='id', columns='d', values='pred').reset_index(drop=False)\n",
    "#     sub_evaluation.columns = [\"id\"] + [f\"F{i}\" for i in range(1,DAYS+1)]\n",
    "\n",
    "# sub_evaluation2 = sub_evaluation.copy()\n",
    "# sub_evaluation2['id'] = sub_evaluation2['id'].str.removesuffix('_validation') + '_evaluation'\n",
    "# \n",
    "# pd.concat([sub_evaluation, sub_evaluation2]).to_csv('../data/uncertainty/fold_1914/final_submissions/' + f'submission_lgb_ensemble{exclude_columns}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_evaluation = pd.read_csv('../submissions/submission_baseline_evaluation.csv').drop(['Unnamed: 0'], axis=1)\n",
    "# pd.concat([sub_validation, sub_evaluation]).to_csv(SUBMISSION_BASE_PATH + f'submission_lgb_ensemble{exclude_columns}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation Prediction, we can compute WRMSSE locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these variables are used later on\n",
    "FORCE_RELOAD = False\n",
    "try:\n",
    "    # simple code to check if variable exists\n",
    "    d_int + 1\n",
    "    if FORCE_RELOAD:\n",
    "        raise Exception()\n",
    "except:\n",
    "    # if not, load again\n",
    "    # takes about 2-3 minutes to reload and parse\n",
    "    d = pd.read_parquet('../data/uncertainty/cv_template/temp.parquet')\n",
    "    d_int = pd.read_parquet('../data/uncertainty/cv_template/temp_d_int.parquet')['d_int']\n",
    "    # d_int = d['d'].str.split('_').apply(lambda x: int(x[1]))\n",
    "    # d_int.to_frame('d_int').to_parquet('../data/uncertainty/cv_template/temp_d_int.parquet', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cv(df: pd.DataFrame, df_sub: pd.DataFrame):\n",
    "    \n",
    "    # to be able to merge\n",
    "    df_sub['id_merge'] = df_sub['id'].str.split('.')\\\n",
    "        .apply(lambda x: x[0])\n",
    "    df_sub['quantile'] = df_sub['id'].str.split('.')\\\n",
    "        .apply(lambda x: float('.'.join([x[-2], x[-1].split('_')[0]])))\n",
    "\n",
    "    # merge predictions in cv template\n",
    "    p = pd.merge(\n",
    "        df,\n",
    "        df_sub,\n",
    "        how='left',\n",
    "        on=['id_merge', 'd']\n",
    "    )\n",
    "    # del df; del df_sub_val\n",
    "    p['id_merge'] = p['id_merge'].astype(str)\n",
    "\n",
    "    for c in ['sold', 'revenue']:\n",
    "        p[c] = p[c].astype(np.float32)\n",
    "    # d = d[d_int < (D_CV_START + 28)]\n",
    "\n",
    "    return WSPL(p, [f'd_{i}' for i in range(D_CV_START, D_CV_START + 500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 23:06:38 - __main__ - INFO - start evaluating exclude columns\n",
      "2023-12-12 23:06:38 - __main__ - INFO - start evaluating include columns\n",
      "2023-12-12 23:06:38 - __main__ - INFO - --------------- ['seasonal', 'auto_sold_ma_28', 'auto_sold_ma_56', 'auto_sold_ma_168', 'state_id', 'store_id'] ---------------\n",
      "2023-12-12 23:06:42 - __main__ - INFO - loading files under path:../data/uncertainty/fold_1914/temp_submissions/\n",
      "2023-12-12 23:07:41 - utils.metrics - INFO - reading weights file\n",
      "2023-12-12 23:07:41 - utils.metrics - INFO - sorting df on d ...\n",
      "2023-12-12 23:08:18 - utils.metrics - INFO - entering loop ...\n",
      "2023-12-12 23:08:25 - utils.metrics - INFO - Level1 - 0.1623796515886052\n",
      "2023-12-12 23:08:38 - utils.metrics - INFO - Level10 - 0.2582612270476265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're not fucked :) (this time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 23:09:20 - utils.metrics - INFO - Level11 - 0.2519612287342402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're not fucked :) (this time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 23:11:31 - utils.metrics - INFO - Level12 - 0.2632835838006847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're not fucked :) (this time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 23:11:32 - utils.metrics - INFO - Level2 - 0.1436001724805093\n",
      "2023-12-12 23:11:32 - utils.metrics - INFO - Level3 - 0.16427132625809526\n",
      "2023-12-12 23:11:32 - utils.metrics - INFO - Level4 - 0.1768691883764463\n",
      "2023-12-12 23:11:32 - utils.metrics - INFO - Level5 - 0.17868699271733957\n",
      "2023-12-12 23:11:32 - utils.metrics - INFO - Level6 - 0.16639477239270037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're not fucked :) (this time)\n",
      "we're not fucked :) (this time)\n",
      "we're not fucked :) (this time)\n",
      "we're not fucked :) (this time)\n",
      "we're not fucked :) (this time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 23:11:32 - utils.metrics - INFO - Level7 - 0.19138719724963996\n",
      "2023-12-12 23:11:32 - utils.metrics - INFO - Level8 - 0.17394958129285998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're not fucked :) (this time)\n",
      "we're not fucked :) (this time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 23:11:32 - utils.metrics - INFO - Level9 - 0.19751043101897722\n",
      "2023-12-12 23:11:40 - __main__ - INFO - 1914 - wspl: 0.1940462794131437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're not fucked :) (this time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 23:11:40 - __main__ - INFO - 1914 - mean wspl: 0.1940462794131437 +/- 0.0\n",
      "2023-12-12 23:11:40 - __main__ - INFO - 1914 - raw results: [0.1940462794131437]\n"
     ]
    }
   ],
   "source": [
    "FILE_NAME_ALL_RESULTS = '../data/uncertainty/all_results.json'\n",
    "USE_ALL = False\n",
    "sparse_features = ['dayofweek', 'dayofmonth', \n",
    "                     'qs_30d_ewm', 'qs_100d_ewm',\n",
    "                    'qs_median_28d', 'qs_mean_28d',# 'qs_stdev_28d',\n",
    "                    'state_id',\n",
    "               #     'store_id',\n",
    "                   'qs_qtile90_28d',\n",
    "                    'pct_nonzero_days_28d',\n",
    "                    'days_fwd'\n",
    "                    ]\n",
    "FOLDER = 'temp_submissions/'\n",
    "\n",
    "TEST_NUMB = 0\n",
    "TEST_NUMBER = 9\n",
    "\n",
    "# load dict to store results in\n",
    "from utils.utils import load_results_as_json\n",
    "results = load_results_as_json(FILE_NAME_ALL_RESULTS)\n",
    "\n",
    "EXCLUDE_COLUMNS_LIST = []\n",
    "\n",
    "logger.info('start evaluating exclude columns')\n",
    "for EXCLUDE_COLUMNS in EXCLUDE_COLUMNS_LIST:\n",
    "    \n",
    "    results['exclude_' + ' '.join(EXCLUDE_COLUMNS)] = {}\n",
    "    \n",
    "    logger.info('--------------- ' + str(EXCLUDE_COLUMNS) + ' ---------------')\n",
    "    res = []\n",
    "    for D_CV_START in D_CROSS_VAL_START_LIST:#[:1 if TEST_NUMBER>0 else 10]:\n",
    "\n",
    "        mean_wspl, res_dict = perform_cv(\n",
    "            _down_cast(d)[d_int < (D_CV_START + DAYS)], \n",
    "            read_concat_predictions(\n",
    "                fold_name = D_CV_START, \n",
    "                exclude_columns = EXCLUDE_COLUMNS,\n",
    "                include_columns = None,\n",
    "                use_all=USE_ALL,\n",
    "                load_submissions_path=FOLDER\n",
    "            )\n",
    "        )\n",
    "        res.append(mean_wspl)\n",
    "        results['exclude_' + ' '.join(EXCLUDE_COLUMNS)]['fold_' + str(D_CV_START)] = res_dict \n",
    "        logger.info(str(D_CV_START) + ' - wspl: ' + str(mean_wspl))\n",
    "\n",
    "    logger.info(' - mean wspl: ' + str(np.mean(res)) + ' +/- ' + str(np.std(res)))\n",
    "    logger.info(str(D_CV_START) + ' - raw results: ' + str(res))\n",
    "    \n",
    "logger.info('start evaluating include columns')\n",
    "# for INCLUDE_COLUMNS in [['k_best']]:\n",
    "for INCLUDE_COLUMNS in INCLUDE_COLUMNS_LIST:\n",
    "    results['include_' + ' '.join(INCLUDE_COLUMNS)] = {}\n",
    "    \n",
    "    logger.info('--------------- ' + str(INCLUDE_COLUMNS) + ' ---------------')\n",
    "    res = []\n",
    "    for D_CV_START in D_CROSS_VAL_START_LIST[-1:]:#[:1 if TEST_NUMBER>0 else 10]:\n",
    "        \n",
    "        mean_wspl, res_dict = perform_cv(\n",
    "            _down_cast(d)[d_int < (D_CV_START + DAYS)], \n",
    "            read_concat_predictions(\n",
    "                fold_name = D_CV_START, \n",
    "                exclude_columns = [], \n",
    "                include_columns = INCLUDE_COLUMNS,\n",
    "                use_all=USE_ALL,\n",
    "                load_submissions_path=FOLDER\n",
    "            )\n",
    "        )\n",
    "        res.append(mean_wspl)\n",
    "\n",
    "        results['include_' + ' '.join(INCLUDE_COLUMNS)]['fold_' + str(D_CV_START)] = res_dict \n",
    "        logger.info(str(D_CV_START) + ' - wspl: ' + str(mean_wspl))\n",
    "\n",
    "    logger.info(str(D_CV_START) + ' - mean wspl: ' + str(np.mean(res)) + ' +/- ' + str(np.std(res)))\n",
    "    logger.info(str(D_CV_START) + ' - raw results: ' + str(res))\n",
    "\n",
    "from utils.utils import store_results_as_json\n",
    "store_results_as_json(results, FILE_NAME_ALL_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Beneath can be used to create submission template\n",
    "The submission template can be used to quickly insert your predictions.\n",
    "It also contains all other (historical) sales to be able to compute the WRMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_validation = pd.read_csv(DATA_BASE_PATH + SALES_VALIDATION)\n",
    "# sales_evaluation = pd.read_csv(DATA_BASE_PATH + SALES_EVALUATION)\n",
    "# calendar = pd.read_csv(DATA_BASE_PATH + CALENDAR)\n",
    "# sell_prices = pd.read_csv(DATA_BASE_PATH + SELL_PRICES)\n",
    "\n",
    "# df_val, submission_idx_val = data_preprocessing(sales_validation, calendar, sell_prices)\n",
    "# del sales_validation\n",
    "# df_eval, submission_idx_eval = data_preprocessing(sales_evaluation, calendar, sell_prices)\n",
    "# del sales_evaluation\n",
    "\n",
    "# df_val_after_release = df_val[(df_val.wm_yr_wk > df_val.release)]# & (df_val[\"sold\"].notna())]\n",
    "# del df_val\n",
    "# df_eval_after_release = df_eval[(df_eval.wm_yr_wk > df_eval.release)]# & (df_eval[\"sold\"].notna())]\n",
    "# del df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# df_eval_after_release['revenue'] = df_eval_after_release['sold'] * df_eval_after_release['sell_price']\n",
    "# for level in list(AGG_LEVEL_COLUMNS.keys()):\n",
    "#     c = AGG_LEVEL_COLUMNS[level]\n",
    "#     logger.info(level)\n",
    "#     agg_dict = {\n",
    "#         'sold': 'sum',\n",
    "#         'revenue': 'sum'\n",
    "#     }\n",
    "#     d1 = df_eval_after_release.groupby(c + ['d']).agg(agg_dict).reset_index(drop=False)\n",
    "#     d = pd.DataFrame({\n",
    "#         'd': d1['d'],\n",
    "#         'sold': d1['sold'],\n",
    "#         'revenue': d1['revenue']\n",
    "#     })\n",
    "#     if len(c) == 0:\n",
    "#         d['agg_column1'] = 'Total'\n",
    "#         d['agg_column2'] = 'X'\n",
    "#     elif len(c) == 1:\n",
    "#         d['agg_column1'] = d1[c[0]]\n",
    "#         d['agg_column2'] = 'X'\n",
    "#     else:\n",
    "#         d['agg_column1'] = d1[c[0]]\n",
    "#         d['agg_column2'] = d1[c[1]]\n",
    "#     d['id_merge'] = d['agg_column1'] + '_' + d['agg_column2']\n",
    "#     d['Level'] = level\n",
    "#     dfs.append(d[['Level', 'agg_column1', 'agg_column2', 'd', 'sold', 'revenue', 'id_merge']])\n",
    "# d = pd.concat(dfs)\n",
    "# d.head(50)\n",
    "# d.to_parquet('temp.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
